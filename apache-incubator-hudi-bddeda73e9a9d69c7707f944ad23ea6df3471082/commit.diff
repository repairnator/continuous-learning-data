diff --git a/hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java b/hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
index 8a50309d2..0d67da7c0 100644
--- a/hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
+++ b/hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieLogFileCommand.java
@@ -38,10 +38,7 @@
 import org.apache.hudi.config.HoodieMemoryConfig;
 import org.apache.hudi.hive.util.SchemaUtil;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.Maps;
 import com.fasterxml.jackson.databind.ObjectMapper;
-
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.FileStatus;
@@ -59,6 +56,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.stream.Collectors;
 
 import scala.Tuple2;
@@ -85,14 +83,14 @@ public String showLogFileCommits(
     List<String> logFilePaths = Arrays.stream(fs.globStatus(new Path(logFilePathPattern)))
         .map(status -> status.getPath().toString()).collect(Collectors.toList());
     Map<String, List<Tuple3<HoodieLogBlockType, Tuple2<Map<HeaderMetadataType, String>, Map<HeaderMetadataType, String>>, Integer>>> commitCountAndMetadata =
-        Maps.newHashMap();
+        new HashMap<>();
     int numCorruptBlocks = 0;
     int dummyInstantTimeCount = 0;
 
     for (String logFilePath : logFilePaths) {
       FileStatus[] fsStatus = fs.listStatus(new Path(logFilePath));
       Schema writerSchema = new AvroSchemaConverter()
-          .convert(Preconditions.checkNotNull(SchemaUtil.readSchemaFromLogFile(fs, new Path(logFilePath))));
+          .convert(Objects.requireNonNull(SchemaUtil.readSchemaFromLogFile(fs, new Path(logFilePath))));
       Reader reader = HoodieLogFormat.newReader(fs, new HoodieLogFile(fsStatus[0].getPath()), writerSchema);
 
       // read the avro blocks
@@ -181,7 +179,7 @@ public String showLogFileRecords(
     AvroSchemaConverter converter = new AvroSchemaConverter();
     // get schema from last log file
     Schema readerSchema =
-        converter.convert(Preconditions.checkNotNull(SchemaUtil.readSchemaFromLogFile(fs, new Path(logFilePaths.get(logFilePaths.size() - 1)))));
+        converter.convert(Objects.requireNonNull(SchemaUtil.readSchemaFromLogFile(fs, new Path(logFilePaths.get(logFilePaths.size() - 1)))));
 
     List<IndexedRecord> allRecords = new ArrayList<>();
 
@@ -204,7 +202,7 @@ public String showLogFileRecords(
     } else {
       for (String logFile : logFilePaths) {
         Schema writerSchema = new AvroSchemaConverter()
-            .convert(Preconditions.checkNotNull(SchemaUtil.readSchemaFromLogFile(client.getFs(), new Path(logFile))));
+            .convert(Objects.requireNonNull(SchemaUtil.readSchemaFromLogFile(client.getFs(), new Path(logFile))));
         HoodieLogFormat.Reader reader =
             HoodieLogFormat.newReader(fs, new HoodieLogFile(new Path(logFile)), writerSchema);
         // read the avro blocks
diff --git a/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java b/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
index 13d1c8b75..b0771c260 100644
--- a/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
+++ b/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java
@@ -18,11 +18,11 @@
 
 package org.apache.hudi.cli.commands;
 
-import com.google.common.base.Strings;
 import org.apache.hudi.HoodieWriteClient;
 import org.apache.hudi.cli.DedupeSparkJob;
 import org.apache.hudi.cli.utils.SparkUtil;
 import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.index.HoodieIndex;
@@ -81,7 +81,7 @@ public static void main(String[] args) throws Exception {
       case UPSERT:
         assert (args.length >= 12);
         String propsFilePath = null;
-        if (!Strings.isNullOrEmpty(args[11])) {
+        if (!StringUtils.isNullOrEmpty(args[11])) {
           propsFilePath = args[11];
         }
         List<String> configs = new ArrayList<>();
@@ -94,7 +94,7 @@ public static void main(String[] args) throws Exception {
       case COMPACT_RUN:
         assert (args.length >= 9);
         propsFilePath = null;
-        if (!Strings.isNullOrEmpty(args[8])) {
+        if (!StringUtils.isNullOrEmpty(args[8])) {
           propsFilePath = args[8];
         }
         configs = new ArrayList<>();
@@ -107,7 +107,7 @@ public static void main(String[] args) throws Exception {
       case COMPACT_SCHEDULE:
         assert (args.length >= 6);
         propsFilePath = null;
-        if (!Strings.isNullOrEmpty(args[5])) {
+        if (!StringUtils.isNullOrEmpty(args[5])) {
           propsFilePath = args[5];
         }
         configs = new ArrayList<>();
@@ -142,7 +142,7 @@ public static void main(String[] args) throws Exception {
       case CLEAN:
         assert (args.length >= 5);
         propsFilePath = null;
-        if (!Strings.isNullOrEmpty(args[3])) {
+        if (!StringUtils.isNullOrEmpty(args[3])) {
           propsFilePath = args[3];
         }
         configs = new ArrayList<>();
diff --git a/hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java b/hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
index b71a979ae..e7ae6f49a 100644
--- a/hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
+++ b/hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java
@@ -24,8 +24,6 @@
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.StringUtils;
 
-import com.google.common.base.Preconditions;
-
 import org.apache.spark.SparkConf;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.launcher.SparkLauncher;
@@ -33,6 +31,7 @@
 import java.io.File;
 import java.net.URISyntaxException;
 import java.util.Map;
+import java.util.Objects;
 
 /**
  * Utility functions dealing with Spark.
@@ -55,7 +54,7 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax
       sparkLauncher.setPropertiesFile(propertiesFile);
     }
     File libDirectory = new File(new File(currentJar).getParent(), "lib");
-    for (String library : Preconditions.checkNotNull(libDirectory.list())) {
+    for (String library : Objects.requireNonNull(libDirectory.list())) {
       sparkLauncher.addJar(new File(libDirectory, library).getAbsolutePath());
     }
     return sparkLauncher;
diff --git a/hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java b/hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java
index 56a47b732..489d8b173 100644
--- a/hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java
+++ b/hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java
@@ -36,13 +36,13 @@
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.func.OperationResult;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
@@ -138,7 +138,7 @@ public CompactionAdminClient(JavaSparkContext jsc, String basePath, Option<Embed
       // TODO: Add a rollback instant but for compaction
       HoodieInstant instant = new HoodieInstant(State.REQUESTED, COMPACTION_ACTION, compactionInstant);
       boolean deleted = metaClient.getFs().delete(new Path(metaClient.getMetaPath(), instant.getFileName()), false);
-      Preconditions.checkArgument(deleted, "Unable to delete compaction instant.");
+      ValidationUtils.checkArgument(deleted, "Unable to delete compaction instant.");
     }
     return res;
   }
@@ -220,8 +220,8 @@ public CompactionAdminClient(JavaSparkContext jsc, String basePath, Option<Embed
   private static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient metaClient, String compactionInstant)
       throws IOException {
     return AvroUtils.deserializeCompactionPlan(
-            metaClient.getActiveTimeline().readPlanAsBytes(
-                    HoodieTimeline.getCompactionRequestedInstant(compactionInstant)).get());
+        metaClient.getActiveTimeline().readPlanAsBytes(
+            HoodieTimeline.getCompactionRequestedInstant(compactionInstant)).get());
   }
 
   /**
@@ -247,7 +247,7 @@ private static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient meta
     List<HoodieLogFile> logFilesToBeMoved =
         merged.getLogFiles().filter(lf -> lf.getLogVersion() > maxVersion).collect(Collectors.toList());
     return logFilesToBeMoved.stream().map(lf -> {
-      Preconditions.checkArgument(lf.getLogVersion() - maxVersion > 0, "Expect new log version to be sane");
+      ValidationUtils.checkArgument(lf.getLogVersion() - maxVersion > 0, "Expect new log version to be sane");
       HoodieLogFile newLogFile = new HoodieLogFile(new Path(lf.getPath().getParent(),
           FSUtils.makeLogFileName(lf.getFileId(), "." + FSUtils.getFileExtensionFromLog(lf.getPath()),
               compactionInstant, lf.getLogVersion() - maxVersion, HoodieLogFormat.UNKNOWN_WRITE_TOKEN)));
@@ -266,9 +266,9 @@ private static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient meta
   protected static void renameLogFile(HoodieTableMetaClient metaClient, HoodieLogFile oldLogFile,
       HoodieLogFile newLogFile) throws IOException {
     FileStatus[] statuses = metaClient.getFs().listStatus(oldLogFile.getPath());
-    Preconditions.checkArgument(statuses.length == 1, "Only one status must be present");
-    Preconditions.checkArgument(statuses[0].isFile(), "Source File must exist");
-    Preconditions.checkArgument(oldLogFile.getPath().getParent().equals(newLogFile.getPath().getParent()),
+    ValidationUtils.checkArgument(statuses.length == 1, "Only one status must be present");
+    ValidationUtils.checkArgument(statuses[0].isFile(), "Source File must exist");
+    ValidationUtils.checkArgument(oldLogFile.getPath().getParent().equals(newLogFile.getPath().getParent()),
         "Log file must only be moved within the parent directory");
     metaClient.getFs().rename(oldLogFile.getPath(), newLogFile.getPath());
   }
@@ -300,9 +300,9 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met
                     new Path(FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()),
                         new Path(operation.getDataFileName().get())))
                 .getPath().toString();
-            Preconditions.checkArgument(df.isPresent(),
+            ValidationUtils.checkArgument(df.isPresent(),
                 "Data File must be present. File Slice was : " + fs + ", operation :" + operation);
-            Preconditions.checkArgument(df.get().getPath().equals(expPath),
+            ValidationUtils.checkArgument(df.get().getPath().equals(expPath),
                 "Base Path in operation is specified as " + expPath + " but got path " + df.get().getPath());
           }
           Set<HoodieLogFile> logFilesInFileSlice = fs.getLogFiles().collect(Collectors.toSet());
@@ -310,7 +310,7 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met
             try {
               FileStatus[] fileStatuses = metaClient.getFs().listStatus(new Path(
                   FSUtils.getPartitionPath(metaClient.getBasePath(), operation.getPartitionPath()), new Path(dp)));
-              Preconditions.checkArgument(fileStatuses.length == 1, "Expect only 1 file-status");
+              ValidationUtils.checkArgument(fileStatuses.length == 1, "Expect only 1 file-status");
               return new HoodieLogFile(fileStatuses[0]);
             } catch (FileNotFoundException fe) {
               throw new CompactionValidationException(fe.getMessage());
@@ -320,12 +320,12 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met
           }).collect(Collectors.toSet());
           Set<HoodieLogFile> missing = logFilesInCompactionOp.stream().filter(lf -> !logFilesInFileSlice.contains(lf))
               .collect(Collectors.toSet());
-          Preconditions.checkArgument(missing.isEmpty(),
+          ValidationUtils.checkArgument(missing.isEmpty(),
               "All log files specified in compaction operation is not present. Missing :" + missing + ", Exp :"
                   + logFilesInCompactionOp + ", Got :" + logFilesInFileSlice);
           Set<HoodieLogFile> diff = logFilesInFileSlice.stream().filter(lf -> !logFilesInCompactionOp.contains(lf))
               .collect(Collectors.toSet());
-          Preconditions.checkArgument(diff.stream().allMatch(lf -> lf.getBaseCommitTime().equals(compactionInstant)),
+          ValidationUtils.checkArgument(diff.stream().allMatch(lf -> lf.getBaseCommitTime().equals(compactionInstant)),
               "There are some log-files which are neither specified in compaction plan "
                   + "nor present after compaction request instant. Some of these :" + diff);
         } else {
diff --git a/hudi-client/src/main/java/org/apache/hudi/HoodieCleanClient.java b/hudi-client/src/main/java/org/apache/hudi/HoodieCleanClient.java
index 9411782bc..5eefa0f94 100644
--- a/hudi-client/src/main/java/org/apache/hudi/HoodieCleanClient.java
+++ b/hudi-client/src/main/java/org/apache/hudi/HoodieCleanClient.java
@@ -31,14 +31,13 @@
 import org.apache.hudi.common.util.AvroUtils;
 import org.apache.hudi.common.util.CleanerUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.metrics.HoodieMetrics;
 import org.apache.hudi.table.HoodieTable;
 
 import com.codahale.metrics.Timer;
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -108,7 +107,6 @@ protected HoodieCleanMetadata clean(String startCleanTime) throws HoodieIOExcept
    * @param startCleanTime Cleaner Instant Time
    * @return Cleaner Plan if generated
    */
-  @VisibleForTesting
   protected Option<HoodieCleanerPlan> scheduleClean(String startCleanTime) {
     // Create a Hoodie table which encapsulated the commits and files visible
     HoodieTable<T> table = HoodieTable.getHoodieTable(createMetaClient(true), config, jsc);
@@ -138,7 +136,6 @@ protected HoodieCleanMetadata clean(String startCleanTime) throws HoodieIOExcept
    * @param table Hoodie Table
    * @param cleanInstant Cleaner Instant
    */
-  @VisibleForTesting
   protected HoodieCleanMetadata runClean(HoodieTable<T> table, HoodieInstant cleanInstant) {
     try {
       HoodieCleanerPlan cleanerPlan = CleanerUtils.getCleanerPlan(table.getMetaClient(), cleanInstant);
@@ -150,7 +147,7 @@ protected HoodieCleanMetadata runClean(HoodieTable<T> table, HoodieInstant clean
 
   private HoodieCleanMetadata runClean(HoodieTable<T> table, HoodieInstant cleanInstant,
       HoodieCleanerPlan cleanerPlan) {
-    Preconditions.checkArgument(
+    ValidationUtils.checkArgument(
         cleanInstant.getState().equals(State.REQUESTED) || cleanInstant.getState().equals(State.INFLIGHT));
 
     try {
diff --git a/hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java b/hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java
index 0b8df71f2..92208188d 100644
--- a/hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java
+++ b/hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java
@@ -42,6 +42,7 @@
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieCommitException;
@@ -61,8 +62,6 @@
 import org.apache.hudi.table.WorkloadStat;
 
 import com.codahale.metrics.Timer;
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -99,7 +98,6 @@
   private final transient HoodieCleanClient<T> cleanClient;
   private transient Timer.Context compactionTimer;
 
-
   /**
    * Create a wirte client, without cleaning up failed/inflight commits.
    *
@@ -121,7 +119,6 @@ public HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig, b
     this(jsc, clientConfig, rollbackPending, HoodieIndex.createIndex(clientConfig, jsc));
   }
 
-  @VisibleForTesting
   HoodieWriteClient(JavaSparkContext jsc, HoodieWriteConfig clientConfig, boolean rollbackPending, HoodieIndex index) {
     this(jsc, clientConfig, rollbackPending, index, Option.empty());
   }
@@ -582,7 +579,7 @@ public boolean savepoint(String commitTime, String user, String comment) {
       }
 
       // Cannot allow savepoint time on a commit that could have been cleaned
-      Preconditions.checkArgument(
+      ValidationUtils.checkArgument(
           HoodieTimeline.compareTimestamps(commitTime, lastCommitRetained, HoodieTimeline.GREATER_OR_EQUAL),
           "Could not savepoint commit " + commitTime + " as this is beyond the lookup window " + lastCommitRetained);
 
@@ -698,8 +695,8 @@ public boolean rollbackToSavepoint(String savepointTime) {
     // Make sure the rollback was successful
     Option<HoodieInstant> lastInstant =
         activeTimeline.reload().getCommitsAndCompactionTimeline().filterCompletedAndCompactionInstants().lastInstant();
-    Preconditions.checkArgument(lastInstant.isPresent());
-    Preconditions.checkArgument(lastInstant.get().getTimestamp().equals(savepointTime),
+    ValidationUtils.checkArgument(lastInstant.isPresent());
+    ValidationUtils.checkArgument(lastInstant.get().getTimestamp().equals(savepointTime),
         savepointTime + "is not the last commit after rolling back " + commitsToRollback + ", last commit was "
             + lastInstant.get().getTimestamp());
     return true;
@@ -869,12 +866,11 @@ private void startCommit(String instantTime) {
     LOG.info("Generate a new instant time " + instantTime);
     HoodieTableMetaClient metaClient = createMetaClient(true);
     // if there are pending compactions, their instantTime must not be greater than that of this instant time
-    metaClient.getActiveTimeline().filterPendingCompactionTimeline().lastInstant().ifPresent(latestPending -> {
-      Preconditions.checkArgument(
-          HoodieTimeline.compareTimestamps(latestPending.getTimestamp(), instantTime, HoodieTimeline.LESSER),
-          "Latest pending compaction instant time must be earlier than this instant time. Latest Compaction :"
-              + latestPending + ",  Ingesting at " + instantTime);
-    });
+    metaClient.getActiveTimeline().filterPendingCompactionTimeline().lastInstant().ifPresent(latestPending ->
+        ValidationUtils.checkArgument(
+        HoodieTimeline.compareTimestamps(latestPending.getTimestamp(), instantTime, HoodieTimeline.LESSER),
+        "Latest pending compaction instant time must be earlier than this instant time. Latest Compaction :"
+            + latestPending + ",  Ingesting at " + instantTime));
     HoodieTable<T> table = HoodieTable.getHoodieTable(metaClient, config, jsc);
     HoodieActiveTimeline activeTimeline = table.getActiveTimeline();
     String commitActionType = table.getMetaClient().getCommitActionType();
@@ -903,18 +899,17 @@ public boolean scheduleCompactionAtInstant(String instantTime, Option<Map<String
       throws IOException {
     HoodieTableMetaClient metaClient = createMetaClient(true);
     // if there are inflight writes, their instantTime must not be less than that of compaction instant time
-    metaClient.getCommitsTimeline().filterPendingExcludingCompaction().firstInstant().ifPresent(earliestInflight -> {
-      Preconditions.checkArgument(
-          HoodieTimeline.compareTimestamps(earliestInflight.getTimestamp(), instantTime, HoodieTimeline.GREATER),
-          "Earliest write inflight instant time must be later than compaction time. Earliest :" + earliestInflight
-              + ", Compaction scheduled at " + instantTime);
-    });
+    metaClient.getCommitsTimeline().filterPendingExcludingCompaction().firstInstant()
+        .ifPresent(earliestInflight -> ValidationUtils.checkArgument(
+        HoodieTimeline.compareTimestamps(earliestInflight.getTimestamp(), instantTime, HoodieTimeline.GREATER),
+        "Earliest write inflight instant time must be later than compaction time. Earliest :" + earliestInflight
+            + ", Compaction scheduled at " + instantTime));
     // Committed and pending compaction instants should have strictly lower timestamps
     List<HoodieInstant> conflictingInstants = metaClient
         .getActiveTimeline().getCommitsAndCompactionTimeline().getInstants().filter(instant -> HoodieTimeline
             .compareTimestamps(instant.getTimestamp(), instantTime, HoodieTimeline.GREATER_OR_EQUAL))
         .collect(Collectors.toList());
-    Preconditions.checkArgument(conflictingInstants.isEmpty(),
+    ValidationUtils.checkArgument(conflictingInstants.isEmpty(),
         "Following instants have timestamps >= compactionInstant (" + instantTime + ") Instants :"
             + conflictingInstants);
     HoodieTable<T> table = HoodieTable.getHoodieTable(metaClient, config, jsc);
@@ -1114,7 +1109,6 @@ protected void commitCompaction(JavaRDD<WriteStatus> compactedStatuses, HoodieTa
    * @param inflightInstant Inflight Compaction Instant
    * @param table Hoodie Table
    */
-  @VisibleForTesting
   void rollbackInflightCompaction(HoodieInstant inflightInstant, HoodieTable table) throws IOException {
     table.rollback(jsc, inflightInstant, false);
     // Revert instant state file
diff --git a/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java b/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
index 376f299f6..440376796 100644
--- a/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
+++ b/hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java
@@ -20,11 +20,10 @@
 
 import org.apache.hudi.common.model.HoodieCleaningPolicy;
 import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.io.compact.strategy.CompactionStrategy;
 import org.apache.hudi.io.compact.strategy.LogFileSizeBasedCompactionStrategy;
 
-import com.google.common.base.Preconditions;
-
 import javax.annotation.concurrent.Immutable;
 
 import java.io.File;
@@ -292,8 +291,8 @@ public HoodieCompactionConfig build() {
       int maxInstantsToKeep = Integer.parseInt(props.getProperty(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP));
       int cleanerCommitsRetained =
           Integer.parseInt(props.getProperty(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP));
-      Preconditions.checkArgument(maxInstantsToKeep > minInstantsToKeep);
-      Preconditions.checkArgument(minInstantsToKeep > cleanerCommitsRetained,
+      ValidationUtils.checkArgument(maxInstantsToKeep > minInstantsToKeep);
+      ValidationUtils.checkArgument(minInstantsToKeep > cleanerCommitsRetained,
           String.format(
               "Increase %s=%d to be greater than %s=%d. Otherwise, there is risk of incremental pull "
                   + "missing data from few instants.",
diff --git a/hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java b/hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
index 0e9dab0dc..1c3d88829 100644
--- a/hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
+++ b/hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java
@@ -29,7 +29,6 @@
 import org.apache.hudi.io.compact.strategy.CompactionStrategy;
 import org.apache.hudi.metrics.MetricsReporterType;
 
-import com.google.common.base.Preconditions;
 import org.apache.parquet.hadoop.metadata.CompressionCodecName;
 import org.apache.spark.storage.StorageLevel;
 
@@ -40,6 +39,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Properties;
 
 /**
@@ -759,7 +759,7 @@ public HoodieWriteConfig build() {
 
       // Build WriteConfig at the end
       HoodieWriteConfig config = new HoodieWriteConfig(props);
-      Preconditions.checkArgument(config.getBasePath() != null);
+      Objects.requireNonNull(config.getBasePath());
       return config;
     }
   }
diff --git a/hudi-client/src/main/java/org/apache/hudi/index/bloom/BloomIndexFileInfo.java b/hudi-client/src/main/java/org/apache/hudi/index/bloom/BloomIndexFileInfo.java
index c04181405..11ffb785f 100644
--- a/hudi-client/src/main/java/org/apache/hudi/index/bloom/BloomIndexFileInfo.java
+++ b/hudi-client/src/main/java/org/apache/hudi/index/bloom/BloomIndexFileInfo.java
@@ -18,9 +18,8 @@
 
 package org.apache.hudi.index.bloom;
 
-import com.google.common.base.Objects;
-
 import java.io.Serializable;
+import java.util.Objects;
 
 /**
  * Metadata about a given file group, useful for index lookup.
@@ -65,9 +64,8 @@ public boolean hasKeyRanges() {
    * Does the given key fall within the range (inclusive).
    */
   public boolean isKeyInRange(String recordKey) {
-    assert minRecordKey != null;
-    assert maxRecordKey != null;
-    return minRecordKey.compareTo(recordKey) <= 0 && maxRecordKey.compareTo(recordKey) >= 0;
+    return Objects.requireNonNull(minRecordKey).compareTo(recordKey) <= 0
+        && Objects.requireNonNull(maxRecordKey).compareTo(recordKey) >= 0;
   }
 
   @Override
@@ -80,14 +78,14 @@ public boolean equals(Object o) {
     }
 
     BloomIndexFileInfo that = (BloomIndexFileInfo) o;
-    return Objects.equal(that.fileId, fileId) && Objects.equal(that.minRecordKey, minRecordKey)
-        && Objects.equal(that.maxRecordKey, maxRecordKey);
+    return Objects.equals(that.fileId, fileId) && Objects.equals(that.minRecordKey, minRecordKey)
+        && Objects.equals(that.maxRecordKey, maxRecordKey);
 
   }
 
   @Override
   public int hashCode() {
-    return Objects.hashCode(fileId, minRecordKey, maxRecordKey);
+    return Objects.hash(fileId, minRecordKey, maxRecordKey);
   }
 
   @Override
diff --git a/hudi-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java b/hudi-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java
index 80458460d..17b7506fe 100644
--- a/hudi-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java
+++ b/hudi-client/src/main/java/org/apache/hudi/index/bloom/BucketizedBloomCheckPartitioner.java
@@ -20,7 +20,6 @@
 
 import org.apache.hudi.common.util.collection.Pair;
 
-import com.google.common.annotations.VisibleForTesting;
 import com.google.common.hash.Hashing;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -152,7 +151,6 @@ public int getPartition(Object key) {
     return candidatePartitions.get(idx);
   }
 
-  @VisibleForTesting
   Map<String, List<Integer>> getFileGroupToPartitions() {
     return fileGroupToPartitions;
   }
diff --git a/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
index 03fed3439..68362d683 100644
--- a/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
+++ b/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java
@@ -33,7 +33,6 @@
 import org.apache.hudi.io.HoodieRangeInfoHandle;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.Partitioner;
@@ -234,7 +233,6 @@ private int determineParallelism(int inputParallelism, int totalSubPartitions) {
   /**
    * Load all involved files as <Partition, filename> pair RDD.
    */
-  @VisibleForTesting
   List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final JavaSparkContext jsc,
       final HoodieTable hoodieTable) {
 
@@ -308,7 +306,6 @@ public boolean isImplicitWithStorage() {
    * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on
    * recordKey ranges in the index info.
    */
-  @VisibleForTesting
   JavaRDD<Tuple2<String, HoodieKey>> explodeRecordRDDWithFileComparisons(
       final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,
       JavaPairRDD<String, String> partitionRecordKeyPairRDD) {
@@ -335,7 +332,6 @@ public boolean isImplicitWithStorage() {
    * <p>
    * Make sure the parallelism is atleast the groupby parallelism for tagging location
    */
-  @VisibleForTesting
   JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(
       final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,
       JavaPairRDD<String, String> partitionRecordKeyPairRDD, int shuffleParallelism, HoodieTable hoodieTable,
diff --git a/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java
index be6f52474..ac0e73c7a 100644
--- a/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java
+++ b/hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieGlobalBloomIndex.java
@@ -29,7 +29,6 @@
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.spark.api.java.JavaPairRDD;
 import org.apache.spark.api.java.JavaRDD;
 import org.apache.spark.api.java.JavaSparkContext;
@@ -56,7 +55,6 @@ public HoodieGlobalBloomIndex(HoodieWriteConfig config) {
    * Load all involved files as <Partition, filename> pair RDD from all partitions in the table.
    */
   @Override
-  @VisibleForTesting
   List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final JavaSparkContext jsc,
                                                              final HoodieTable hoodieTable) {
     HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();
@@ -80,7 +78,6 @@ public HoodieGlobalBloomIndex(HoodieWriteConfig config) {
    */
 
   @Override
-  @VisibleForTesting
   JavaRDD<Tuple2<String, HoodieKey>> explodeRecordRDDWithFileComparisons(
       final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,
       JavaPairRDD<String, String> partitionRecordKeyPairRDD) {
diff --git a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java
index 3f7909639..e53a9185a 100644
--- a/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java
+++ b/hudi-client/src/main/java/org/apache/hudi/index/hbase/HBaseIndex.java
@@ -36,7 +36,6 @@
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HRegionLocation;
@@ -114,7 +113,6 @@ private void init(HoodieWriteConfig config) {
     this.hBaseIndexQPSResourceAllocator = createQPSResourceAllocator(this.config);
   }
 
-  @VisibleForTesting
   public HBaseIndexQPSResourceAllocator createQPSResourceAllocator(HoodieWriteConfig config) {
     try {
       LOG.info("createQPSResourceAllocator :" + config.getHBaseQPSResourceAllocatorClass());
@@ -205,9 +203,7 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm
         }
       }
       List<HoodieRecord<T>> taggedRecords = new ArrayList<>();
-      HTable hTable = null;
-      try {
-        hTable = (HTable) hbaseConnection.getTable(TableName.valueOf(tableName));
+      try (HTable hTable = (HTable) hbaseConnection.getTable(TableName.valueOf(tableName))) {
         List<Get> statements = new ArrayList<>();
         List<HoodieRecord> currentBatchOfRecords = new LinkedList<>();
         // Do the tagging.
@@ -250,16 +246,9 @@ private boolean checkIfValidCommit(HoodieTableMetaClient metaClient, String comm
         }
       } catch (IOException e) {
         throw new HoodieIndexException("Failed to Tag indexed locations because of exception with HBase Client", e);
-      } finally {
-        if (hTable != null) {
-          try {
-            hTable.close();
-          } catch (IOException e) {
-            // Ignore
-          }
-        }
-
       }
+      // Ignore
+
       return taggedRecords.iterator();
     };
   }
@@ -398,7 +387,6 @@ private void setPutBatchSize(JavaRDD<WriteStatus> writeStatusRDD,
     }
   }
 
-  @VisibleForTesting
   public Tuple2<Long, Integer> getHBasePutAccessParallelism(final JavaRDD<WriteStatus> writeStatusRDD) {
     final JavaPairRDD<Long, Integer> insertOnlyWriteStatusRDD = writeStatusRDD
         .filter(w -> w.getStat().getNumInserts() > 0).mapToPair(w -> new Tuple2<>(w.getStat().getNumInserts(), 1));
@@ -444,16 +432,12 @@ private void setPutBatchSize(JavaRDD<WriteStatus> writeStatusRDD,
      */
     public int getBatchSize(int numRegionServersForTable, int maxQpsPerRegionServer, int numTasksDuringPut,
         int maxExecutors, int sleepTimeMs, float qpsFraction) {
-      int numRSAlive = numRegionServersForTable;
-      int maxReqPerSec = (int) (qpsFraction * numRSAlive * maxQpsPerRegionServer);
-      int numTasks = numTasksDuringPut;
-      int maxParallelPuts = Math.max(1, Math.min(numTasks, maxExecutors));
+      int maxReqPerSec = (int) (qpsFraction * numRegionServersForTable * maxQpsPerRegionServer);
+      int maxParallelPuts = Math.max(1, Math.min(numTasksDuringPut, maxExecutors));
       int maxReqsSentPerTaskPerSec = MILLI_SECONDS_IN_A_SECOND / sleepTimeMs;
       int multiPutBatchSize = Math.max(1, maxReqPerSec / (maxParallelPuts * maxReqsSentPerTaskPerSec));
       LOG.info("HbaseIndexThrottling: qpsFraction :" + qpsFraction);
-      LOG.info("HbaseIndexThrottling: numRSAlive :" + numRSAlive);
       LOG.info("HbaseIndexThrottling: maxReqPerSec :" + maxReqPerSec);
-      LOG.info("HbaseIndexThrottling: numTasks :" + numTasks);
       LOG.info("HbaseIndexThrottling: maxExecutors :" + maxExecutors);
       LOG.info("HbaseIndexThrottling: maxParallelPuts :" + maxParallelPuts);
       LOG.info("HbaseIndexThrottling: maxReqsSentPerTaskPerSec :" + maxReqsSentPerTaskPerSec);
@@ -510,7 +494,6 @@ public boolean isImplicitWithStorage() {
     return false;
   }
 
-  @VisibleForTesting
   public void setHbaseConnection(Connection hbaseConnection) {
     HBaseIndex.hbaseConnection = hbaseConnection;
   }
diff --git a/hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java b/hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
index e2dbf6403..de48ca5b3 100644
--- a/hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
+++ b/hudi-client/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java
@@ -44,7 +44,6 @@
 import org.apache.hudi.exception.HoodieUpsertException;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.collect.Maps;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.Path;
@@ -55,6 +54,7 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
@@ -95,7 +95,7 @@
   // Max block size to limit to for a log block
   private int maxBlockSize = config.getLogFileDataBlockMaxSize();
   // Header metadata for a log block
-  private Map<HeaderMetadataType, String> header = Maps.newHashMap();
+  private Map<HeaderMetadataType, String> header = new HashMap<>();
   // Total number of new records inserted into the delta file
   private long insertRecordsWritten = 0;
 
diff --git a/hudi-client/src/main/java/org/apache/hudi/io/HoodieCommitArchiveLog.java b/hudi-client/src/main/java/org/apache/hudi/io/HoodieCommitArchiveLog.java
index bafbc8dfa..6757abf72 100644
--- a/hudi-client/src/main/java/org/apache/hudi/io/HoodieCommitArchiveLog.java
+++ b/hudi-client/src/main/java/org/apache/hudi/io/HoodieCommitArchiveLog.java
@@ -49,8 +49,6 @@
 
 import com.fasterxml.jackson.databind.DeserializationFeature;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.collect.Maps;
-import com.google.common.collect.Sets;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.Path;
@@ -62,6 +60,7 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Comparator;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
@@ -145,11 +144,11 @@ public boolean archiveIfRequired(final JavaSparkContext jsc) throws IOException
     // TODO: Handle ROLLBACK_ACTION in future
     // ROLLBACK_ACTION is currently not defined in HoodieActiveTimeline
     HoodieTimeline cleanAndRollbackTimeline = table.getActiveTimeline()
-        .getTimelineOfActions(Sets.newHashSet(HoodieTimeline.CLEAN_ACTION)).filterCompletedInstants();
+        .getTimelineOfActions(Stream.of(HoodieTimeline.CLEAN_ACTION).collect(Collectors.toSet())).filterCompletedInstants();
     Stream<HoodieInstant> instants = cleanAndRollbackTimeline.getInstants()
-        .collect(Collectors.groupingBy(s -> s.getAction())).entrySet().stream().map(i -> {
-          if (i.getValue().size() > maxCommitsToKeep) {
-            return i.getValue().subList(0, i.getValue().size() - minCommitsToKeep);
+        .collect(Collectors.groupingBy(HoodieInstant::getAction)).values().stream().map(hoodieInstants -> {
+          if (hoodieInstants.size() > maxCommitsToKeep) {
+            return hoodieInstants.subList(0, hoodieInstants.size() - minCommitsToKeep);
           } else {
             return new ArrayList<HoodieInstant>();
           }
@@ -270,7 +269,7 @@ public Path getArchiveFilePath() {
 
   private void writeToFile(Schema wrapperSchema, List<IndexedRecord> records) throws Exception {
     if (records.size() > 0) {
-      Map<HeaderMetadataType, String> header = Maps.newHashMap();
+      Map<HeaderMetadataType, String> header = new HashMap<>();
       header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, wrapperSchema.toString());
       HoodieAvroDataBlock block = new HoodieAvroDataBlock(records, header);
       this.writer = writer.appendBlock(block);
diff --git a/hudi-client/src/main/java/org/apache/hudi/io/compact/HoodieRealtimeTableCompactor.java b/hudi-client/src/main/java/org/apache/hudi/io/compact/HoodieRealtimeTableCompactor.java
index 8ad6f870a..5c0282477 100644
--- a/hudi-client/src/main/java/org/apache/hudi/io/compact/HoodieRealtimeTableCompactor.java
+++ b/hudi-client/src/main/java/org/apache/hudi/io/compact/HoodieRealtimeTableCompactor.java
@@ -35,15 +35,13 @@
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.HoodieAvroUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.compact.strategy.CompactionStrategy;
 import org.apache.hudi.table.HoodieCopyOnWriteTable;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Sets;
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -56,11 +54,13 @@
 import org.apache.spark.util.LongAccumulator;
 
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 import java.util.stream.Collectors;
+import java.util.stream.Stream;
 import java.util.stream.StreamSupport;
 
 import static java.util.stream.Collectors.toList;
@@ -112,8 +112,8 @@
     // loaded and load it using CompositeAvroLogReader
     // Since a DeltaCommit is not defined yet, reading all the records. revisit this soon.
     String maxInstantTime = metaClient
-        .getActiveTimeline().getTimelineOfActions(Sets.newHashSet(HoodieTimeline.COMMIT_ACTION,
-            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))
+        .getActiveTimeline().getTimelineOfActions(Stream.of(HoodieTimeline.COMMIT_ACTION,
+            HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION).collect(Collectors.toSet()))
         .filterCompletedInstants().lastInstant().get().getTimestamp();
     LOG.info("MaxMemoryPerCompaction => " + config.getMaxMemoryPerCompaction());
 
@@ -125,7 +125,7 @@
         config.getCompactionReverseLogReadEnabled(), config.getMaxDFSStreamBufferSize(),
         config.getSpillableMapBasePath());
     if (!scanner.iterator().hasNext()) {
-      return Lists.<WriteStatus>newArrayList();
+      return new ArrayList<>();
     }
 
     Option<HoodieDataFile> oldDataFileOpt =
@@ -169,7 +169,7 @@ public HoodieCompactionPlan generateCompactionPlan(JavaSparkContext jsc, HoodieT
     jsc.sc().register(totalLogFiles);
     jsc.sc().register(totalFileSlices);
 
-    Preconditions.checkArgument(hoodieTable.getMetaClient().getTableType() == HoodieTableType.MERGE_ON_READ,
+    ValidationUtils.checkArgument(hoodieTable.getMetaClient().getTableType() == HoodieTableType.MERGE_ON_READ,
         "HoodieRealtimeTableCompactor can only compact table of type " + HoodieTableType.MERGE_ON_READ + " and not "
             + hoodieTable.getMetaClient().getTableType().name());
 
@@ -214,7 +214,7 @@ public HoodieCompactionPlan generateCompactionPlan(JavaSparkContext jsc, HoodieT
     // compactions only
     HoodieCompactionPlan compactionPlan = config.getCompactionStrategy().generateCompactionPlan(config, operations,
         CompactionUtils.getAllPendingCompactionPlans(metaClient).stream().map(Pair::getValue).collect(toList()));
-    Preconditions.checkArgument(
+    ValidationUtils.checkArgument(
         compactionPlan.getOperations().stream().noneMatch(
             op -> fgIdsInPendingCompactions.contains(new HoodieFileGroupId(op.getPartitionPath(), op.getFileId()))),
         "Bad Compaction Plan. FileId MUST NOT have multiple pending compactions. "
diff --git a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedIOCompactionStrategy.java b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedIOCompactionStrategy.java
index c84df1b82..8976d2286 100644
--- a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedIOCompactionStrategy.java
+++ b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedIOCompactionStrategy.java
@@ -22,8 +22,7 @@
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.config.HoodieWriteConfig;
 
-import com.google.common.collect.Lists;
-
+import java.util.ArrayList;
 import java.util.List;
 
 /**
@@ -40,7 +39,7 @@
     // Iterate through the operations in order and accept operations as long as we are within the
     // IO limit
     // Preserves the original ordering of compactions
-    List<HoodieCompactionOperation> finalOperations = Lists.newArrayList();
+    List<HoodieCompactionOperation> finalOperations = new ArrayList<>();
     long targetIORemaining = writeConfig.getTargetIOPerCompactionInMB();
     for (HoodieCompactionOperation op : operations) {
       long opIo = op.getMetrics().get(TOTAL_IO_MB).longValue();
diff --git a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedPartitionAwareCompactionStrategy.java b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedPartitionAwareCompactionStrategy.java
index 5b64cedb1..6ba82132d 100644
--- a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedPartitionAwareCompactionStrategy.java
+++ b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/BoundedPartitionAwareCompactionStrategy.java
@@ -22,8 +22,6 @@
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.config.HoodieWriteConfig;
 
-import com.google.common.annotations.VisibleForTesting;
-
 import java.text.SimpleDateFormat;
 import java.util.Calendar;
 import java.util.Comparator;
@@ -68,7 +66,6 @@
         .filter(e -> comparator.compare(earliestPartitionPathToCompact, e) >= 0).collect(Collectors.toList());
   }
 
-  @VisibleForTesting
   public static Date getDateAtOffsetFromToday(int offset) {
     Calendar calendar = Calendar.getInstance();
     calendar.add(Calendar.DATE, offset);
diff --git a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/CompactionStrategy.java b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/CompactionStrategy.java
index 79a14b64c..e69f8b4f8 100644
--- a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/CompactionStrategy.java
+++ b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/CompactionStrategy.java
@@ -28,9 +28,8 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.io.compact.HoodieRealtimeTableCompactor;
 
-import com.google.common.collect.Maps;
-
 import java.io.Serializable;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -61,11 +60,11 @@
    */
   public Map<String, Double> captureMetrics(HoodieWriteConfig writeConfig, Option<HoodieDataFile> dataFile,
       String partitionPath, List<HoodieLogFile> logFiles) {
-    Map<String, Double> metrics = Maps.newHashMap();
+    Map<String, Double> metrics = new HashMap<>();
     Long defaultMaxParquetFileSize = writeConfig.getParquetMaxFileSize();
     // Total size of all the log files
     Long totalLogFileSize = logFiles.stream().map(HoodieLogFile::getFileSize).filter(size -> size >= 0)
-        .reduce((size1, size2) -> size1 + size2).orElse(0L);
+        .reduce(Long::sum).orElse(0L);
     // Total read will be the base file + all the log files
     Long totalIORead =
         FSUtils.getSizeInMB((dataFile.isPresent() ? dataFile.get().getFileSize() : 0L) + totalLogFileSize);
diff --git a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/DayBasedCompactionStrategy.java b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/DayBasedCompactionStrategy.java
index a491818d1..9d537763b 100644
--- a/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/DayBasedCompactionStrategy.java
+++ b/hudi-client/src/main/java/org/apache/hudi/io/compact/strategy/DayBasedCompactionStrategy.java
@@ -23,8 +23,6 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 
-import com.google.common.annotations.VisibleForTesting;
-
 import java.text.ParseException;
 import java.text.SimpleDateFormat;
 import java.util.Comparator;
@@ -55,7 +53,6 @@
     }
   };
 
-  @VisibleForTesting
   public Comparator<String> getComparator() {
     return comparator;
   }
diff --git a/hudi-client/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java b/hudi-client/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java
index b6fcd09e8..4b2b48b01 100644
--- a/hudi-client/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java
+++ b/hudi-client/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java
@@ -23,7 +23,6 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 
 import com.codahale.metrics.Timer;
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -187,7 +186,6 @@ public void updateIndexMetrics(final String action, final long durationInMs) {
     }
   }
 
-  @VisibleForTesting
   String getMetricsName(String action, String metric) {
     return config == null ? null : String.format("%s.%s.%s", tableName, action, metric);
   }
diff --git a/hudi-client/src/main/java/org/apache/hudi/metrics/JmxMetricsReporter.java b/hudi-client/src/main/java/org/apache/hudi/metrics/JmxMetricsReporter.java
index 2559a4b4c..921dcea07 100644
--- a/hudi-client/src/main/java/org/apache/hudi/metrics/JmxMetricsReporter.java
+++ b/hudi-client/src/main/java/org/apache/hudi/metrics/JmxMetricsReporter.java
@@ -21,7 +21,6 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 
-import com.google.common.base.Preconditions;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -32,6 +31,7 @@
 import java.io.Closeable;
 import java.lang.management.ManagementFactory;
 import java.rmi.registry.LocateRegistry;
+import java.util.Objects;
 
 /**
  * Implementation of Jmx reporter, which used to report jmx metric.
@@ -67,7 +67,7 @@ public JmxMetricsReporter(HoodieWriteConfig config) {
   @Override
   public void start() {
     try {
-      Preconditions.checkNotNull(connector, "Cannot start as the jmxReporter is null.");
+      Objects.requireNonNull(connector, "Cannot start as the jmxReporter is null.");
       connector.start();
     } catch (Exception e) {
       throw new HoodieException(e);
diff --git a/hudi-client/src/main/java/org/apache/hudi/metrics/Metrics.java b/hudi-client/src/main/java/org/apache/hudi/metrics/Metrics.java
index 4b194416a..3e4a02e9b 100644
--- a/hudi-client/src/main/java/org/apache/hudi/metrics/Metrics.java
+++ b/hudi-client/src/main/java/org/apache/hudi/metrics/Metrics.java
@@ -18,12 +18,12 @@
 
 package org.apache.hudi.metrics;
 
+import org.apache.hudi.common.util.FileIOUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieException;
 
 import com.codahale.metrics.Gauge;
 import com.codahale.metrics.MetricRegistry;
-import com.google.common.io.Closeables;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -49,17 +49,14 @@ private Metrics(HoodieWriteConfig metricConfig) {
     }
     // reporter.start();
 
-    Runtime.getRuntime().addShutdownHook(new Thread() {
-      @Override
-      public void run() {
-        try {
-          reporter.report();
-          Closeables.close(reporter.getReporter(), true);
-        } catch (Exception e) {
-          e.printStackTrace();
-        }
+    Runtime.getRuntime().addShutdownHook(new Thread(() -> {
+      try {
+        reporter.report();
+        FileIOUtils.close(reporter.getReporter(), true);
+      } catch (Exception e) {
+        e.printStackTrace();
       }
-    });
+    }));
   }
 
   public static Metrics getInstance() {
diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java
index f1f277bad..565e95ade 100644
--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java
+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java
@@ -680,7 +680,6 @@ private void assignInserts(WorkloadProfile profile) {
 
       // smallFiles only for partitionPath
       List<SmallFile> smallFileLocations = new ArrayList<>();
-
       HoodieTimeline commitTimeline = getCompletedCommitsTimeline();
 
       if (!commitTimeline.empty()) { // if we have some commits
diff --git a/hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java b/hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java
index a654fcbf2..cc6bafa05 100644
--- a/hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java
+++ b/hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java
@@ -34,6 +34,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieCompactionException;
 import org.apache.hudi.exception.HoodieIOException;
@@ -42,8 +43,6 @@
 import org.apache.hudi.io.HoodieAppendHandle;
 import org.apache.hudi.io.compact.HoodieRealtimeTableCompactor;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.apache.spark.Partitioner;
@@ -408,7 +407,6 @@ private boolean isSmallFile(FileSlice fileSlice) {
     }
 
     // TODO (NA) : Make this static part of utility
-    @VisibleForTesting
     public long convertLogFilesSizeToExpectedParquetSize(List<HoodieLogFile> hoodieLogFiles) {
       long totalSizeOfLogFiles = hoodieLogFiles.stream().map(HoodieLogFile::getFileSize)
           .filter(size -> size > 0).reduce(Long::sum).orElse(0L);
@@ -421,7 +419,7 @@ public long convertLogFilesSizeToExpectedParquetSize(List<HoodieLogFile> hoodieL
 
   private List<RollbackRequest> generateAppendRollbackBlocksAction(String partitionPath, HoodieInstant rollbackInstant,
       HoodieCommitMetadata commitMetadata) {
-    Preconditions.checkArgument(rollbackInstant.getAction().equals(HoodieTimeline.DELTA_COMMIT_ACTION));
+    ValidationUtils.checkArgument(rollbackInstant.getAction().equals(HoodieTimeline.DELTA_COMMIT_ACTION));
 
     // wStat.getPrevCommit() might not give the right commit time in the following
     // scenario : If a compaction was scheduled, the new commitTime associated with the requested compaction will be
@@ -438,7 +436,7 @@ public long convertLogFilesSizeToExpectedParquetSize(List<HoodieLogFile> hoodieL
 
       if (validForRollback) {
         // For sanity, log instant time can never be less than base-commit on which we are rolling back
-        Preconditions
+        ValidationUtils
             .checkArgument(HoodieTimeline.compareTimestamps(fileIdToBaseCommitTimeForLogMap.get(wStat.getFileId()),
                 rollbackInstant.getTimestamp(), HoodieTimeline.LESSER_OR_EQUAL));
       }
diff --git a/hudi-client/src/main/java/org/apache/hudi/table/RollbackExecutor.java b/hudi-client/src/main/java/org/apache/hudi/table/RollbackExecutor.java
index 0f3297ca3..e78c9cfb3 100644
--- a/hudi-client/src/main/java/org/apache/hudi/table/RollbackExecutor.java
+++ b/hudi-client/src/main/java/org/apache/hudi/table/RollbackExecutor.java
@@ -28,11 +28,10 @@
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.exception.HoodieRollbackException;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.Maps;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.PathFilter;
@@ -47,6 +46,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 
 import scala.Tuple2;
 
@@ -129,7 +129,7 @@ public RollbackExecutor(HoodieTableMetaClient metaClient, HoodieWriteConfig conf
           // getFileStatus would reflect correct stats and FileNotFoundException is not thrown in
           // cloud-storage : HUDI-168
           Map<FileStatus, Long> filesToNumBlocksRollback = new HashMap<>();
-          filesToNumBlocksRollback.put(metaClient.getFs().getFileStatus(Preconditions.checkNotNull(writer).getLogFile().getPath()), 1L);
+          filesToNumBlocksRollback.put(metaClient.getFs().getFileStatus(Objects.requireNonNull(writer).getLogFile().getPath()), 1L);
           return new Tuple2<>(rollbackRequest.getPartitionPath(),
                   HoodieRollbackStat.newBuilder().withPartitionPath(rollbackRequest.getPartitionPath())
                           .withRollbackBlockAppendResults(filesToNumBlocksRollback).build());
@@ -148,7 +148,7 @@ public RollbackExecutor(HoodieTableMetaClient metaClient, HoodieWriteConfig conf
    * @return Merged HoodieRollbackStat
    */
   private HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRollbackStat stat2) {
-    Preconditions.checkArgument(stat1.getPartitionPath().equals(stat2.getPartitionPath()));
+    ValidationUtils.checkArgument(stat1.getPartitionPath().equals(stat2.getPartitionPath()));
     final List<String> successDeleteFiles = new ArrayList<>();
     final List<String> failedDeleteFiles = new ArrayList<>();
     final Map<FileStatus, Long> commandBlocksCount = new HashMap<>();
@@ -215,7 +215,7 @@ private HoodieRollbackStat mergeRollbackStat(HoodieRollbackStat stat1, HoodieRol
 
   private Map<HeaderMetadataType, String> generateHeader(String commit) {
     // generate metadata
-    Map<HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HeaderMetadataType, String> header = new HashMap<>();
     header.put(HeaderMetadataType.INSTANT_TIME, metaClient.getActiveTimeline().lastInstant().get().getTimestamp());
     header.put(HeaderMetadataType.TARGET_INSTANT_TIME, commit);
     header.put(HeaderMetadataType.COMMAND_BLOCK_TYPE,
diff --git a/hudi-client/src/test/java/org/apache/hudi/TestCleaner.java b/hudi-client/src/test/java/org/apache/hudi/TestCleaner.java
index baec87508..128e78ab2 100644
--- a/hudi-client/src/test/java/org/apache/hudi/TestCleaner.java
+++ b/hudi-client/src/test/java/org/apache/hudi/TestCleaner.java
@@ -41,6 +41,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.util.AvroUtils;
 import org.apache.hudi.common.util.CleanerUtils;
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.ConsistencyGuardConfig;
 import org.apache.hudi.common.util.FSUtils;
@@ -52,9 +53,7 @@
 import org.apache.hudi.index.HoodieIndex;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Iterables;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.RemoteIterator;
 import org.apache.log4j.LogManager;
@@ -68,6 +67,7 @@
 import java.nio.file.Paths;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -286,7 +286,7 @@ private void testInsertAndCleanByVersions(
                 List<String> commitedVersions = new ArrayList<>(fileIdToVersions.get(fileId));
                 for (int i = 0; i < dataFiles.size(); i++) {
                   assertEquals("File " + fileId + " does not have latest versions on commits" + commitedVersions,
-                      Iterables.get(dataFiles, i).getCommitTime(),
+                      (dataFiles.get(i)).getCommitTime(),
                       commitedVersions.get(commitedVersions.size() - 1 - i));
                 }
               }
@@ -606,8 +606,8 @@ public void testUpgradeDowngrade() {
     String filePath2 = metaClient.getBasePath() + "/" + partition1 + "/" + fileName2;
 
     List<String> deletePathPatterns1 = Arrays.asList(filePath1, filePath2);
-    List<String> successDeleteFiles1 = Arrays.asList(filePath1);
-    List<String> failedDeleteFiles1 = Arrays.asList(filePath2);
+    List<String> successDeleteFiles1 = Collections.singletonList(filePath1);
+    List<String> failedDeleteFiles1 = Collections.singletonList(filePath2);
 
     // create partition1 clean stat.
     HoodieCleanStat cleanStat1 = new HoodieCleanStat(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS,
@@ -630,7 +630,8 @@ public void testUpgradeDowngrade() {
 
     // map with relative path.
     Map<String, Tuple3> newExpected = new HashMap<>();
-    newExpected.put(partition1, new Tuple3<>(Arrays.asList(fileName1, fileName2), Arrays.asList(fileName1), Arrays.asList(fileName2)));
+    newExpected.put(partition1, new Tuple3<>(Arrays.asList(fileName1, fileName2), Collections.singletonList(fileName1),
+        Collections.singletonList(fileName2)));
     newExpected.put(partition2, new Tuple3<>(deletePathPatterns2, successDeleteFiles2, failedDeleteFiles2));
 
     HoodieCleanMetadata metadata =
@@ -736,10 +737,8 @@ private void testKeepLatestCommits(boolean simulateFailureRetry, boolean enableI
         HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, "000");
 
     HoodieCommitMetadata commitMetadata = generateCommitMetadata(new ImmutableMap.Builder()
-        .put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,
-            new ImmutableList.Builder<>().add(file1P0C0).build())
-        .put(HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH,
-            new ImmutableList.Builder<>().add(file1P1C0).build())
+        .put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, CollectionUtils.createImmutableList(file1P0C0))
+        .put(HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, CollectionUtils.createImmutableList(file1P1C0))
         .build());
     metaClient.getActiveTimeline().saveAsComplete(
         new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, "000"),
@@ -774,10 +773,8 @@ private void testKeepLatestCommits(boolean simulateFailureRetry, boolean enableI
     HoodieTestUtils
         .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, "001", file1P1C0); // update
     commitMetadata = generateCommitMetadata(new ImmutableMap.Builder()
-        .put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,
-            new ImmutableList.Builder<>().add(file1P0C0).add(file2P0C1).build())
-        .put(HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH,
-            new ImmutableList.Builder<>().add(file1P1C0).add(file2P1C1).build())
+        .put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, CollectionUtils.createImmutableList(file1P0C0, file2P0C1))
+        .put(HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH, CollectionUtils.createImmutableList(file1P1C0, file2P1C1))
         .build());
     metaClient.getActiveTimeline().saveAsComplete(
         new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, "001"),
@@ -809,10 +806,9 @@ private void testKeepLatestCommits(boolean simulateFailureRetry, boolean enableI
     String file3P0C2 =
         HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, "002");
 
-    commitMetadata = generateCommitMetadata(new ImmutableMap.Builder()
-        .put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,
-            new ImmutableList.Builder<>().add(file1P0C0).add(file2P0C1).add(file3P0C2).build())
-        .build());
+    commitMetadata = generateCommitMetadata(CollectionUtils
+        .createImmutableMap(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,
+            CollectionUtils.createImmutableList(file1P0C0, file2P0C1, file3P0C2)));
     metaClient.getActiveTimeline().saveAsComplete(
         new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, "002"),
         Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
@@ -835,10 +831,8 @@ private void testKeepLatestCommits(boolean simulateFailureRetry, boolean enableI
         .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, "003", file2P0C1); // update
     String file4P0C3 =
         HoodieTestUtils.createNewDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, "003");
-    commitMetadata = generateCommitMetadata(new ImmutableMap.Builder()
-        .put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,
-            new ImmutableList.Builder<>().add(file1P0C0).add(file2P0C1).add(file4P0C3).build())
-        .build());
+    commitMetadata = generateCommitMetadata(CollectionUtils.createImmutableMap(
+        HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, CollectionUtils.createImmutableList(file1P0C0, file2P0C1, file4P0C3)));
     metaClient.getActiveTimeline().saveAsComplete(
         new HoodieInstant(State.INFLIGHT, HoodieTimeline.COMMIT_ACTION, "003"),
         Option.of(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
@@ -866,10 +860,8 @@ private void testKeepLatestCommits(boolean simulateFailureRetry, boolean enableI
     // No cleaning on partially written file, with no commit.
     HoodieTestUtils
         .createDataFile(basePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH, "004", file3P0C2); // update
-    commitMetadata = generateCommitMetadata(new ImmutableMap.Builder()
-        .put(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,
-            new ImmutableList.Builder<>().add(file3P0C2).build())
-        .build());
+    commitMetadata = generateCommitMetadata(CollectionUtils.createImmutableMap(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,
+            CollectionUtils.createImmutableList(file3P0C2)));
     metaClient.getActiveTimeline().createNewInstant(
         new HoodieInstant(State.REQUESTED, HoodieTimeline.COMMIT_ACTION, "004"));
     metaClient.getActiveTimeline().transitionRequestedToInflight(
@@ -1123,7 +1115,7 @@ private void testPendingCompactions(HoodieWriteConfig config, int expNumFilesDel
    * @throws IOException in case of error
    */
   private int getTotalTempFiles() throws IOException {
-    RemoteIterator itr = fs.listFiles(new Path(basePath, HoodieTableMetaClient.TEMPFOLDER_NAME), true);
+    RemoteIterator<?> itr = fs.listFiles(new Path(basePath, HoodieTableMetaClient.TEMPFOLDER_NAME), true);
     int count = 0;
     while (itr.hasNext()) {
       count++;
diff --git a/hudi-client/src/test/java/org/apache/hudi/TestClientRollback.java b/hudi-client/src/test/java/org/apache/hudi/TestClientRollback.java
index abefe86e5..0e18c7f0c 100644
--- a/hudi-client/src/test/java/org/apache/hudi/TestClientRollback.java
+++ b/hudi-client/src/test/java/org/apache/hudi/TestClientRollback.java
@@ -61,7 +61,7 @@ public void testSavepointAndRollback() throws Exception {
     try (HoodieWriteClient client = getHoodieWriteClient(cfg);) {
       HoodieTestDataGenerator.writePartitionMetadata(fs, HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS, basePath);
 
-      /**
+      /*
        * Write 1 (only inserts)
        */
       String newCommitTime = "001";
@@ -73,7 +73,7 @@ public void testSavepointAndRollback() throws Exception {
       List<WriteStatus> statuses = client.upsert(writeRecords, newCommitTime).collect();
       assertNoWriteErrors(statuses);
 
-      /**
+      /*
        * Write 2 (updates)
        */
       newCommitTime = "002";
@@ -86,7 +86,7 @@ public void testSavepointAndRollback() throws Exception {
 
       client.savepoint("hoodie-unit-test", "test");
 
-      /**
+      /*
        * Write 3 (updates)
        */
       newCommitTime = "003";
@@ -112,7 +112,7 @@ public void testSavepointAndRollback() throws Exception {
       }).collect(Collectors.toList());
       assertEquals("The data files for commit 002 should be present", 3, dataFiles.size());
 
-      /**
+      /*
        * Write 4 (updates)
        */
       newCommitTime = "004";
@@ -145,14 +145,10 @@ public void testSavepointAndRollback() throws Exception {
       metaClient = HoodieTableMetaClient.reload(metaClient);
       table = HoodieTable.getHoodieTable(metaClient, getConfig(), jsc);
       final ReadOptimizedView view3 = table.getROFileSystemView();
-      dataFiles = partitionPaths.stream().flatMap(s -> {
-        return view3.getAllDataFiles(s).filter(f -> f.getCommitTime().equals("002"));
-      }).collect(Collectors.toList());
+      dataFiles = partitionPaths.stream().flatMap(s -> view3.getAllDataFiles(s).filter(f -> f.getCommitTime().equals("002"))).collect(Collectors.toList());
       assertEquals("The data files for commit 002 be available", 3, dataFiles.size());
 
-      dataFiles = partitionPaths.stream().flatMap(s -> {
-        return view3.getAllDataFiles(s).filter(f -> f.getCommitTime().equals("003"));
-      }).collect(Collectors.toList());
+      dataFiles = partitionPaths.stream().flatMap(s -> view3.getAllDataFiles(s).filter(f -> f.getCommitTime().equals("003"))).collect(Collectors.toList());
       assertEquals("The data files for commit 003 should be rolled back", 0, dataFiles.size());
 
       dataFiles = partitionPaths.stream().flatMap(s -> {
diff --git a/hudi-client/src/test/java/org/apache/hudi/TestCompactionAdminClient.java b/hudi-client/src/test/java/org/apache/hudi/TestCompactionAdminClient.java
index 2ce452ad3..664b87c02 100644
--- a/hudi-client/src/test/java/org/apache/hudi/TestCompactionAdminClient.java
+++ b/hudi-client/src/test/java/org/apache/hudi/TestCompactionAdminClient.java
@@ -32,8 +32,8 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
-
 import org.apache.hudi.func.OperationResult;
+
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 import org.junit.After;
diff --git a/hudi-client/src/test/java/org/apache/hudi/common/HoodieTestDataGenerator.java b/hudi-client/src/test/java/org/apache/hudi/common/HoodieTestDataGenerator.java
index d484b60b9..4c697f0cf 100644
--- a/hudi-client/src/test/java/org/apache/hudi/common/HoodieTestDataGenerator.java
+++ b/hudi-client/src/test/java/org/apache/hudi/common/HoodieTestDataGenerator.java
@@ -209,13 +209,10 @@ public static void createCompactionAuxiliaryMetadata(String basePath, HoodieInst
     Path commitFile =
         new Path(basePath + "/" + HoodieTableMetaClient.AUXILIARYFOLDER_NAME + "/" + instant.getFileName());
     FileSystem fs = FSUtils.getFs(basePath, configuration);
-    FSDataOutputStream os = fs.create(commitFile, true);
-    HoodieCompactionPlan workload = new HoodieCompactionPlan();
-    try {
+    try (FSDataOutputStream os = fs.create(commitFile, true)) {
+      HoodieCompactionPlan workload = new HoodieCompactionPlan();
       // Write empty commit metadata
       os.writeBytes(new String(AvroUtils.serializeCompactionPlan(workload).get(), StandardCharsets.UTF_8));
-    } finally {
-      os.close();
     }
   }
 
@@ -224,13 +221,10 @@ public static void createSavepointFile(String basePath, String commitTime, Confi
     Path commitFile = new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
         + HoodieTimeline.makeSavePointFileName(commitTime));
     FileSystem fs = FSUtils.getFs(basePath, configuration);
-    FSDataOutputStream os = fs.create(commitFile, true);
-    HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();
-    try {
+    try (FSDataOutputStream os = fs.create(commitFile, true)) {
+      HoodieCommitMetadata commitMetadata = new HoodieCommitMetadata();
       // Write empty commit metadata
       os.writeBytes(new String(commitMetadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
-    } finally {
-      os.close();
     }
   }
 
diff --git a/hudi-client/src/test/java/org/apache/hudi/config/TestHoodieWriteConfig.java b/hudi-client/src/test/java/org/apache/hudi/config/TestHoodieWriteConfig.java
index e2d2a6c28..80b59a006 100644
--- a/hudi-client/src/test/java/org/apache/hudi/config/TestHoodieWriteConfig.java
+++ b/hudi-client/src/test/java/org/apache/hudi/config/TestHoodieWriteConfig.java
@@ -20,13 +20,13 @@
 
 import org.apache.hudi.config.HoodieWriteConfig.Builder;
 
-import com.google.common.collect.Maps;
 import org.junit.Test;
 
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
 import java.util.Date;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.Properties;
 
@@ -37,7 +37,7 @@
   @Test
   public void testPropertyLoading() throws IOException {
     Builder builder = HoodieWriteConfig.newBuilder().withPath("/tmp");
-    Map<String, String> params = Maps.newHashMap();
+    Map<String, String> params = new HashMap<>();
     params.put(HoodieCompactionConfig.CLEANER_COMMITS_RETAINED_PROP, "1");
     params.put(HoodieCompactionConfig.MAX_COMMITS_TO_KEEP_PROP, "5");
     params.put(HoodieCompactionConfig.MIN_COMMITS_TO_KEEP_PROP, "2");
diff --git a/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java b/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java
index c121c1436..7a8deda26 100644
--- a/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java
+++ b/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieBloomIndex.java
@@ -37,7 +37,6 @@
 import org.apache.hudi.io.HoodieKeyLookupHandle;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.collect.Lists;
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.Path;
 import org.apache.spark.api.java.JavaPairRDD;
@@ -50,8 +49,10 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collection;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -148,11 +149,11 @@ public void testLoadInvolvedFiles() throws IOException {
     HoodieRecord record4 =
         new HoodieRecord(new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath()), rowChange4);
 
-    HoodieClientTestUtils.writeParquetFile(basePath, "2016/04/01", "2_0_20160401010101.parquet", Lists.newArrayList(),
+    HoodieClientTestUtils.writeParquetFile(basePath, "2016/04/01", "2_0_20160401010101.parquet", new ArrayList<>(),
         schema, null, false);
-    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "1_0_20150312101010.parquet", Lists.newArrayList(),
+    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "1_0_20150312101010.parquet", new ArrayList<>(),
         schema, null, false);
-    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "3_0_20150312101010.parquet", Arrays.asList(record1),
+    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "3_0_20150312101010.parquet", Collections.singletonList(record1),
         schema, null, false);
     HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "4_0_20150312101010.parquet",
         Arrays.asList(record2, record3, record4), schema, null, false);
@@ -352,14 +353,14 @@ public void testTagLocation() throws Exception {
     for (HoodieRecord record : taggedRecordRDD.collect()) {
       if (record.getRecordKey().equals(rowKey1)) {
         if (record.getPartitionPath().equals("2015/01/31")) {
-          assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename3)));
+          assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename3));
         } else {
-          assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename1)));
+          assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename1));
         }
       } else if (record.getRecordKey().equals(rowKey2)) {
-        assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename2)));
+        assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename2));
       } else if (record.getRecordKey().equals(rowKey3)) {
-        assertTrue(!record.isCurrentLocationKnown());
+        assertFalse(record.isCurrentLocationKnown());
       }
     }
   }
@@ -407,11 +408,11 @@ public void testCheckExists() throws Exception {
 
     // We create three parquet file, each having one record. (two different partitions)
     String filename1 =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2016/01/31", Arrays.asList(record1), schema, null, true);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2016/01/31", Collections.singletonList(record1), schema, null, true);
     String filename2 =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2016/01/31", Arrays.asList(record2), schema, null, true);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2016/01/31", Collections.singletonList(record2), schema, null, true);
     String filename3 =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2015/01/31", Arrays.asList(record4), schema, null, true);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2015/01/31", Collections.singletonList(record4), schema, null, true);
 
     // We do the tag again
     metaClient = HoodieTableMetaClient.reload(metaClient);
@@ -431,7 +432,7 @@ public void testCheckExists() throws Exception {
           assertEquals(FSUtils.getFileId(filename2), record._2.get().getRight());
         }
       } else if (record._1.getRecordKey().equals("3eb5b87c-1fej-4edd-87b4-6ec96dc405a0")) {
-        assertTrue(!record._2.isPresent());
+        assertFalse(record._2.isPresent());
       }
     }
   }
@@ -456,7 +457,7 @@ public void testBloomFilterFalseError() throws IOException, InterruptedException
         BloomFilterTypeCode.SIMPLE.name());
     filter.add(record2.getRecordKey());
     String filename =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2016/01/31", Arrays.asList(record1), schema, filter, true);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2016/01/31", Collections.singletonList(record1), schema, filter, true);
     assertTrue(filter.mightContain(record1.getRecordKey()));
     assertTrue(filter.mightContain(record2.getRecordKey()));
 
@@ -472,7 +473,7 @@ public void testBloomFilterFalseError() throws IOException, InterruptedException
     // Check results
     for (HoodieRecord record : taggedRecordRDD.collect()) {
       if (record.getKey().equals("1eb5b87a-1feh-4edd-87b4-6ec96dc405a0")) {
-        assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename)));
+        assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename));
       } else if (record.getRecordKey().equals("2eb5b87b-1feu-4edd-87b4-6ec96dc405a0")) {
         assertFalse(record.isCurrentLocationKnown());
       }
diff --git a/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java b/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
index 15a8af70a..79488e323 100644
--- a/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
+++ b/hudi-client/src/test/java/org/apache/hudi/index/bloom/TestHoodieGlobalBloomIndex.java
@@ -31,7 +31,6 @@
 import org.apache.hudi.config.HoodieWriteConfig;
 import org.apache.hudi.table.HoodieTable;
 
-import com.google.common.collect.Lists;
 import org.apache.avro.Schema;
 import org.apache.spark.api.java.JavaPairRDD;
 import org.apache.spark.api.java.JavaRDD;
@@ -41,7 +40,9 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -58,7 +59,6 @@
 
 public class TestHoodieGlobalBloomIndex extends HoodieClientTestHarness {
 
-  private String schemaStr;
   private Schema schema;
 
   public TestHoodieGlobalBloomIndex() {
@@ -69,7 +69,7 @@ public void setUp() throws Exception {
     initSparkContexts("TestHoodieGlobalBloomIndex");
     initPath();
     // We have some records to be tagged (two different partitions)
-    schemaStr = FileIOUtils.readAsUTFString(getClass().getResourceAsStream("/exampleSchema.txt"));
+    String schemaStr = FileIOUtils.readAsUTFString(getClass().getResourceAsStream("/exampleSchema.txt"));
     schema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(schemaStr));
     initMetaClient();
   }
@@ -114,11 +114,11 @@ public void testLoadInvolvedFiles() throws IOException {
     HoodieRecord record4 =
         new HoodieRecord(new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath()), rowChange4);
 
-    HoodieClientTestUtils.writeParquetFile(basePath, "2016/04/01", "2_0_20160401010101.parquet", Lists.newArrayList(),
+    HoodieClientTestUtils.writeParquetFile(basePath, "2016/04/01", "2_0_20160401010101.parquet", new ArrayList<>(),
         schema, null, false);
-    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "1_0_20150312101010.parquet", Lists.newArrayList(),
+    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "1_0_20150312101010.parquet", new ArrayList<>(),
         schema, null, false);
-    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "3_0_20150312101010.parquet", Arrays.asList(record1),
+    HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "3_0_20150312101010.parquet", Collections.singletonList(record1),
         schema, null, false);
     HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", "4_0_20150312101010.parquet",
         Arrays.asList(record2, record3, record4), schema, null, false);
@@ -246,13 +246,13 @@ public void testTagLocation() throws Exception {
     JavaRDD<HoodieRecord> recordRDD = jsc.parallelize(Arrays.asList(record1, record2, record3, record5));
 
     String filename0 =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2016/04/01", Arrays.asList(record1), schema, null, false);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2016/04/01", Collections.singletonList(record1), schema, null, false);
     String filename1 =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", Lists.newArrayList(), schema, null, false);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", new ArrayList<>(), schema, null, false);
     String filename2 =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", Arrays.asList(record2), schema, null, false);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", Collections.singletonList(record2), schema, null, false);
     String filename3 =
-        HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", Arrays.asList(record4), schema, null, false);
+        HoodieClientTestUtils.writeParquetFile(basePath, "2015/03/12", Collections.singletonList(record4), schema, null, false);
 
     // intentionally missed the partition "2015/03/12" to see if the GlobalBloomIndex can pick it up
     metaClient = HoodieTableMetaClient.reload(metaClient);
@@ -265,21 +265,29 @@ public void testTagLocation() throws Exception {
     JavaRDD<HoodieRecord> taggedRecordRDD = index.tagLocation(recordRDD, jsc, table);
 
     for (HoodieRecord record : taggedRecordRDD.collect()) {
-      if (record.getRecordKey().equals("000")) {
-        assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename0)));
-        assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange1.getJsonData());
-      } else if (record.getRecordKey().equals("001")) {
-        assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename2)));
-        assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange2.getJsonData());
-      } else if (record.getRecordKey().equals("002")) {
-        assertTrue(!record.isCurrentLocationKnown());
-        assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange3.getJsonData());
-      } else if (record.getRecordKey().equals("003")) {
-        assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename3)));
-        assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange5.getJsonData());
-      } else if (record.getRecordKey().equals("004")) {
-        assertTrue(record.getCurrentLocation().getFileId().equals(FSUtils.getFileId(filename3)));
-        assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange4.getJsonData());
+      switch (record.getRecordKey()) {
+        case "000":
+          assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename0));
+          assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange1.getJsonData());
+          break;
+        case "001":
+          assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename2));
+          assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange2.getJsonData());
+          break;
+        case "002":
+          assertFalse(record.isCurrentLocationKnown());
+          assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange3.getJsonData());
+          break;
+        case "003":
+          assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename3));
+          assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange5.getJsonData());
+          break;
+        case "004":
+          assertEquals(record.getCurrentLocation().getFileId(), FSUtils.getFileId(filename3));
+          assertEquals(((TestRawTripPayload) record.getData()).getJsonData(), rowChange4.getJsonData());
+          break;
+        default:
+          throw new IllegalArgumentException("Unknown Key Type :" + record.getRecordKey());
       }
     }
   }
diff --git a/hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java b/hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java
index c0fb1ad73..4f622938c 100644
--- a/hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java
+++ b/hudi-client/src/test/java/org/apache/hudi/io/TestHoodieCommitArchiveLog.java
@@ -34,7 +34,6 @@
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
 
-import com.google.common.collect.Sets;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.conf.Configuration;
@@ -48,6 +47,7 @@
 import java.util.List;
 import java.util.Set;
 import java.util.stream.Collectors;
+import java.util.stream.Stream;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
@@ -216,9 +216,8 @@ public void testArchiveTableWithArchival() throws IOException {
     assertEquals("Total archived records and total read records are the same count", 24, archivedRecordsCount);
     assertTrue("Average Archived records per block is greater than 1", archivedRecordsCount / numBlocks > 1);
     // make sure the archived commits are the same as the (originalcommits - commitsleft)
-    Set<String> readCommits = readRecords.stream().map(r -> (GenericRecord) r).map(r -> {
-      return r.get("commitTime").toString();
-    }).collect(Collectors.toSet());
+    Set<String> readCommits = readRecords.stream().map(r -> (GenericRecord) r).map(r ->
+        r.get("commitTime").toString()).collect(Collectors.toSet());
 
     assertEquals("Read commits map should match the originalCommits - commitsLoadedFromArchival",
         originalCommits.stream().map(HoodieInstant::getTimestamp).collect(Collectors.toSet()), readCommits);
@@ -399,7 +398,7 @@ public void testArchiveCommitCompactionNoHole() throws IOException {
 
   private void verifyInflightInstants(HoodieTableMetaClient metaClient, int expectedTotalInstants) {
     HoodieTimeline timeline = metaClient.getActiveTimeline().reload()
-        .getTimelineOfActions(Sets.newHashSet(HoodieTimeline.CLEAN_ACTION)).filterInflights();
+        .getTimelineOfActions(Stream.of(HoodieTimeline.CLEAN_ACTION).collect(Collectors.toSet())).filterInflights();
     assertEquals("Loaded inflight clean actions and the count should match", expectedTotalInstants,
         timeline.countInstants());
   }
diff --git a/hudi-client/src/test/java/org/apache/hudi/io/strategy/TestHoodieCompactionStrategy.java b/hudi-client/src/test/java/org/apache/hudi/io/strategy/TestHoodieCompactionStrategy.java
index 5eda0b2c9..3af3caea3 100644
--- a/hudi-client/src/test/java/org/apache/hudi/io/strategy/TestHoodieCompactionStrategy.java
+++ b/hudi-client/src/test/java/org/apache/hudi/io/strategy/TestHoodieCompactionStrategy.java
@@ -33,18 +33,19 @@
 import org.apache.hudi.io.compact.strategy.UnBoundedPartitionAwareCompactionStrategy;
 
 import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
 import org.junit.Assert;
 import org.junit.Test;
 
 import java.text.SimpleDateFormat;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.Date;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Random;
 import java.util.stream.Collectors;
+import java.util.stream.Stream;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
@@ -57,11 +58,11 @@
 
   @Test
   public void testUnBounded() {
-    Map<Long, List<Long>> sizesMap = Maps.newHashMap();
-    sizesMap.put(120 * MB, Lists.newArrayList(60 * MB, 10 * MB, 80 * MB));
-    sizesMap.put(110 * MB, Lists.newArrayList());
-    sizesMap.put(100 * MB, Lists.newArrayList(MB));
-    sizesMap.put(90 * MB, Lists.newArrayList(1024 * MB));
+    Map<Long, List<Long>> sizesMap = new HashMap<>();
+    sizesMap.put(120 * MB, Stream.of(60 * MB, 10 * MB, 80 * MB).collect(Collectors.toList()));
+    sizesMap.put(110 * MB, new ArrayList<>());
+    sizesMap.put(100 * MB, Collections.singletonList(MB));
+    sizesMap.put(90 * MB, Collections.singletonList(1024 * MB));
     UnBoundedCompactionStrategy strategy = new UnBoundedCompactionStrategy();
     HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder().withPath("/tmp")
         .withCompactionConfig(HoodieCompactionConfig.newBuilder().withCompactionStrategy(strategy).build()).build();
@@ -72,11 +73,11 @@ public void testUnBounded() {
 
   @Test
   public void testBoundedIOSimple() {
-    Map<Long, List<Long>> sizesMap = Maps.newHashMap();
-    sizesMap.put(120 * MB, Lists.newArrayList(60 * MB, 10 * MB, 80 * MB));
-    sizesMap.put(110 * MB, Lists.newArrayList());
-    sizesMap.put(100 * MB, Lists.newArrayList(MB));
-    sizesMap.put(90 * MB, Lists.newArrayList(1024 * MB));
+    Map<Long, List<Long>> sizesMap = new HashMap<>();
+    sizesMap.put(120 * MB, Stream.of(60 * MB, 10 * MB, 80 * MB).collect(Collectors.toList()));
+    sizesMap.put(110 * MB, new ArrayList<>());
+    sizesMap.put(100 * MB, Collections.singletonList(MB));
+    sizesMap.put(90 * MB, Collections.singletonList(1024 * MB));
     BoundedIOCompactionStrategy strategy = new BoundedIOCompactionStrategy();
     HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder().withPath("/tmp").withCompactionConfig(
         HoodieCompactionConfig.newBuilder().withCompactionStrategy(strategy).withTargetIOPerCompactionInMB(400).build())
@@ -95,11 +96,11 @@ public void testBoundedIOSimple() {
 
   @Test
   public void testLogFileSizeCompactionSimple() {
-    Map<Long, List<Long>> sizesMap = Maps.newHashMap();
-    sizesMap.put(120 * MB, Lists.newArrayList(60 * MB, 10 * MB, 80 * MB));
-    sizesMap.put(110 * MB, Lists.newArrayList());
-    sizesMap.put(100 * MB, Lists.newArrayList(MB));
-    sizesMap.put(90 * MB, Lists.newArrayList(1024 * MB));
+    Map<Long, List<Long>> sizesMap = new HashMap<>();
+    sizesMap.put(120 * MB, Stream.of(60 * MB, 10 * MB, 80 * MB).collect(Collectors.toList()));
+    sizesMap.put(110 * MB, new ArrayList<>());
+    sizesMap.put(100 * MB, Collections.singletonList(MB));
+    sizesMap.put(90 * MB, Collections.singletonList(1024 * MB));
     LogFileSizeBasedCompactionStrategy strategy = new LogFileSizeBasedCompactionStrategy();
     HoodieWriteConfig writeConfig = HoodieWriteConfig.newBuilder().withPath("/tmp").withCompactionConfig(
         HoodieCompactionConfig.newBuilder().withCompactionStrategy(strategy).withTargetIOPerCompactionInMB(400).build())
@@ -119,11 +120,11 @@ public void testLogFileSizeCompactionSimple() {
 
   @Test
   public void testDayBasedCompactionSimple() {
-    Map<Long, List<Long>> sizesMap = Maps.newHashMap();
-    sizesMap.put(120 * MB, Lists.newArrayList(60 * MB, 10 * MB, 80 * MB));
-    sizesMap.put(110 * MB, Lists.newArrayList());
-    sizesMap.put(100 * MB, Lists.newArrayList(MB));
-    sizesMap.put(90 * MB, Lists.newArrayList(1024 * MB));
+    Map<Long, List<Long>> sizesMap = new HashMap<>();
+    sizesMap.put(120 * MB, Stream.of(60 * MB, 10 * MB, 80 * MB).collect(Collectors.toList()));
+    sizesMap.put(110 * MB, new ArrayList<>());
+    sizesMap.put(100 * MB, Collections.singletonList(MB));
+    sizesMap.put(90 * MB, Collections.singletonList(1024 * MB));
 
     Map<Long, String> keyToPartitionMap = new ImmutableMap.Builder().put(120 * MB, partitionPaths[2])
         .put(110 * MB, partitionPaths[2]).put(100 * MB, partitionPaths[1]).put(90 * MB, partitionPaths[0]).build();
@@ -147,13 +148,13 @@ public void testDayBasedCompactionSimple() {
 
   @Test
   public void testBoundedPartitionAwareCompactionSimple() {
-    Map<Long, List<Long>> sizesMap = Maps.newHashMap();
-    sizesMap.put(120 * MB, Lists.newArrayList(60 * MB, 10 * MB, 80 * MB));
-    sizesMap.put(110 * MB, Lists.newArrayList());
-    sizesMap.put(100 * MB, Lists.newArrayList(MB));
-    sizesMap.put(70 * MB, Lists.newArrayList(MB));
-    sizesMap.put(80 * MB, Lists.newArrayList(MB));
-    sizesMap.put(90 * MB, Lists.newArrayList(1024 * MB));
+    Map<Long, List<Long>> sizesMap = new HashMap<>();
+    sizesMap.put(120 * MB, Stream.of(60 * MB, 10 * MB, 80 * MB).collect(Collectors.toList()));
+    sizesMap.put(110 * MB, new ArrayList<>());
+    sizesMap.put(100 * MB, Collections.singletonList(MB));
+    sizesMap.put(70 * MB, Collections.singletonList(MB));
+    sizesMap.put(80 * MB, Collections.singletonList(MB));
+    sizesMap.put(90 * MB, Collections.singletonList(1024 * MB));
 
     SimpleDateFormat format = new SimpleDateFormat("yyyy/MM/dd");
     Date today = new Date();
@@ -189,13 +190,13 @@ public void testBoundedPartitionAwareCompactionSimple() {
 
   @Test
   public void testUnboundedPartitionAwareCompactionSimple() {
-    Map<Long, List<Long>> sizesMap = Maps.newHashMap();
-    sizesMap.put(120 * MB, Lists.newArrayList(60 * MB, 10 * MB, 80 * MB));
-    sizesMap.put(110 * MB, Lists.newArrayList());
-    sizesMap.put(100 * MB, Lists.newArrayList(MB));
-    sizesMap.put(80 * MB, Lists.newArrayList(MB));
-    sizesMap.put(70 * MB, Lists.newArrayList(MB));
-    sizesMap.put(90 * MB, Lists.newArrayList(1024 * MB));
+    Map<Long, List<Long>> sizesMap = new HashMap<>();
+    sizesMap.put(120 * MB, Stream.of(60 * MB, 10 * MB, 80 * MB).collect(Collectors.toList()));
+    sizesMap.put(110 * MB, new ArrayList<>());
+    sizesMap.put(100 * MB, Collections.singletonList(MB));
+    sizesMap.put(80 * MB, Collections.singletonList(MB));
+    sizesMap.put(70 * MB, Collections.singletonList(MB));
+    sizesMap.put(90 * MB, Collections.singletonList(1024 * MB));
 
     SimpleDateFormat format = new SimpleDateFormat("yyyy/MM/dd");
     Date today = new Date();
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/HoodieJsonPayload.java b/hudi-common/src/main/java/org/apache/hudi/common/HoodieJsonPayload.java
index 9e95fd801..1c15c6641 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/HoodieJsonPayload.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/HoodieJsonPayload.java
@@ -86,11 +86,8 @@ private String getJsonData() throws IOException {
   }
 
   private String unCompressData(byte[] data) throws IOException {
-    InflaterInputStream iis = new InflaterInputStream(new ByteArrayInputStream(data));
-    try {
+    try (InflaterInputStream iis = new InflaterInputStream(new ByteArrayInputStream(data))) {
       return FileIOUtils.readAsUTFString(iis, dataSize);
-    } finally {
-      iis.close();
     }
   }
 
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java
index 22a05faa2..a9401c9c0 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieKey.java
@@ -18,9 +18,8 @@
 
 package org.apache.hudi.common.model;
 
-import com.google.common.base.Objects;
-
 import java.io.Serializable;
+import java.util.Objects;
 
 /**
  * HoodieKey consists of
@@ -58,12 +57,12 @@ public boolean equals(Object o) {
       return false;
     }
     HoodieKey otherKey = (HoodieKey) o;
-    return Objects.equal(recordKey, otherKey.recordKey) && Objects.equal(partitionPath, otherKey.partitionPath);
+    return Objects.equals(recordKey, otherKey.recordKey) && Objects.equals(partitionPath, otherKey.partitionPath);
   }
 
   @Override
   public int hashCode() {
-    return Objects.hashCode(recordKey, partitionPath);
+    return Objects.hash(recordKey, partitionPath);
   }
 
   @Override
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java
index 3f1e95ad8..7da08294a 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecord.java
@@ -18,13 +18,12 @@
 
 package org.apache.hudi.common.model;
 
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.Option;
 
-import com.google.common.base.Objects;
-import com.google.common.collect.ImmutableList;
-
 import java.io.Serializable;
 import java.util.List;
+import java.util.Objects;
 
 /**
  * A Single Record managed by Hoodie.
@@ -38,8 +37,8 @@
   public static String FILENAME_METADATA_FIELD = "_hoodie_file_name";
 
   public static final List<String> HOODIE_META_COLUMNS =
-      new ImmutableList.Builder<String>().add(COMMIT_TIME_METADATA_FIELD).add(COMMIT_SEQNO_METADATA_FIELD)
-          .add(RECORD_KEY_METADATA_FIELD).add(PARTITION_PATH_METADATA_FIELD).add(FILENAME_METADATA_FIELD).build();
+      CollectionUtils.createImmutableList(COMMIT_TIME_METADATA_FIELD, COMMIT_SEQNO_METADATA_FIELD,
+          RECORD_KEY_METADATA_FIELD, PARTITION_PATH_METADATA_FIELD, FILENAME_METADATA_FIELD);
 
   /**
    * Identifies the record across the table.
@@ -141,13 +140,13 @@ public boolean equals(Object o) {
       return false;
     }
     HoodieRecord that = (HoodieRecord) o;
-    return Objects.equal(key, that.key) && Objects.equal(data, that.data)
-        && Objects.equal(currentLocation, that.currentLocation) && Objects.equal(newLocation, that.newLocation);
+    return Objects.equals(key, that.key) && Objects.equals(data, that.data)
+        && Objects.equals(currentLocation, that.currentLocation) && Objects.equals(newLocation, that.newLocation);
   }
 
   @Override
   public int hashCode() {
-    return Objects.hashCode(key, data, currentLocation, newLocation);
+    return Objects.hash(key, data, currentLocation, newLocation);
   }
 
   @Override
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordLocation.java b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordLocation.java
index 2c522d166..690db8837 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordLocation.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordLocation.java
@@ -18,9 +18,8 @@
 
 package org.apache.hudi.common.model;
 
-import com.google.common.base.Objects;
-
 import java.io.Serializable;
+import java.util.Objects;
 
 /**
  * Location of a HoodieRecord within the partition it belongs to. Ultimately, this points to an actual file on disk
@@ -44,12 +43,12 @@ public boolean equals(Object o) {
       return false;
     }
     HoodieRecordLocation otherLoc = (HoodieRecordLocation) o;
-    return Objects.equal(instantTime, otherLoc.instantTime) && Objects.equal(fileId, otherLoc.fileId);
+    return Objects.equals(instantTime, otherLoc.instantTime) && Objects.equals(fileId, otherLoc.fileId);
   }
 
   @Override
   public int hashCode() {
-    return Objects.hashCode(instantTime, fileId);
+    return Objects.hash(instantTime, fileId);
   }
 
   @Override
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/model/TimelineLayoutVersion.java b/hudi-common/src/main/java/org/apache/hudi/common/model/TimelineLayoutVersion.java
index 531f1ffa9..0636aa790 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/model/TimelineLayoutVersion.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/model/TimelineLayoutVersion.java
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.common.model;
 
-import com.google.common.base.Preconditions;
+import org.apache.hudi.common.util.ValidationUtils;
 
 import java.io.Serializable;
 import java.util.Objects;
@@ -37,8 +37,8 @@
   private Integer version;
 
   public TimelineLayoutVersion(Integer version) {
-    Preconditions.checkArgument(version <= CURR_VERSION);
-    Preconditions.checkArgument(version >= VERSION_0);
+    ValidationUtils.checkArgument(version <= CURR_VERSION);
+    ValidationUtils.checkArgument(version >= VERSION_0);
     this.version = version;
   }
 
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java b/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
index 1d27d81b7..cfc87701d 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java
@@ -30,10 +30,10 @@
 import org.apache.hudi.common.util.FailSafeConsistencyGuard;
 import org.apache.hudi.common.util.NoOpConsistencyGuard;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.TableNotFoundException;
 import org.apache.hudi.exception.HoodieException;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -227,7 +227,7 @@ public TimelineLayoutVersion getTimelineLayoutVersion() {
   public HoodieWrapperFileSystem getFs() {
     if (fs == null) {
       FileSystem fileSystem = FSUtils.getFs(metaPath, hadoopConf.newCopy());
-      Preconditions.checkArgument(!(fileSystem instanceof HoodieWrapperFileSystem),
+      ValidationUtils.checkArgument(!(fileSystem instanceof HoodieWrapperFileSystem),
           "File System not expected to be that of HoodieWrapperFileSystem");
       fs = new HoodieWrapperFileSystem(fileSystem,
           consistencyGuardConfig.isConsistencyCheckEnabled()
@@ -240,7 +240,7 @@ public HoodieWrapperFileSystem getFs() {
   /**
    * Return raw file-system.
    * 
-   * @return
+   * @return fs
    */
   public FileSystem getRawFs() {
     return getFs().getFileSystem();
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java b/hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
index 354f8090d..109cf5975 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFileReader.java
@@ -28,11 +28,11 @@
 import org.apache.hudi.common.table.log.block.HoodieLogBlock.HoodieLogBlockType;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.CorruptedLogFileException;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.exception.HoodieNotSupportedException;
 
-import com.google.common.base.Preconditions;
 import org.apache.avro.Schema;
 import org.apache.hadoop.fs.BufferedFSInputStream;
 import org.apache.hadoop.fs.FSDataInputStream;
@@ -46,6 +46,7 @@
 import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.Objects;
 
 /**
  * Scans a log file and provides block level iterator on the log file Loads the entire block contents in memory Can emit
@@ -107,25 +108,22 @@ public HoodieLogFile getLogFile() {
    * Close the inputstream if not closed when the JVM exits.
    */
   private void addShutDownHook() {
-    Runtime.getRuntime().addShutdownHook(new Thread() {
-      @Override
-      public void run() {
-        try {
-          close();
-        } catch (Exception e) {
-          LOG.warn("unable to close input stream for log file " + logFile, e);
-          // fail silently for any sort of exception
-        }
+    Runtime.getRuntime().addShutdownHook(new Thread(() -> {
+      try {
+        close();
+      } catch (Exception e) {
+        LOG.warn("unable to close input stream for log file " + logFile, e);
+        // fail silently for any sort of exception
       }
-    });
+    }));
   }
 
   // TODO : convert content and block length to long by using ByteBuffer, raw byte [] allows
   // for max of Integer size
   private HoodieLogBlock readBlock() throws IOException {
 
-    int blocksize = -1;
-    int type = -1;
+    int blocksize;
+    int type;
     HoodieLogBlockType blockType = null;
     Map<HeaderMetadataType, String> header = null;
 
@@ -153,7 +151,7 @@ private HoodieLogBlock readBlock() throws IOException {
     if (nextBlockVersion.getVersion() != HoodieLogFormatVersion.DEFAULT_VERSION) {
       type = inputStream.readInt();
 
-      Preconditions.checkArgument(type < HoodieLogBlockType.values().length, "Invalid block byte type found " + type);
+      ValidationUtils.checkArgument(type < HoodieLogBlockType.values().length, "Invalid block byte type found " + type);
       blockType = HoodieLogBlockType.values()[type];
     }
 
@@ -190,7 +188,7 @@ private HoodieLogBlock readBlock() throws IOException {
     // 9. Read the log block end position in the log file
     long blockEndPos = inputStream.getPos();
 
-    switch (blockType) {
+    switch (Objects.requireNonNull(blockType)) {
       // based on type read the block
       case AVRO_DATA_BLOCK:
         if (nextBlockVersion.getVersion() == HoodieLogFormatVersion.DEFAULT_VERSION) {
@@ -279,7 +277,7 @@ public void close() throws IOException {
   }
 
   @Override
-  /**
+  /*
    * hasNext is not idempotent. TODO - Fix this. It is okay for now - PR
    */
   public boolean hasNext() {
@@ -315,10 +313,7 @@ private boolean hasNextMagic() throws IOException {
     long pos = inputStream.getPos();
     // 1. Read magic header from the start of the block
     inputStream.readFully(MAGIC_BUFFER, 0, 6);
-    if (!Arrays.equals(MAGIC_BUFFER, HoodieLogFormat.MAGIC)) {
-      return false;
-    }
-    return true;
+    return Arrays.equals(MAGIC_BUFFER, HoodieLogFormat.MAGIC);
   }
 
   @Override
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java b/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
index 69e9fb6ce..8c1fc341d 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieIOException;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericDatumReader;
 import org.apache.avro.generic.GenericDatumWriter;
@@ -220,7 +219,6 @@ private void createRecordsFromContentBytes() throws IOException {
    * HoodieLogFormat V1.
    */
   @Deprecated
-  @VisibleForTesting
   public HoodieAvroDataBlock(List<IndexedRecord> records, Schema schema) {
     super(new HashMap<>(), new HashMap<>(), Option.empty(), Option.empty(), null, false);
     this.records = records;
@@ -264,7 +262,6 @@ public static HoodieLogBlock getBlock(byte[] content, Schema readerSchema) throw
   }
 
   @Deprecated
-  @VisibleForTesting
   public byte[] getBytes(Schema schema) throws IOException {
 
     GenericDatumWriter<IndexedRecord> writer = new GenericDatumWriter<>(schema);
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java b/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
index 3c45de7d7..20eaad477 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieLogBlock.java
@@ -23,9 +23,7 @@
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.exception.HoodieException;
-import org.apache.hudi.exception.HoodieIOException;
 
-import com.google.common.collect.Maps;
 import org.apache.hadoop.fs.FSDataInputStream;
 
 import javax.annotation.Nonnull;
@@ -35,9 +33,9 @@
 import java.io.DataOutputStream;
 import java.io.EOFException;
 import java.io.IOException;
+import java.util.HashMap;
 import java.util.Map;
 
-
 /**
  * Abstract class defining a block in HoodieLogFile.
  */
@@ -192,7 +190,7 @@ public long getBlockEndPos() {
    */
   public static Map<HeaderMetadataType, String> getLogMetadata(DataInputStream dis) throws IOException {
 
-    Map<HeaderMetadataType, String> metadata = Maps.newHashMap();
+    Map<HeaderMetadataType, String> metadata = new HashMap<>();
     // 1. Read the metadata written out
     int metadataCount = dis.readInt();
     try {
@@ -231,7 +229,7 @@ public long getBlockEndPos() {
   /**
    * When lazyReading of blocks is turned on, inflate the content of a log block from disk.
    */
-  protected void inflate() throws IOException {
+  protected void inflate() throws HoodieException {
 
     try {
       content = Option.of(new byte[(int) this.getBlockContentLocation().get().getBlockSize()]);
@@ -239,13 +237,9 @@ protected void inflate() throws IOException {
       inputStream.readFully(content.get(), 0, content.get().length);
       safeSeek(inputStream, this.getBlockContentLocation().get().getBlockEndPos());
     } catch (IOException e) {
-      try {
-        // TODO : fs.open() and return inputstream again, need to pass FS configuration
-        // because the inputstream might close/timeout for large number of log blocks to be merged
-        inflate();
-      } catch (IOException io) {
-        throw new HoodieIOException("unable to lazily read log block from disk", io);
-      }
+      // TODO : fs.open() and return inputstream again, need to pass FS configuration
+      // because the inputstream might close/timeout for large number of log blocks to be merged
+      inflate();
     }
   }
 
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
index f135d80e3..05289d40d 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieActiveTimeline.java
@@ -21,13 +21,12 @@
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.FileIOUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieIOException;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.ImmutableSet;
-import com.google.common.collect.Sets;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
 import org.apache.hadoop.fs.Path;
@@ -38,8 +37,10 @@
 import java.io.Serializable;
 import java.text.SimpleDateFormat;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.Date;
 import java.util.HashSet;
+import java.util.Objects;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.function.Function;
@@ -63,10 +64,10 @@
   public static final SimpleDateFormat COMMIT_FORMATTER = new SimpleDateFormat("yyyyMMddHHmmss");
 
   public static final Set<String> VALID_EXTENSIONS_IN_ACTIVE_TIMELINE = new HashSet<>(Arrays.asList(
-      new String[]{COMMIT_EXTENSION, INFLIGHT_COMMIT_EXTENSION, REQUESTED_COMMIT_EXTENSION, DELTA_COMMIT_EXTENSION,
-          INFLIGHT_DELTA_COMMIT_EXTENSION, REQUESTED_DELTA_COMMIT_EXTENSION, SAVEPOINT_EXTENSION,
-          INFLIGHT_SAVEPOINT_EXTENSION, CLEAN_EXTENSION, REQUESTED_CLEAN_EXTENSION, INFLIGHT_CLEAN_EXTENSION,
-          INFLIGHT_COMPACTION_EXTENSION, REQUESTED_COMPACTION_EXTENSION, INFLIGHT_RESTORE_EXTENSION, RESTORE_EXTENSION}));
+      COMMIT_EXTENSION, INFLIGHT_COMMIT_EXTENSION, REQUESTED_COMMIT_EXTENSION, DELTA_COMMIT_EXTENSION,
+      INFLIGHT_DELTA_COMMIT_EXTENSION, REQUESTED_DELTA_COMMIT_EXTENSION, SAVEPOINT_EXTENSION,
+      INFLIGHT_SAVEPOINT_EXTENSION, CLEAN_EXTENSION, REQUESTED_CLEAN_EXTENSION, INFLIGHT_CLEAN_EXTENSION,
+      INFLIGHT_COMPACTION_EXTENSION, REQUESTED_COMPACTION_EXTENSION, INFLIGHT_RESTORE_EXTENSION, RESTORE_EXTENSION));
 
   private static final Logger LOG = LogManager.getLogger(HoodieActiveTimeline.class);
   protected HoodieTableMetaClient metaClient;
@@ -78,7 +79,7 @@
    */
   public static String createNewInstantTime() {
     return lastInstantTime.updateAndGet((oldVal) -> {
-      String newCommitTime = null;
+      String newCommitTime;
       do {
         newCommitTime = HoodieActiveTimeline.COMMIT_FORMATTER.format(new Date());
       } while (HoodieTimeline.compareTimestamps(newCommitTime, oldVal, LESSER_OR_EQUAL));
@@ -107,13 +108,11 @@ protected HoodieActiveTimeline(HoodieTableMetaClient metaClient, Set<String> inc
   }
 
   public HoodieActiveTimeline(HoodieTableMetaClient metaClient) {
-    this(metaClient, new ImmutableSet.Builder<String>().addAll(VALID_EXTENSIONS_IN_ACTIVE_TIMELINE).build());
+    this(metaClient, Collections.unmodifiableSet(VALID_EXTENSIONS_IN_ACTIVE_TIMELINE));
   }
 
   public HoodieActiveTimeline(HoodieTableMetaClient metaClient, boolean applyLayoutFilter) {
-    this(metaClient,
-        new ImmutableSet.Builder<String>()
-            .addAll(VALID_EXTENSIONS_IN_ACTIVE_TIMELINE).build(), applyLayoutFilter);
+    this(metaClient, Collections.unmodifiableSet(VALID_EXTENSIONS_IN_ACTIVE_TIMELINE), applyLayoutFilter);
   }
 
   /**
@@ -137,7 +136,7 @@ private void readObject(java.io.ObjectInputStream in) throws IOException, ClassN
    * Get all instants (commits, delta commits) that produce new data, in the active timeline.
    */
   public HoodieTimeline getCommitsTimeline() {
-    return getTimelineOfActions(Sets.newHashSet(COMMIT_ACTION, DELTA_COMMIT_ACTION));
+    return getTimelineOfActions(CollectionUtils.createImmutableSet(COMMIT_ACTION, DELTA_COMMIT_ACTION));
   }
 
   /**
@@ -147,7 +146,7 @@ public HoodieTimeline getCommitsTimeline() {
    */
   @Override
   public HoodieTimeline getCommitsAndCompactionTimeline() {
-    return getTimelineOfActions(Sets.newHashSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION));
+    return getTimelineOfActions(CollectionUtils.createImmutableSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION));
   }
 
   /**
@@ -155,7 +154,7 @@ public HoodieTimeline getCommitsAndCompactionTimeline() {
    * timeline.
    */
   public HoodieTimeline getAllCommitsTimeline() {
-    return getTimelineOfActions(Sets.newHashSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, CLEAN_ACTION, COMPACTION_ACTION,
+    return getTimelineOfActions(CollectionUtils.createImmutableSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, CLEAN_ACTION, COMPACTION_ACTION,
         SAVEPOINT_ACTION, ROLLBACK_ACTION));
   }
 
@@ -163,7 +162,7 @@ public HoodieTimeline getAllCommitsTimeline() {
    * Get only pure commits (inflight and completed) in the active timeline.
    */
   public HoodieTimeline getCommitTimeline() {
-    return getTimelineOfActions(Sets.newHashSet(COMMIT_ACTION));
+    return getTimelineOfActions(Collections.singleton(COMMIT_ACTION));
   }
 
   /**
@@ -228,7 +227,7 @@ public void createNewInstant(HoodieInstant instant) {
 
   public void saveAsComplete(HoodieInstant instant, Option<byte[]> data) {
     LOG.info("Marking instant complete " + instant);
-    Preconditions.checkArgument(instant.isInflight(),
+    ValidationUtils.checkArgument(instant.isInflight(),
         "Could not mark an already completed instant as complete again " + instant);
     transitionState(instant, HoodieTimeline.getCompletedInstant(instant), data);
     LOG.info("Completed " + instant);
@@ -243,18 +242,18 @@ public HoodieInstant revertToInflight(HoodieInstant instant) {
   }
 
   public void deleteInflight(HoodieInstant instant) {
-    Preconditions.checkArgument(instant.isInflight());
+    ValidationUtils.checkArgument(instant.isInflight());
     deleteInstantFile(instant);
   }
 
   public void deletePending(HoodieInstant instant) {
-    Preconditions.checkArgument(!instant.isCompleted());
+    ValidationUtils.checkArgument(!instant.isCompleted());
     deleteInstantFile(instant);
   }
 
   public void deleteCompactionRequested(HoodieInstant instant) {
-    Preconditions.checkArgument(instant.isRequested());
-    Preconditions.checkArgument(instant.getAction() == HoodieTimeline.COMPACTION_ACTION);
+    ValidationUtils.checkArgument(instant.isRequested());
+    ValidationUtils.checkArgument(Objects.equals(instant.getAction(), HoodieTimeline.COMPACTION_ACTION));
     deleteInstantFile(instant);
   }
 
@@ -284,7 +283,7 @@ private void deleteInstantFile(HoodieInstant instant) {
   //-----------------------------------------------------------------
 
   public Option<byte[]> readPlanAsBytes(HoodieInstant instant) {
-    Path detailPath = null;
+    Path detailPath;
     if (metaClient.getTimelineLayoutVersion().isNullVersion()) {
       detailPath = new Path(metaClient.getMetaAuxiliaryPath(), instant.getFileName());
     } else {
@@ -300,8 +299,8 @@ private void deleteInstantFile(HoodieInstant instant) {
    * @return requested instant
    */
   public HoodieInstant revertCompactionInflightToRequested(HoodieInstant inflightInstant) {
-    Preconditions.checkArgument(inflightInstant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
-    Preconditions.checkArgument(inflightInstant.isInflight());
+    ValidationUtils.checkArgument(inflightInstant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
+    ValidationUtils.checkArgument(inflightInstant.isInflight());
     HoodieInstant requestedInstant =
         new HoodieInstant(State.REQUESTED, COMPACTION_ACTION, inflightInstant.getTimestamp());
     if (metaClient.getTimelineLayoutVersion().isNullVersion()) {
@@ -320,8 +319,8 @@ public HoodieInstant revertCompactionInflightToRequested(HoodieInstant inflightI
    * @return inflight instant
    */
   public HoodieInstant transitionCompactionRequestedToInflight(HoodieInstant requestedInstant) {
-    Preconditions.checkArgument(requestedInstant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
-    Preconditions.checkArgument(requestedInstant.isRequested());
+    ValidationUtils.checkArgument(requestedInstant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
+    ValidationUtils.checkArgument(requestedInstant.isRequested());
     HoodieInstant inflightInstant =
         new HoodieInstant(State.INFLIGHT, COMPACTION_ACTION, requestedInstant.getTimestamp());
     transitionState(requestedInstant, inflightInstant, Option.empty());
@@ -336,8 +335,8 @@ public HoodieInstant transitionCompactionRequestedToInflight(HoodieInstant reque
    * @return commit instant
    */
   public HoodieInstant transitionCompactionInflightToComplete(HoodieInstant inflightInstant, Option<byte[]> data) {
-    Preconditions.checkArgument(inflightInstant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
-    Preconditions.checkArgument(inflightInstant.isInflight());
+    ValidationUtils.checkArgument(inflightInstant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
+    ValidationUtils.checkArgument(inflightInstant.isInflight());
     HoodieInstant commitInstant = new HoodieInstant(State.COMPLETED, COMMIT_ACTION, inflightInstant.getTimestamp());
     transitionState(inflightInstant, commitInstant, data);
     return commitInstant;
@@ -366,8 +365,8 @@ private void createFileInAuxiliaryFolder(HoodieInstant instant, Option<byte[]> d
    * @return commit instant
    */
   public HoodieInstant transitionCleanInflightToComplete(HoodieInstant inflightInstant, Option<byte[]> data) {
-    Preconditions.checkArgument(inflightInstant.getAction().equals(HoodieTimeline.CLEAN_ACTION));
-    Preconditions.checkArgument(inflightInstant.isInflight());
+    ValidationUtils.checkArgument(inflightInstant.getAction().equals(HoodieTimeline.CLEAN_ACTION));
+    ValidationUtils.checkArgument(inflightInstant.isInflight());
     HoodieInstant commitInstant = new HoodieInstant(State.COMPLETED, CLEAN_ACTION, inflightInstant.getTimestamp());
     // First write metadata to aux folder
     createFileInAuxiliaryFolder(commitInstant, data);
@@ -384,15 +383,15 @@ public HoodieInstant transitionCleanInflightToComplete(HoodieInstant inflightIns
    * @return commit instant
    */
   public HoodieInstant transitionCleanRequestedToInflight(HoodieInstant requestedInstant, Option<byte[]> data) {
-    Preconditions.checkArgument(requestedInstant.getAction().equals(HoodieTimeline.CLEAN_ACTION));
-    Preconditions.checkArgument(requestedInstant.isRequested());
+    ValidationUtils.checkArgument(requestedInstant.getAction().equals(HoodieTimeline.CLEAN_ACTION));
+    ValidationUtils.checkArgument(requestedInstant.isRequested());
     HoodieInstant inflight = new HoodieInstant(State.INFLIGHT, CLEAN_ACTION, requestedInstant.getTimestamp());
     transitionState(requestedInstant, inflight, data);
     return inflight;
   }
 
   private void transitionState(HoodieInstant fromInstant, HoodieInstant toInstant, Option<byte[]> data) {
-    Preconditions.checkArgument(fromInstant.getTimestamp().equals(toInstant.getTimestamp()));
+    ValidationUtils.checkArgument(fromInstant.getTimestamp().equals(toInstant.getTimestamp()));
     try {
       if (metaClient.getTimelineLayoutVersion().isNullVersion()) {
         // Re-create the .inflight file by opening a new file and write the commit metadata in
@@ -406,7 +405,7 @@ private void transitionState(HoodieInstant fromInstant, HoodieInstant toInstant,
       } else {
         // Ensures old state exists in timeline
         LOG.info("Checking for file exists ?" + new Path(metaClient.getMetaPath(), fromInstant.getFileName()));
-        Preconditions.checkArgument(metaClient.getFs().exists(new Path(metaClient.getMetaPath(),
+        ValidationUtils.checkArgument(metaClient.getFs().exists(new Path(metaClient.getMetaPath(),
             fromInstant.getFileName())));
         // Use Write Once to create Target File
         createImmutableFileInPath(new Path(metaClient.getMetaPath(), toInstant.getFileName()), data);
@@ -418,7 +417,7 @@ private void transitionState(HoodieInstant fromInstant, HoodieInstant toInstant,
   }
 
   private void revertCompleteToInflight(HoodieInstant completed, HoodieInstant inflight) {
-    Preconditions.checkArgument(completed.getTimestamp().equals(inflight.getTimestamp()));
+    ValidationUtils.checkArgument(completed.getTimestamp().equals(inflight.getTimestamp()));
     Path inFlightCommitFilePath = new Path(metaClient.getMetaPath(), inflight.getFileName());
     Path commitFilePath = new Path(metaClient.getMetaPath(), completed.getFileName());
     try {
@@ -444,7 +443,7 @@ private void revertCompleteToInflight(HoodieInstant completed, HoodieInstant inf
         }
 
         boolean success = metaClient.getFs().delete(commitFilePath, false);
-        Preconditions.checkArgument(success, "State Reverting failed");
+        ValidationUtils.checkArgument(success, "State Reverting failed");
       }
     } catch (IOException e) {
       throw new HoodieIOException("Could not complete revert " + completed, e);
@@ -453,7 +452,7 @@ private void revertCompleteToInflight(HoodieInstant completed, HoodieInstant inf
 
   public void transitionRequestedToInflight(HoodieInstant requested, Option<byte[]> content) {
     HoodieInstant inflight = new HoodieInstant(State.INFLIGHT, requested.getAction(), requested.getTimestamp());
-    Preconditions.checkArgument(requested.isRequested(), "Instant " + requested + " in wrong state");
+    ValidationUtils.checkArgument(requested.isRequested(), "Instant " + requested + " in wrong state");
     transitionState(requested, inflight, content);
   }
 
@@ -462,15 +461,15 @@ public void saveToCompactionRequested(HoodieInstant instant, Option<byte[]> cont
   }
 
   public void saveToCompactionRequested(HoodieInstant instant, Option<byte[]> content, boolean overwrite) {
-    Preconditions.checkArgument(instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
+    ValidationUtils.checkArgument(instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
     // Write workload to auxiliary folder
     createFileInAuxiliaryFolder(instant, content);
     createFileInMetaPath(instant.getFileName(), content, overwrite);
   }
 
   public void saveToCleanRequested(HoodieInstant instant, Option<byte[]> content) {
-    Preconditions.checkArgument(instant.getAction().equals(HoodieTimeline.CLEAN_ACTION));
-    Preconditions.checkArgument(instant.getState().equals(State.REQUESTED));
+    ValidationUtils.checkArgument(instant.getAction().equals(HoodieTimeline.CLEAN_ACTION));
+    ValidationUtils.checkArgument(instant.getState().equals(State.REQUESTED));
     // Write workload to auxiliary folder
     createFileInAuxiliaryFolder(instant, content);
     // Plan is stored in meta path
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
index c61355ce3..677792b8e 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java
@@ -24,7 +24,6 @@
 import org.apache.hudi.common.util.StringUtils;
 import org.apache.hudi.exception.HoodieException;
 
-import com.google.common.collect.Sets;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -94,9 +93,7 @@ public HoodieTimeline filterInflightsAndRequested() {
 
   @Override
   public HoodieTimeline filterPendingExcludingCompaction() {
-    return new HoodieDefaultTimeline(instants.stream().filter(instant -> {
-      return (!instant.isCompleted()) && (!instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION));
-    }), details);
+    return new HoodieDefaultTimeline(instants.stream().filter(instant -> (!instant.isCompleted()) && (!instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION))), details);
   }
 
   @Override
@@ -106,14 +103,12 @@ public HoodieTimeline filterCompletedInstants() {
 
   @Override
   public HoodieTimeline filterCompletedAndCompactionInstants() {
-    return new HoodieDefaultTimeline(instants.stream().filter(s -> {
-      return !s.isInflight() || s.getAction().equals(HoodieTimeline.COMPACTION_ACTION);
-    }), details);
+    return new HoodieDefaultTimeline(instants.stream().filter(s -> !s.isInflight() || s.getAction().equals(HoodieTimeline.COMPACTION_ACTION)), details);
   }
 
   @Override
   public HoodieTimeline getCommitsAndCompactionTimeline() {
-    Set<String> validActions = Sets.newHashSet(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION);
+    Set<String> validActions = Stream.of(COMMIT_ACTION, DELTA_COMMIT_ACTION, COMPACTION_ACTION).collect(Collectors.toSet());
     return new HoodieDefaultTimeline(instants.stream().filter(s -> validActions.contains(s.getAction())), details);
   }
 
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java
index 460d0c05c..e98e5bf49 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieInstant.java
@@ -19,9 +19,9 @@
 package org.apache.hudi.common.table.timeline;
 
 import org.apache.hudi.common.table.HoodieTimeline;
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.FSUtils;
 
-import com.google.common.collect.ImmutableMap;
 import org.apache.hadoop.fs.FileStatus;
 
 import java.io.Serializable;
@@ -41,17 +41,17 @@
    * A COMPACTION action eventually becomes COMMIT when completed. So, when grouping instants
    * for state transitions, this needs to be taken into account
    */
-  private static final Map<String, String> COMPARABLE_ACTIONS = new ImmutableMap.Builder<String, String>()
-      .put(HoodieTimeline.COMPACTION_ACTION, HoodieTimeline.COMMIT_ACTION).build();
+  private static final Map<String, String> COMPARABLE_ACTIONS =
+      CollectionUtils.createImmutableMap(HoodieTimeline.COMPACTION_ACTION, HoodieTimeline.COMMIT_ACTION);
 
   public static final Comparator<HoodieInstant> ACTION_COMPARATOR =
-      Comparator.<HoodieInstant, String>comparing(instant -> getComparableAction(instant.getAction()));
+      Comparator.comparing(instant -> getComparableAction(instant.getAction()));
 
   public static final Comparator<HoodieInstant> COMPARATOR = Comparator.comparing(HoodieInstant::getTimestamp)
       .thenComparing(ACTION_COMPARATOR).thenComparing(HoodieInstant::getState);
 
-  public static final String getComparableAction(String action) {
-    return COMPARABLE_ACTIONS.containsKey(action) ? COMPARABLE_ACTIONS.get(action) : action;
+  public static String getComparableAction(String action) {
+    return COMPARABLE_ACTIONS.getOrDefault(action, action);
   }
 
   /**
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
index 3ffa9fbd8..aec26f835 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/AbstractTableFileSystemView.java
@@ -32,10 +32,10 @@
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.HoodieTimer;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
@@ -209,7 +209,7 @@ public final void reset() {
    */
   private void ensurePartitionLoadedCorrectly(String partition) {
 
-    Preconditions.checkArgument(!isClosed(), "View is already closed");
+    ValidationUtils.checkArgument(!isClosed(), "View is already closed");
 
     // ensure we list files only once even in the face of concurrency
     addedPartitions.computeIfAbsent(partition, (partitionPathStr) -> {
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java
index b72696a4e..c5a034b80 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/FileSystemViewStorageConfig.java
@@ -18,10 +18,9 @@
 
 package org.apache.hudi.common.table.view;
 
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.DefaultHoodieConfig;
 
-import com.google.common.base.Preconditions;
-
 import java.io.File;
 import java.io.FileReader;
 import java.io.IOException;
@@ -113,12 +112,9 @@ public String getRocksdbBasePath() {
     private final Properties props = new Properties();
 
     public Builder fromFile(File propertiesFile) throws IOException {
-      FileReader reader = new FileReader(propertiesFile);
-      try {
+      try (FileReader reader = new FileReader(propertiesFile)) {
         props.load(reader);
         return this;
-      } finally {
-        reader.close();
       }
     }
 
@@ -197,7 +193,7 @@ public FileSystemViewStorageConfig build() {
       // Validations
       FileSystemViewStorageType.valueOf(props.getProperty(FILESYSTEM_VIEW_STORAGE_TYPE));
       FileSystemViewStorageType.valueOf(props.getProperty(FILESYSTEM_SECONDARY_VIEW_STORAGE_TYPE));
-      Preconditions.checkArgument(Integer.parseInt(props.getProperty(FILESYSTEM_VIEW_REMOTE_PORT)) > 0);
+      ValidationUtils.checkArgument(Integer.parseInt(props.getProperty(FILESYSTEM_VIEW_REMOTE_PORT)) > 0);
       return new FileSystemViewStorageConfig(props);
     }
   }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java
index dd711244a..98f400140 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/HoodieTableFileSystemView.java
@@ -25,9 +25,9 @@
 import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.table.TableFileSystemView;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -142,7 +142,7 @@ protected void resetPendingCompactionOperations(Stream<Pair<String, CompactionOp
   @Override
   protected void addPendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations) {
     operations.forEach(opInstantPair -> {
-      Preconditions.checkArgument(!fgIdToPendingCompaction.containsKey(opInstantPair.getValue().getFileGroupId()),
+      ValidationUtils.checkArgument(!fgIdToPendingCompaction.containsKey(opInstantPair.getValue().getFileGroupId()),
           "Duplicate FileGroupId found in pending compaction operations. FgId :"
               + opInstantPair.getValue().getFileGroupId());
       fgIdToPendingCompaction.put(opInstantPair.getValue().getFileGroupId(),
@@ -153,7 +153,7 @@ protected void addPendingCompactionOperations(Stream<Pair<String, CompactionOper
   @Override
   protected void removePendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations) {
     operations.forEach(opInstantPair -> {
-      Preconditions.checkArgument(fgIdToPendingCompaction.containsKey(opInstantPair.getValue().getFileGroupId()),
+      ValidationUtils.checkArgument(fgIdToPendingCompaction.containsKey(opInstantPair.getValue().getFileGroupId()),
           "Trying to remove a FileGroupId which is not found in pending compaction operations. FgId :"
               + opInstantPair.getValue().getFileGroupId());
       fgIdToPendingCompaction.remove(opInstantPair.getValue().getFileGroupId());
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java
index 62ef3e6a2..63dd0b460 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RemoteHoodieTableFileSystemView.java
@@ -34,12 +34,12 @@
 import org.apache.hudi.common.table.timeline.dto.TimelineDTO;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.StringUtils;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieRemoteException;
 
 import com.fasterxml.jackson.core.type.TypeReference;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import org.apache.http.client.fluent.Request;
 import org.apache.http.client.fluent.Response;
 import org.apache.http.client.utils.URIBuilder;
@@ -134,7 +134,7 @@ public RemoteHoodieTableFileSystemView(String server, int port, HoodieTableMetaC
 
   private <T> T executeRequest(String requestPath, Map<String, String> queryParameters, TypeReference reference,
       RequestMethod method) throws IOException {
-    Preconditions.checkArgument(!closed, "View already closed");
+    ValidationUtils.checkArgument(!closed, "View already closed");
 
     URIBuilder builder =
         new URIBuilder().setHost(serverHost).setPort(serverPort).setPath(requestPath).setScheme("http");
@@ -197,7 +197,7 @@ public RemoteHoodieTableFileSystemView(String server, int port, HoodieTableMetaC
     Map<String, String> paramsMap = new HashMap<>();
     paramsMap.put(BASEPATH_PARAM, basePath);
     paramsMap.put(PARTITION_PARAM, partitionPath);
-    Preconditions.checkArgument(paramNames.length == paramVals.length);
+    ValidationUtils.checkArgument(paramNames.length == paramVals.length);
     for (int i = 0; i < paramNames.length; i++) {
       paramsMap.put(paramNames[i], paramVals[i]);
     }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java
index 9ef99a63c..8e92f4cd4 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/table/view/RocksDbBasedFileSystemView.java
@@ -29,9 +29,9 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.RocksDBDAO;
 import org.apache.hudi.common.util.RocksDBSchemaHelper;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
@@ -110,7 +110,7 @@ protected void resetPendingCompactionOperations(Stream<Pair<String, CompactionOp
   protected void addPendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations) {
     rocksDB.writeBatch(batch -> {
       operations.forEach(opInstantPair -> {
-        Preconditions.checkArgument(!isPendingCompactionScheduledForFileId(opInstantPair.getValue().getFileGroupId()),
+        ValidationUtils.checkArgument(!isPendingCompactionScheduledForFileId(opInstantPair.getValue().getFileGroupId()),
             "Duplicate FileGroupId found in pending compaction operations. FgId :"
                 + opInstantPair.getValue().getFileGroupId());
         rocksDB.putInBatch(batch, schemaHelper.getColFamilyForPendingCompaction(),
@@ -123,7 +123,7 @@ protected void addPendingCompactionOperations(Stream<Pair<String, CompactionOper
   void removePendingCompactionOperations(Stream<Pair<String, CompactionOperation>> operations) {
     rocksDB.writeBatch(batch -> {
       operations.forEach(opInstantPair -> {
-        Preconditions.checkArgument(
+        ValidationUtils.checkArgument(
             getPendingCompactionOperationWithInstant(opInstantPair.getValue().getFileGroupId()) != null,
             "Trying to remove a FileGroupId which is not found in pending compaction operations. FgId :"
                 + opInstantPair.getValue().getFileGroupId());
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/AvroUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/AvroUtils.java
index a856926db..683a4dd30 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/AvroUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/AvroUtils.java
@@ -28,7 +28,6 @@
 import org.apache.hudi.avro.model.HoodieSavepointPartitionMetadata;
 import org.apache.hudi.common.HoodieRollbackStat;
 
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
 import org.apache.avro.file.DataFileReader;
 import org.apache.avro.file.DataFileWriter;
@@ -42,7 +41,7 @@
 
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
-import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -58,7 +57,7 @@ public static HoodieRestoreMetadata convertRestoreMetadata(String startRestoreTi
     ImmutableMap.Builder<String, List<HoodieRollbackMetadata>> commitToStatBuilder = ImmutableMap.builder();
     for (Map.Entry<String, List<HoodieRollbackStat>> commitToStat : commitToStats.entrySet()) {
       commitToStatBuilder.put(commitToStat.getKey(),
-          Arrays.asList(convertRollbackMetadata(startRestoreTime, durationInMs, commits, commitToStat.getValue())));
+          Collections.singletonList(convertRollbackMetadata(startRestoreTime, durationInMs, commits, commitToStat.getValue())));
     }
     return new HoodieRestoreMetadata(startRestoreTime, durationInMs.orElseGet(() -> -1L), commits,
         commitToStatBuilder.build(), DEFAULT_VERSION);
@@ -145,7 +144,7 @@ public static HoodieSavepointMetadata deserializeHoodieSavepointMetadata(byte[]
       throws IOException {
     DatumReader<T> reader = new SpecificDatumReader<>(clazz);
     FileReader<T> fileReader = DataFileReader.openReader(new SeekableByteArrayInput(bytes), reader);
-    Preconditions.checkArgument(fileReader.hasNext(), "Could not deserialize metadata of type " + clazz);
+    ValidationUtils.checkArgument(fileReader.hasNext(), "Could not deserialize metadata of type " + clazz);
     return fileReader.next();
   }
 }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java
index c5261efb4..14f99d520 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/CleanerUtils.java
@@ -55,10 +55,9 @@ public static HoodieCleanMetadata convertCleanMetadata(HoodieTableMetaClient met
       }
     }
 
-    HoodieCleanMetadata metadata = new HoodieCleanMetadata(startCleanTime,
+    return new HoodieCleanMetadata(startCleanTime,
         durationInMs.orElseGet(() -> -1L), totalDeleted, earliestCommitToRetain,
         partitionMetadataBuilder.build(), CLEAN_METADATA_VERSION_2);
-    return metadata;
   }
 
   /**
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java
new file mode 100644
index 000000000..fc99edf04
--- /dev/null
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/CollectionUtils.java
@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hudi.common.util;
+
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+public class CollectionUtils {
+  /**
+   * Determines whether two iterators contain equal elements in the same order. More specifically,
+   * this method returns {@code true} if {@code iterator1} and {@code iterator2} contain the same
+   * number of elements and every element of {@code iterator1} is equal to the corresponding element
+   * of {@code iterator2}.
+   *
+   * <p>Note that this will modify the supplied iterators, since they will have been advanced some
+   * number of elements forward.
+   */
+  public static boolean elementsEqual(Iterator<?> iterator1, Iterator<?> iterator2) {
+    while (iterator1.hasNext()) {
+      if (!iterator2.hasNext()) {
+        return false;
+      }
+      Object o1 = iterator1.next();
+      Object o2 = iterator2.next();
+      if (!Objects.equals(o1, o2)) {
+        return false;
+      }
+    }
+    return !iterator2.hasNext();
+  }
+
+  public static <T> List<T> createImmutableList(final T element) {
+    return Collections.unmodifiableList(Collections.singletonList(element));
+  }
+
+  public static <T> Set<T> createImmutableSet(final T element) {
+    return Collections.unmodifiableSet(Collections.singleton(element));
+  }
+
+  public static <K,V> Map<K, V> createImmutableMap(final K key, final V value) {
+    return Collections.unmodifiableMap(Collections.singletonMap(key, value));
+  }
+
+  @SafeVarargs
+  public static <T> List<T> createImmutableList(final T... elements) {
+    return Collections.unmodifiableList(Stream.of(elements).collect(Collectors.toList()));
+  }
+
+  @SafeVarargs
+  public static <T> Set<T> createImmutableSet(final T... elements) {
+    return Collections.unmodifiableSet(Stream.of(elements).collect(Collectors.toSet()));
+  }
+
+  public static <T> Set<T> createImmutableSet(final Set<T> set) {
+    return Collections.unmodifiableSet(set);
+  }
+
+  public static <T> List<T> createImmutableList(final List<T> list) {
+    return Collections.unmodifiableList(list);
+  }
+
+  private static Object[] checkElementsNotNull(Object... array) {
+    return checkElementsNotNull(array, array.length);
+  }
+
+  private static Object[] checkElementsNotNull(Object[] array, int length) {
+    for (int i = 0; i < length; i++) {
+      checkElementNotNull(array[i], i);
+    }
+    return array;
+  }
+
+  private static Object checkElementNotNull(Object element, int index) {
+    if (element == null) {
+      throw new NullPointerException("at index " + index);
+    }
+    return element;
+  }
+}
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
index 7d5f786a0..f47ece6c3 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/CompactionUtils.java
@@ -90,7 +90,7 @@ public static HoodieCompactionPlan buildFromFileSlices(List<Pair<String, FileSli
       Option<Map<String, String>> extraMetadata,
       Option<Function<Pair<String, FileSlice>, Map<String, Double>>> metricsCaptureFunction) {
     HoodieCompactionPlan.Builder builder = HoodieCompactionPlan.newBuilder();
-    extraMetadata.ifPresent(m -> builder.setExtraMetadata(m));
+    extraMetadata.ifPresent(builder::setExtraMetadata);
 
     builder.setOperations(partitionFileSlicePairs.stream()
         .map(pfPair -> buildFromFileSlice(pfPair.getKey(), pfPair.getValue(), metricsCaptureFunction))
@@ -157,9 +157,7 @@ public static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient metaC
 
     Map<HoodieFileGroupId, Pair<String, HoodieCompactionOperation>> fgIdToPendingCompactionWithInstantMap =
         new HashMap<>();
-    pendingCompactionPlanWithInstants.stream().flatMap(instantPlanPair -> {
-      return getPendingCompactionOperations(instantPlanPair.getKey(), instantPlanPair.getValue());
-    }).forEach(pair -> {
+    pendingCompactionPlanWithInstants.stream().flatMap(CompactionUtils::apply).forEach(pair -> {
       // Defensive check to ensure a single-fileId does not have more than one pending compaction with different
       // file slices. If we find a full duplicate we assume it is caused by eventual nature of the move operation
       // on some DFSs.
@@ -183,10 +181,8 @@ public static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient metaC
       HoodieInstant instant, HoodieCompactionPlan compactionPlan) {
     List<HoodieCompactionOperation> ops = compactionPlan.getOperations();
     if (null != ops) {
-      return ops.stream().map(op -> {
-        return Pair.of(new HoodieFileGroupId(op.getPartitionPath(), op.getFileId()),
-            Pair.of(instant.getTimestamp(), op));
-      });
+      return ops.stream().map(op -> Pair.of(new HoodieFileGroupId(op.getPartitionPath(), op.getFileId()),
+          Pair.of(instant.getTimestamp(), op)));
     } else {
       return Stream.empty();
     }
@@ -200,4 +196,8 @@ public static HoodieCompactionPlan getCompactionPlan(HoodieTableMetaClient metaC
   public static List<HoodieInstant> getPendingCompactionInstantTimes(HoodieTableMetaClient metaClient) {
     return metaClient.getActiveTimeline().filterPendingCompactionTimeline().getInstants().collect(Collectors.toList());
   }
+
+  private static Stream<? extends Pair<HoodieFileGroupId, Pair<String, HoodieCompactionOperation>>> apply(Pair<HoodieInstant, HoodieCompactionPlan> instantPlanPair) {
+    return getPendingCompactionOperations(instantPlanPair.getKey(), instantPlanPair.getValue());
+  }
 }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/FSUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/FSUtils.java
index d9161e512..f392608cb 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/FSUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/FSUtils.java
@@ -28,8 +28,6 @@
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.exception.InvalidHoodiePathException;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileStatus;
@@ -49,6 +47,7 @@
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map.Entry;
+import java.util.Objects;
 import java.util.UUID;
 import java.util.function.Function;
 import java.util.regex.Matcher;
@@ -70,12 +69,7 @@
   private static final long MIN_ROLLBACK_TO_KEEP = 10;
   private static final String HOODIE_ENV_PROPS_PREFIX = "HOODIE_ENV_";
 
-  private static final PathFilter ALLOW_ALL_FILTER = new PathFilter() {
-    @Override
-    public boolean accept(Path file) {
-      return true;
-    }
-  };
+  private static final PathFilter ALLOW_ALL_FILTER = file -> true;
 
   public static Configuration prepareHadoopConf(Configuration conf) {
     conf.set("fs.hdfs.impl", org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());
@@ -93,7 +87,7 @@ public static Configuration prepareHadoopConf(Configuration conf) {
 
   public static FileSystem getFs(String path, Configuration conf) {
     FileSystem fs;
-    conf = prepareHadoopConf(conf);
+    prepareHadoopConf(conf);
     try {
       fs = new Path(path).getFileSystem(conf);
     } catch (IOException e) {
@@ -120,11 +114,11 @@ public static String makeMarkerFile(String commitTime, String writeToken, String
   }
 
   public static String translateMarkerToDataPath(String basePath, String markerPath, String instantTs) {
-    Preconditions.checkArgument(markerPath.endsWith(HoodieTableMetaClient.MARKER_EXTN));
+    ValidationUtils.checkArgument(markerPath.endsWith(HoodieTableMetaClient.MARKER_EXTN));
     String markerRootPath = Path.getPathWithoutSchemeAndAuthority(
         new Path(String.format("%s/%s/%s", basePath, HoodieTableMetaClient.TEMPFOLDER_NAME, instantTs))).toString();
     int begin = markerPath.indexOf(markerRootPath);
-    Preconditions.checkArgument(begin >= 0,
+    ValidationUtils.checkArgument(begin >= 0,
         "Not in marker dir. Marker Path=" + markerPath + ", Expected Marker Root=" + markerRootPath);
     String rPath = markerPath.substring(begin + markerRootPath.length() + 1);
     return String.format("%s/%s%s", basePath, rPath.replace(HoodieTableMetaClient.MARKER_EXTN, ""),
@@ -198,7 +192,7 @@ public static String getRelativePartitionPath(Path basePath, Path partitionPath)
     return partitions;
   }
 
-  public static final List<String> getAllDataFilesForMarkers(FileSystem fs, String basePath, String instantTs,
+  public static List<String> getAllDataFilesForMarkers(FileSystem fs, String basePath, String instantTs,
       String markerDir) throws IOException {
     List<String> dataFiles = new LinkedList<>();
     processFiles(fs, markerDir, (status) -> {
@@ -219,15 +213,13 @@ public static String getRelativePartitionPath(Path basePath, Path partitionPath)
    * @param basePathStr Base-Path
    * @param consumer Callback for processing
    * @param excludeMetaFolder Exclude .hoodie folder
-   * @throws IOException
+   * @throws IOException -
    */
-  @VisibleForTesting
   static void processFiles(FileSystem fs, String basePathStr, Function<FileStatus, Boolean> consumer,
       boolean excludeMetaFolder) throws IOException {
     PathFilter pathFilter = excludeMetaFolder ? getExcludeMetaPathFilter() : ALLOW_ALL_FILTER;
     FileStatus[] topLevelStatuses = fs.listStatus(new Path(basePathStr));
-    for (int i = 0; i < topLevelStatuses.length; i++) {
-      FileStatus child = topLevelStatuses[i];
+    for (FileStatus child : topLevelStatuses) {
       if (child.isFile()) {
         boolean success = consumer.apply(child);
         if (!success) {
@@ -256,7 +248,7 @@ static void processFiles(FileSystem fs, String basePathStr, Function<FileStatus,
   }
 
   public static String getFileExtension(String fullName) {
-    Preconditions.checkNotNull(fullName);
+    Objects.requireNonNull(fullName);
     String fileName = (new File(fullName)).getName();
     int dotIndex = fileName.indexOf('.');
     return dotIndex == -1 ? "" : fileName.substring(dotIndex);
@@ -264,12 +256,7 @@ public static String getFileExtension(String fullName) {
 
   private static PathFilter getExcludeMetaPathFilter() {
     // Avoid listing and including any folders under the metafolder
-    return (path) -> {
-      if (path.toString().contains(HoodieTableMetaClient.METAFOLDER_NAME)) {
-        return false;
-      }
-      return true;
-    };
+    return (path) -> !path.toString().contains(HoodieTableMetaClient.METAFOLDER_NAME);
   }
 
   public static String getInstantTime(String name) {
@@ -396,17 +383,14 @@ public static String makeLogFileName(String fileId, String logFileExtension, Str
 
   public static boolean isLogFile(Path logPath) {
     Matcher matcher = LOG_FILE_PATTERN.matcher(logPath.getName());
-    if (!matcher.find()) {
-      return false;
-    }
-    return true;
+    return matcher.find();
   }
 
   /**
    * Get the latest log file written from the list of log files passed in.
    */
   public static Option<HoodieLogFile> getLatestLogFile(Stream<HoodieLogFile> logFiles) {
-    return Option.fromJavaOptional(logFiles.sorted(HoodieLogFile.getReverseLogFileComparator()).findFirst());
+    return Option.fromJavaOptional(logFiles.min(HoodieLogFile.getReverseLogFileComparator()));
   }
 
   /**
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/FailSafeConsistencyGuard.java b/hudi-common/src/main/java/org/apache/hudi/common/util/FailSafeConsistencyGuard.java
index b4a099179..8af1def0b 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/FailSafeConsistencyGuard.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/FailSafeConsistencyGuard.java
@@ -18,7 +18,6 @@
 
 package org.apache.hudi.common.util;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -47,7 +46,7 @@
   public FailSafeConsistencyGuard(FileSystem fs, ConsistencyGuardConfig consistencyGuardConfig) {
     this.fs = fs;
     this.consistencyGuardConfig = consistencyGuardConfig;
-    Preconditions.checkArgument(consistencyGuardConfig.isConsistencyCheckEnabled());
+    ValidationUtils.checkArgument(consistencyGuardConfig.isConsistencyCheckEnabled());
   }
 
   @Override
@@ -81,7 +80,7 @@ public void waitTillAllFilesDisappear(String dirPath, List<String> files) throws
   public void waitForFilesVisibility(String dirPath, List<String> files, FileVisibility event) throws TimeoutException {
     Path dir = new Path(dirPath);
     List<String> filesWithoutSchemeAndAuthority =
-        files.stream().map(f -> Path.getPathWithoutSchemeAndAuthority(new Path(f))).map(p -> p.toString())
+        files.stream().map(f -> Path.getPathWithoutSchemeAndAuthority(new Path(f))).map(Path::toString)
             .collect(Collectors.toList());
 
     retryTillSuccess((retryNum) -> {
@@ -89,7 +88,7 @@ public void waitForFilesVisibility(String dirPath, List<String> files, FileVisib
         LOG.info("Trying " + retryNum);
         FileStatus[] entries = fs.listStatus(dir);
         List<String> gotFiles = Arrays.stream(entries).map(e -> Path.getPathWithoutSchemeAndAuthority(e.getPath()))
-            .map(p -> p.toString()).collect(Collectors.toList());
+            .map(Path::toString).collect(Collectors.toList());
         List<String> candidateFiles = new ArrayList<>(filesWithoutSchemeAndAuthority);
         boolean altered = candidateFiles.removeAll(gotFiles);
 
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java
index f1095b684..f86ff0595 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/FileIOUtils.java
@@ -18,7 +18,10 @@
 
 package org.apache.hudi.common.util;
 
+import javax.annotation.Nullable;
+
 import java.io.ByteArrayOutputStream;
+import java.io.Closeable;
 import java.io.File;
 import java.io.FileOutputStream;
 import java.io.IOException;
@@ -91,4 +94,55 @@ public static void writeStringToFile(String str, String filePath) throws IOExcep
     out.flush();
     out.close();
   }
+
+  /**
+   * Closes a {@link Closeable}, with control over whether an {@code IOException} may be thrown.
+   * @param closeable the {@code Closeable} object to be closed, or null,
+   *      in which case this method does nothing.
+   * @param swallowIOException if true, don't propagate IO exceptions thrown by the {@code close} methods.
+   *
+   * @throws IOException if {@code swallowIOException} is false and {@code close} throws an {@code IOException}.
+   */
+  public static void close(@Nullable Closeable closeable, boolean swallowIOException)
+      throws IOException {
+    if (closeable == null) {
+      return;
+    }
+    try {
+      closeable.close();
+    } catch (IOException e) {
+      if (!swallowIOException) {
+        throw e;
+      }
+    }
+  }
+
+  /** Maximum loop count when creating temp directories. */
+  private static final int TEMP_DIR_ATTEMPTS = 10000;
+
+  /**
+   * Create a Temporary Directory.
+   * @return {@code File}
+   */
+  public static File createTempDir() {
+    File baseDir = new File(System.getProperty("java.io.tmpdir"));
+    String baseName = System.currentTimeMillis() + "-";
+
+    for (int counter = 0; counter < TEMP_DIR_ATTEMPTS; counter++) {
+      File tempDir = new File(baseDir, baseName + counter);
+      if (tempDir.mkdir()) {
+        return tempDir;
+      }
+    }
+
+    throw new IllegalStateException(
+        "Failed to create directory within "
+            + TEMP_DIR_ATTEMPTS
+            + " attempts (tried "
+            + baseName
+            + "0 to "
+            + baseName
+            + (TEMP_DIR_ATTEMPTS - 1)
+            + ')');
+  }
 }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/HoodieAvroUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/HoodieAvroUtils.java
index d030ce810..6b26f2839 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/HoodieAvroUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/HoodieAvroUtils.java
@@ -43,6 +43,7 @@
 import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
@@ -84,7 +85,7 @@
   public static GenericRecord bytesToAvro(byte[] bytes, Schema schema) throws IOException {
     BinaryDecoder decoder = DecoderFactory.get().binaryDecoder(bytes, reuseDecoder.get());
     reuseDecoder.set(decoder);
-    GenericDatumReader<GenericRecord> reader = new GenericDatumReader<GenericRecord>(schema);
+    GenericDatumReader<GenericRecord> reader = new GenericDatumReader<>(schema);
     return reader.read(null, decoder);
   }
 
@@ -141,7 +142,7 @@ private static Schema initRecordKeySchema() {
     Schema.Field recordKeyField =
         new Schema.Field(HoodieRecord.RECORD_KEY_METADATA_FIELD, METADATA_FIELD_SCHEMA, "", NullNode.getInstance());
     Schema recordKeySchema = Schema.createRecord("HoodieRecordKey", "", "", false);
-    recordKeySchema.setFields(Arrays.asList(recordKeyField));
+    recordKeySchema.setFields(Collections.singletonList(recordKeyField));
     return recordKeySchema;
   }
 
@@ -166,9 +167,8 @@ public static GenericRecord addHoodieKeyToRecord(GenericRecord record, String re
    * @param newFieldNames Null Field names to be added
    */
   public static Schema appendNullSchemaFields(Schema schema, List<String> newFieldNames) {
-    List<Field> newFields = schema.getFields().stream().map(field -> {
-      return new Schema.Field(field.name(), field.schema(), field.doc(), field.defaultValue());
-    }).collect(Collectors.toList());
+    List<Field> newFields = schema.getFields().stream().map(field ->
+        new Field(field.name(), field.schema(), field.doc(), field.defaultValue())).collect(Collectors.toList());
     for (String newField : newFieldNames) {
       newFields.add(new Schema.Field(newField, METADATA_FIELD_SCHEMA, "", NullNode.getInstance()));
     }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/LogReaderUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/LogReaderUtils.java
index 649396d36..52b30cbc2 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/LogReaderUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/LogReaderUtils.java
@@ -49,7 +49,7 @@ private static Schema readSchemaFromLogFileInReverse(FileSystem fs, HoodieActive
     HoodieTimeline completedTimeline = activeTimeline.getCommitsTimeline().filterCompletedInstants();
     while (reader.hasPrev()) {
       HoodieLogBlock block = reader.prev();
-      if (block instanceof HoodieAvroDataBlock && block != null) {
+      if (block instanceof HoodieAvroDataBlock) {
         HoodieAvroDataBlock lastBlock = (HoodieAvroDataBlock) block;
         if (completedTimeline
             .containsOrBeforeTimelineStarts(lastBlock.getLogBlockHeader().get(HeaderMetadataType.INSTANT_TIME))) {
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/NumericUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/NumericUtils.java
index bb22838a9..9e010a33a 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/NumericUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/NumericUtils.java
@@ -18,6 +18,11 @@
 
 package org.apache.hudi.common.util;
 
+import java.nio.charset.StandardCharsets;
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
+import java.util.Objects;
+
 /**
  * A utility class for numeric.
  */
@@ -31,4 +36,14 @@ public static String humanReadableByteCount(double bytes) {
     String pre = "KMGTPE".charAt(exp - 1) + "";
     return String.format("%.1f %sB", bytes / Math.pow(1024, exp), pre);
   }
+
+  public static byte[] getMessageDigestHash(final String string) {
+    MessageDigest md = null;
+    try {
+      md = MessageDigest.getInstance("MD5");
+    } catch (NoSuchAlgorithmException e) {
+      e.printStackTrace();
+    }
+    return Objects.requireNonNull(md).digest(string.getBytes(StandardCharsets.UTF_8));
+  }
 }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/ObjectSizeCalculator.java b/hudi-common/src/main/java/org/apache/hudi/common/util/ObjectSizeCalculator.java
index b11ac6c1d..8ab4fcbe6 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/ObjectSizeCalculator.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/ObjectSizeCalculator.java
@@ -16,12 +16,9 @@
 
 package org.apache.hudi.common.util;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import com.google.common.cache.CacheBuilder;
 import com.google.common.cache.CacheLoader;
 import com.google.common.cache.LoadingCache;
-import com.google.common.collect.Sets;
 
 import java.lang.management.ManagementFactory;
 import java.lang.management.MemoryPoolMXBean;
@@ -30,9 +27,12 @@
 import java.lang.reflect.Modifier;
 import java.util.ArrayDeque;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.Deque;
+import java.util.IdentityHashMap;
 import java.util.LinkedList;
 import java.util.List;
+import java.util.Objects;
 import java.util.Set;
 
 /**
@@ -131,8 +131,8 @@ public ClassSizeInfo load(Class<?> clazz) {
       });
 
 
-  private final Set<Object> alreadyVisited = Sets.newIdentityHashSet();
-  private final Deque<Object> pending = new ArrayDeque<Object>(16 * 1024);
+  private final Set<Object> alreadyVisited = Collections.newSetFromMap(new IdentityHashMap<>());
+  private final Deque<Object> pending = new ArrayDeque<>(16 * 1024);
   private long size;
 
   /**
@@ -141,7 +141,7 @@ public ClassSizeInfo load(Class<?> clazz) {
    * @param memoryLayoutSpecification a description of the JVM memory layout.
    */
   public ObjectSizeCalculator(MemoryLayoutSpecification memoryLayoutSpecification) {
-    Preconditions.checkNotNull(memoryLayoutSpecification);
+    Objects.requireNonNull(memoryLayoutSpecification);
     arrayHeaderSize = memoryLayoutSpecification.getArrayHeaderSize();
     objectHeaderSize = memoryLayoutSpecification.getObjectHeaderSize();
     objectPadding = memoryLayoutSpecification.getObjectPadding();
@@ -252,7 +252,6 @@ void increaseSize(long objectSize) {
     size += objectSize;
   }
 
-  @VisibleForTesting
   static long roundTo(long x, int multiple) {
     return ((x + multiple - 1) / multiple) * multiple;
   }
@@ -268,7 +267,7 @@ static long roundTo(long x, int multiple) {
 
     public ClassSizeInfo(Class<?> clazz) {
       long fieldsSize = 0;
-      final List<Field> referenceFields = new LinkedList<Field>();
+      final List<Field> referenceFields = new LinkedList<>();
       for (Field f : clazz.getDeclaredFields()) {
         if (Modifier.isStatic(f.getModifiers())) {
           continue;
@@ -290,7 +289,7 @@ public ClassSizeInfo(Class<?> clazz) {
       }
       this.fieldsSize = fieldsSize;
       this.objectSize = roundTo(objectHeaderSize + fieldsSize, objectPadding);
-      this.referenceFields = referenceFields.toArray(new Field[referenceFields.size()]);
+      this.referenceFields = referenceFields.toArray(new Field[0]);
     }
 
     void visit(Object obj, ObjectSizeCalculator calc) {
@@ -303,9 +302,7 @@ public void enqueueReferencedObjects(Object obj, ObjectSizeCalculator calc) {
         try {
           calc.enqueue(f.get(obj));
         } catch (IllegalAccessException e) {
-          final AssertionError ae = new AssertionError("Unexpected denial of access to " + f);
-          ae.initCause(e);
-          throw ae;
+          throw new AssertionError("Unexpected denial of access to " + f, e);
         }
       }
     }
@@ -327,7 +324,6 @@ private static long getPrimitiveFieldSize(Class<?> type) {
     throw new AssertionError("Encountered unexpected primitive type " + type.getName());
   }
 
-  @VisibleForTesting
   static MemoryLayoutSpecification getEffectiveMemoryLayoutSpecification() {
     final String vmName = System.getProperty("java.vm.name");
     if (vmName == null || !(vmName.startsWith("Java HotSpot(TM) ") || vmName.startsWith("OpenJDK")
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBDAO.java b/hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBDAO.java
index 59af74bf2..372225f29 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBDAO.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/RocksDBDAO.java
@@ -22,10 +22,9 @@
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
+import org.rocksdb.AbstractImmutableNativeReference;
 import org.rocksdb.ColumnFamilyDescriptor;
 import org.rocksdb.ColumnFamilyHandle;
 import org.rocksdb.ColumnFamilyOptions;
@@ -61,13 +60,11 @@
   private transient ConcurrentHashMap<String, ColumnFamilyDescriptor> managedDescriptorMap;
   private transient RocksDB rocksDB;
   private boolean closed = false;
-  private final String basePath;
   private final String rocksDBBasePath;
 
   public RocksDBDAO(String basePath, String rocksDBBasePath) {
-    this.basePath = basePath;
     this.rocksDBBasePath =
-        String.format("%s/%s/%s", rocksDBBasePath, this.basePath.replace("/", "_"), UUID.randomUUID().toString());
+        String.format("%s/%s/%s", rocksDBBasePath, basePath.replace("/", "_"), UUID.randomUUID().toString());
     init();
   }
 
@@ -107,7 +104,7 @@ protected void log(InfoLogLevel infoLogLevel, String logMsg) {
       FileIOUtils.mkdir(new File(rocksDBBasePath));
       rocksDB = RocksDB.open(dbOptions, rocksDBBasePath, managedColumnFamilies, managedHandles);
 
-      Preconditions.checkArgument(managedHandles.size() == managedColumnFamilies.size(),
+      ValidationUtils.checkArgument(managedHandles.size() == managedColumnFamilies.size(),
           "Unexpected number of handles are returned");
       for (int index = 0; index < managedHandles.size(); index++) {
         ColumnFamilyHandle handle = managedHandles.get(index);
@@ -115,7 +112,7 @@ protected void log(InfoLogLevel infoLogLevel, String logMsg) {
         String familyNameFromHandle = new String(handle.getName());
         String familyNameFromDescriptor = new String(descriptor.getName());
 
-        Preconditions.checkArgument(familyNameFromDescriptor.equals(familyNameFromHandle),
+        ValidationUtils.checkArgument(familyNameFromDescriptor.equals(familyNameFromHandle),
             "Family Handles not in order with descriptors");
         managedHandlesMap.put(familyNameFromHandle, handle);
         managedDescriptorMap.put(familyNameFromDescriptor, descriptor);
@@ -153,14 +150,11 @@ private static ColumnFamilyDescriptor getColumnFamilyDescriptor(byte[] columnFam
    * Perform a batch write operation.
    */
   public void writeBatch(BatchHandler handler) {
-    WriteBatch batch = new WriteBatch();
-    try {
+    try (WriteBatch batch = new WriteBatch()) {
       handler.apply(batch);
       getRocksDB().write(new WriteOptions(), batch);
     } catch (RocksDBException re) {
       throw new HoodieException(re);
-    } finally {
-      batch.close();
     }
   }
 
@@ -302,7 +296,7 @@ public void delete(String columnFamilyName, String key) {
    * @param <T> Type of object stored.
    */
   public <T extends Serializable> T get(String columnFamilyName, String key) {
-    Preconditions.checkArgument(!closed);
+    ValidationUtils.checkArgument(!closed);
     try {
       byte[] val = getRocksDB().get(managedHandlesMap.get(columnFamilyName), key.getBytes());
       return val == null ? null : SerializationUtils.deserialize(val);
@@ -319,7 +313,7 @@ public void delete(String columnFamilyName, String key) {
    * @param <T> Type of object stored.
    */
   public <K extends Serializable, T extends Serializable> T get(String columnFamilyName, K key) {
-    Preconditions.checkArgument(!closed);
+    ValidationUtils.checkArgument(!closed);
     try {
       byte[] val = getRocksDB().get(managedHandlesMap.get(columnFamilyName), SerializationUtils.serialize(key));
       return val == null ? null : SerializationUtils.deserialize(val);
@@ -336,7 +330,7 @@ public void delete(String columnFamilyName, String key) {
    * @param <T> Type of value stored
    */
   public <T extends Serializable> Stream<Pair<String, T>> prefixSearch(String columnFamilyName, String prefix) {
-    Preconditions.checkArgument(!closed);
+    ValidationUtils.checkArgument(!closed);
     final HoodieTimer timer = new HoodieTimer();
     timer.startTimer();
     long timeTakenMicro = 0;
@@ -365,7 +359,7 @@ public void delete(String columnFamilyName, String key) {
    * @param <T> Type of value stored
    */
   public <T extends Serializable> void prefixDelete(String columnFamilyName, String prefix) {
-    Preconditions.checkArgument(!closed);
+    ValidationUtils.checkArgument(!closed);
     LOG.info("Prefix DELETE (query=" + prefix + ") on " + columnFamilyName);
     final RocksIterator it = getRocksDB().newIterator(managedHandlesMap.get(columnFamilyName));
     it.seek(prefix.getBytes());
@@ -401,7 +395,7 @@ public void delete(String columnFamilyName, String key) {
    * @param columnFamilyName Column family name
    */
   public void addColumnFamily(String columnFamilyName) {
-    Preconditions.checkArgument(!closed);
+    ValidationUtils.checkArgument(!closed);
 
     managedDescriptorMap.computeIfAbsent(columnFamilyName, colFamilyName -> {
       try {
@@ -421,7 +415,7 @@ public void addColumnFamily(String columnFamilyName) {
    * @param columnFamilyName Column Family Name
    */
   public void dropColumnFamily(String columnFamilyName) {
-    Preconditions.checkArgument(!closed);
+    ValidationUtils.checkArgument(!closed);
 
     managedDescriptorMap.computeIfPresent(columnFamilyName, (colFamilyName, descriptor) -> {
       ColumnFamilyHandle handle = managedHandlesMap.get(colFamilyName);
@@ -442,9 +436,7 @@ public void dropColumnFamily(String columnFamilyName) {
   public synchronized void close() {
     if (!closed) {
       closed = true;
-      managedHandlesMap.values().forEach(columnFamilyHandle -> {
-        columnFamilyHandle.close();
-      });
+      managedHandlesMap.values().forEach(AbstractImmutableNativeReference::close);
       managedHandlesMap.clear();
       managedDescriptorMap.clear();
       getRocksDB().close();
@@ -456,7 +448,6 @@ public synchronized void close() {
     }
   }
 
-  @VisibleForTesting
   String getRocksDBBasePath() {
     return rocksDBBasePath;
   }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java
index 1c17e8334..9096080bb 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/SerializationUtils.java
@@ -41,7 +41,7 @@
 
   // Caching kryo serializer to avoid creating kryo instance for every serde operation
   private static final ThreadLocal<KryoSerializerInstance> SERIALIZER_REF =
-      ThreadLocal.withInitial(() -> new KryoSerializerInstance());
+      ThreadLocal.withInitial(KryoSerializerInstance::new);
 
   // Serialize
   // -----------------------------------------------------------------------
@@ -99,7 +99,7 @@
       kryo.setRegistrationRequired(false);
     }
 
-    byte[] serialize(Object obj) throws IOException {
+    byte[] serialize(Object obj) {
       kryo.reset();
       baos.reset();
       Output output = new Output(baos);
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/StringUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/StringUtils.java
index 63f02fe56..c6891b41f 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/StringUtils.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/StringUtils.java
@@ -18,6 +18,8 @@
 
 package org.apache.hudi.common.util;
 
+import javax.annotation.Nullable;
+
 /**
  * Simple utility for operations on strings.
  */
@@ -67,4 +69,45 @@ public static String toHexString(byte[] bytes) {
   public static boolean isNullOrEmpty(String str) {
     return str == null || str.length() == 0;
   }
+
+  /**
+   * Returns the given string if it is non-null; the empty string otherwise.
+   *
+   * @param string the string to test and possibly return
+   * @return {@code string} itself if it is non-null; {@code ""} if it is null
+   */
+  public static String nullToEmpty(@Nullable String string) {
+    return string == null ? "" : string;
+  }
+
+  /**
+   * Returns the given string if it is nonempty; {@code null} otherwise.
+   *
+   * @param string the string to test and possibly return
+   * @return {@code string} itself if it is nonempty; {@code null} if it is empty or null
+   */
+  public static @Nullable String emptyToNull(@Nullable String string) {
+    return stringIsNullOrEmpty(string) ? null : string;
+  }
+
+  private static boolean stringIsNullOrEmpty(@Nullable String string) {
+    return string == null || string.isEmpty();
+  }
+
+  /**
+   * Convert a Signed Hash to Hex.
+   * @param bytes - hashed bytes
+   * @return Hex representation of hash
+   */
+  public static String bytesToHexString(final byte[] bytes) {
+    StringBuilder stringBuilder = new StringBuilder();
+    for (byte aByte : bytes) {
+      String hex = Integer.toHexString(0xff & aByte);
+      if (bytes.length == 1) {
+        stringBuilder.append('0');
+      }
+      stringBuilder.append(hex);
+    }
+    return stringBuilder.toString();
+  }
 }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/TypedProperties.java b/hudi-common/src/main/java/org/apache/hudi/common/util/TypedProperties.java
index ed21f341a..784c8b9df 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/TypedProperties.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/TypedProperties.java
@@ -61,37 +61,37 @@ public String getString(String property, String defaultValue) {
 
   public int getInteger(String property) {
     checkKey(property);
-    return Integer.valueOf(getProperty(property));
+    return Integer.parseInt(getProperty(property));
   }
 
   public int getInteger(String property, int defaultValue) {
-    return containsKey(property) ? Integer.valueOf(getProperty(property)) : defaultValue;
+    return containsKey(property) ? Integer.parseInt(getProperty(property)) : defaultValue;
   }
 
   public long getLong(String property) {
     checkKey(property);
-    return Long.valueOf(getProperty(property));
+    return Long.parseLong(getProperty(property));
   }
 
   public long getLong(String property, long defaultValue) {
-    return containsKey(property) ? Long.valueOf(getProperty(property)) : defaultValue;
+    return containsKey(property) ? Long.parseLong(getProperty(property)) : defaultValue;
   }
 
   public boolean getBoolean(String property) {
     checkKey(property);
-    return Boolean.valueOf(getProperty(property));
+    return Boolean.parseBoolean(getProperty(property));
   }
 
   public boolean getBoolean(String property, boolean defaultValue) {
-    return containsKey(property) ? Boolean.valueOf(getProperty(property)) : defaultValue;
+    return containsKey(property) ? Boolean.parseBoolean(getProperty(property)) : defaultValue;
   }
 
   public double getDouble(String property) {
     checkKey(property);
-    return Double.valueOf(getProperty(property));
+    return Double.parseDouble(getProperty(property));
   }
 
   public double getDouble(String property, double defaultValue) {
-    return containsKey(property) ? Double.valueOf(getProperty(property)) : defaultValue;
+    return containsKey(property) ? Double.parseDouble(getProperty(property)) : defaultValue;
   }
 }
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/ValidationUtils.java b/hudi-common/src/main/java/org/apache/hudi/common/util/ValidationUtils.java
new file mode 100644
index 000000000..57a051aec
--- /dev/null
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/ValidationUtils.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hudi.common.util;
+
+/*
+ * Simple utility to test validation conditions
+ */
+public class ValidationUtils {
+
+  /**
+   * Ensures the truth of an expression.
+   */
+  public static void checkArgument(final boolean expression) {
+    if (!expression) {
+      throw new IllegalArgumentException();
+    }
+  }
+
+  /**
+   * Ensures the truth of an expression, throwing the custom errorMessage otherwise.
+   */
+  public static void checkArgument(final boolean expression, final String errorMessage) {
+    if (!expression) {
+      throw new IllegalArgumentException(errorMessage);
+    }
+  }
+
+  /**
+   * Ensures the truth of an expression involving the state of the calling instance, but not
+   * involving any parameters to the calling method.
+   *
+   * @param expression a boolean expression
+   * @throws IllegalStateException if {@code expression} is false
+   */
+  public static void checkState(final boolean expression) {
+    if (!expression) {
+      throw new IllegalStateException();
+    }
+  }
+
+  /**
+   * Ensures the truth of an expression involving the state of the calling instance, but not
+   * involving any parameters to the calling method.
+   *
+   * @param expression a boolean expression
+   * @param errorMessage the exception message to use if the check fails; will be converted to a
+   *     string using {@link String#valueOf(Object)}
+   * @throws IllegalStateException if {@code expression} is false
+   */
+  public static void checkState(final boolean expression, String errorMessage) {
+    if (!expression) {
+      throw new IllegalStateException(errorMessage);
+    }
+  }
+}
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java b/hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java
index fd211e0f7..38dcaaaf3 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/collection/RocksDBBasedMap.java
@@ -84,11 +84,7 @@ public R remove(Object key) {
 
   @Override
   public void putAll(Map<? extends K, ? extends R> m) {
-    getRocksDBDAO().writeBatch(batch -> {
-      m.entrySet().forEach(entry -> {
-        getRocksDBDAO().putInBatch(batch, columnFamilyName, entry.getKey(), entry.getValue());
-      });
-    });
+    getRocksDBDAO().writeBatch(batch -> m.forEach((key, value) -> getRocksDBDAO().putInBatch(batch, columnFamilyName, key, value)));
   }
 
   private RocksDBDAO getRocksDBDAO() {
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryQueue.java b/hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryQueue.java
index 2c5ce5d97..2c2f919cd 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryQueue.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/util/queue/BoundedInMemoryQueue.java
@@ -21,10 +21,9 @@
 import org.apache.hudi.common.util.DefaultSizeEstimator;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.SizeEstimator;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieException;
 
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.base.Preconditions;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
 
@@ -62,7 +61,6 @@
   // It indicates number of records to cache. We will be using sampled record's average size to
   // determine how many
   // records we should cache and will change (increase/decrease) permits accordingly.
-  @VisibleForTesting
   public final Semaphore rateLimiter = new Semaphore(1);
   // used for sampling records with "RECORD_SAMPLING_RATE" frequency.
   public final AtomicLong samplingRecordCounter = new AtomicLong(-1);
@@ -73,7 +71,7 @@
   // it holds the root cause of the exception in case either queueing records (consuming from
   // inputIterator) fails or
   // thread reading records from queue fails.
-  private final AtomicReference<Exception> hasFailed = new AtomicReference(null);
+  private final AtomicReference<Exception> hasFailed = new AtomicReference<>(null);
   // used for indicating that all the records from queue are read successfully.
   private final AtomicBoolean isReadDone = new AtomicBoolean(false);
   // used for indicating that all records have been enqueued
@@ -86,10 +84,8 @@
   private final QueueIterator iterator;
   // indicates rate limit (number of records to cache). it is updated whenever there is a change
   // in avg record size.
-  @VisibleForTesting
   public int currentRateLimit = 1;
   // indicates avg record size in bytes. It is updated whenever a new record is sampled.
-  @VisibleForTesting
   public long avgRecordSizeInBytes = 0;
   // indicates number of samples collected so far.
   private long numSamples = 0;
@@ -119,7 +115,6 @@ public BoundedInMemoryQueue(final long memoryLimit, final Function<I, O> transfo
     this.iterator = new QueueIterator();
   }
 
-  @VisibleForTesting
   public int size() {
     return this.queue.size();
   }
@@ -222,7 +217,7 @@ private boolean expectMoreRecords() {
   /**
    * Puts an empty entry to queue to denote termination.
    */
-  public void close() throws InterruptedException {
+  public void close() {
     // done queueing records notifying queue-reader.
     isWriteDone.set(true);
   }
@@ -267,7 +262,7 @@ public boolean hasNext() {
 
     @Override
     public O next() {
-      Preconditions.checkState(hasNext() && this.nextRecord != null);
+      ValidationUtils.checkState(hasNext() && this.nextRecord != null);
       final O ret = this.nextRecord;
       this.nextRecord = null;
       return ret;
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/versioning/MetadataMigrator.java b/hudi-common/src/main/java/org/apache/hudi/common/versioning/MetadataMigrator.java
index 3b2bec8d7..f92dcea34 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/versioning/MetadataMigrator.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/versioning/MetadataMigrator.java
@@ -19,10 +19,9 @@
 package org.apache.hudi.common.versioning;
 
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 
-import com.google.common.base.Preconditions;
-
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
@@ -75,8 +74,8 @@ public T upgradeToLatest(T metadata, int metadataVersion) {
    * @return Metadata conforming to the target version
    */
   public T migrateToVersion(T metadata, int metadataVersion, int targetVersion) {
-    Preconditions.checkArgument(targetVersion >= oldestVersion);
-    Preconditions.checkArgument(targetVersion <= latestVersion);
+    ValidationUtils.checkArgument(targetVersion >= oldestVersion);
+    ValidationUtils.checkArgument(targetVersion <= latestVersion);
     if (metadataVersion == targetVersion) {
       return metadata;
     } else if (metadataVersion > targetVersion) {
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV1MigrationHandler.java b/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV1MigrationHandler.java
index 998d4f04f..749d2a0b3 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV1MigrationHandler.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV1MigrationHandler.java
@@ -22,10 +22,10 @@
 import org.apache.hudi.avro.model.HoodieCleanPartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.common.versioning.AbstractMigratorBase;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.Path;
 
 import java.util.Map;
@@ -52,7 +52,7 @@ public HoodieCleanMetadata upgradeFrom(HoodieCleanMetadata input) {
 
   @Override
   public HoodieCleanMetadata downgradeFrom(HoodieCleanMetadata input) {
-    Preconditions.checkArgument(input.getVersion() == 2,
+    ValidationUtils.checkArgument(input.getVersion() == 2,
         "Input version is " + input.getVersion() + ". Must be 2");
     final Path basePath = new Path(metaClient.getBasePath());
 
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV2MigrationHandler.java b/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV2MigrationHandler.java
index 84896a031..14b288678 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV2MigrationHandler.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/versioning/clean/CleanV2MigrationHandler.java
@@ -21,10 +21,10 @@
 import org.apache.hudi.avro.model.HoodieCleanMetadata;
 import org.apache.hudi.avro.model.HoodieCleanPartitionMetadata;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.common.versioning.AbstractMigratorBase;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.Path;
 
 import java.util.List;
@@ -46,7 +46,7 @@ public Integer getManagedVersion() {
 
   @Override
   public HoodieCleanMetadata upgradeFrom(HoodieCleanMetadata input) {
-    Preconditions.checkArgument(input.getVersion() == 1,
+    ValidationUtils.checkArgument(input.getVersion() == 1,
         "Input version is " + input.getVersion() + ". Must be 1");
     HoodieCleanMetadata metadata = new HoodieCleanMetadata();
     metadata.setEarliestCommitToRetain(input.getEarliestCommitToRetain());
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV1MigrationHandler.java b/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV1MigrationHandler.java
index 3abd95218..d56d0fa14 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV1MigrationHandler.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV1MigrationHandler.java
@@ -22,9 +22,9 @@
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
 import org.apache.hudi.common.util.FSUtils;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.versioning.AbstractMigratorBase;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.Path;
 
 import java.util.ArrayList;
@@ -54,19 +54,18 @@ public HoodieCompactionPlan upgradeFrom(HoodieCompactionPlan input) {
 
   @Override
   public HoodieCompactionPlan downgradeFrom(HoodieCompactionPlan input) {
-    Preconditions.checkArgument(input.getVersion() == 2, "Input version is " + input.getVersion() + ". Must be 2");
+    ValidationUtils.checkArgument(input.getVersion() == 2, "Input version is " + input.getVersion() + ". Must be 2");
     HoodieCompactionPlan compactionPlan = new HoodieCompactionPlan();
     final Path basePath = new Path(metaClient.getBasePath());
     List<HoodieCompactionOperation> v1CompactionOperationList = new ArrayList<>();
     if (null != input.getOperations()) {
-      v1CompactionOperationList = input.getOperations().stream().map(inp -> {
-        return HoodieCompactionOperation.newBuilder().setBaseInstantTime(inp.getBaseInstantTime())
-            .setFileId(inp.getFileId()).setPartitionPath(inp.getPartitionPath()).setMetrics(inp.getMetrics())
-            .setDataFilePath(convertToV1Path(basePath, inp.getPartitionPath(), inp.getDataFilePath()))
-            .setDeltaFilePaths(inp.getDeltaFilePaths().stream()
-                .map(s -> convertToV1Path(basePath, inp.getPartitionPath(), s)).collect(Collectors.toList()))
-            .build();
-      }).collect(Collectors.toList());
+      v1CompactionOperationList = input.getOperations().stream().map(inp ->
+          HoodieCompactionOperation.newBuilder().setBaseInstantTime(inp.getBaseInstantTime())
+          .setFileId(inp.getFileId()).setPartitionPath(inp.getPartitionPath()).setMetrics(inp.getMetrics())
+          .setDataFilePath(convertToV1Path(basePath, inp.getPartitionPath(), inp.getDataFilePath()))
+          .setDeltaFilePaths(inp.getDeltaFilePaths().stream()
+              .map(s -> convertToV1Path(basePath, inp.getPartitionPath(), s)).collect(Collectors.toList()))
+          .build()).collect(Collectors.toList());
     }
     compactionPlan.setOperations(v1CompactionOperationList);
     compactionPlan.setExtraMetadata(input.getExtraMetadata());
diff --git a/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV2MigrationHandler.java b/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV2MigrationHandler.java
index cc73a2f3d..2830c4a13 100644
--- a/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV2MigrationHandler.java
+++ b/hudi-common/src/main/java/org/apache/hudi/common/versioning/compaction/CompactionV2MigrationHandler.java
@@ -21,9 +21,9 @@
 import org.apache.hudi.avro.model.HoodieCompactionOperation;
 import org.apache.hudi.avro.model.HoodieCompactionPlan;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.versioning.AbstractMigratorBase;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.Path;
 
 import java.util.ArrayList;
@@ -48,7 +48,7 @@ public Integer getManagedVersion() {
 
   @Override
   public HoodieCompactionPlan upgradeFrom(HoodieCompactionPlan input) {
-    Preconditions.checkArgument(input.getVersion() == 1, "Input version is " + input.getVersion() + ". Must be 1");
+    ValidationUtils.checkArgument(input.getVersion() == 1, "Input version is " + input.getVersion() + ". Must be 1");
     HoodieCompactionPlan compactionPlan = new HoodieCompactionPlan();
     List<HoodieCompactionOperation> v2CompactionOperationList = new ArrayList<>();
     if (null != input.getOperations()) {
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/minicluster/HdfsTestService.java b/hudi-common/src/test/java/org/apache/hudi/common/minicluster/HdfsTestService.java
index 9bc9a8d7b..5f34cd18c 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/minicluster/HdfsTestService.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/minicluster/HdfsTestService.java
@@ -21,8 +21,6 @@
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.util.FileIOUtils;
 
-import com.google.common.base.Preconditions;
-import com.google.common.io.Files;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DFSConfigKeys;
@@ -32,6 +30,7 @@
 
 import java.io.File;
 import java.io.IOException;
+import java.util.Objects;
 
 /**
  * An HDFS minicluster service implementation.
@@ -58,7 +57,7 @@
   private MiniDFSCluster miniDfsCluster;
 
   public HdfsTestService() {
-    workDir = Files.createTempDir().getAbsolutePath();
+    workDir = FileIOUtils.createTempDir().getAbsolutePath();
   }
 
   public Configuration getHadoopConf() {
@@ -66,7 +65,7 @@ public Configuration getHadoopConf() {
   }
 
   public MiniDFSCluster start(boolean format) throws IOException {
-    Preconditions.checkState(workDir != null, "The work dir must be set before starting cluster.");
+    Objects.requireNonNull(workDir, "The work dir must be set before starting cluster.");
     hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
 
     // If clean, then remove the work dir so we can start fresh.
@@ -79,7 +78,7 @@ public MiniDFSCluster start(boolean format) throws IOException {
 
     // Configure and start the HDFS cluster
     // boolean format = shouldFormatDFSCluster(localDFSLocation, clean);
-    hadoopConf = configureDFSCluster(hadoopConf, localDFSLocation, bindIP, namenodeRpcPort,
+    configureDFSCluster(hadoopConf, localDFSLocation, bindIP, namenodeRpcPort,
         datanodePort, datanodeIpcPort, datanodeHttpPort);
     miniDfsCluster = new MiniDFSCluster.Builder(hadoopConf).numDataNodes(1).format(format).checkDataNodeAddrConfig(true)
         .checkDataNodeHostConfig(true).build();
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/minicluster/ZookeeperTestService.java b/hudi-common/src/test/java/org/apache/hudi/common/minicluster/ZookeeperTestService.java
index 670be4461..6f46f951a 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/minicluster/ZookeeperTestService.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/minicluster/ZookeeperTestService.java
@@ -18,8 +18,8 @@
 
 package org.apache.hudi.common.minicluster;
 
-import com.google.common.base.Preconditions;
-import com.google.common.io.Files;
+import org.apache.hudi.common.util.FileIOUtils;
+
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileUtil;
 import org.apache.log4j.LogManager;
@@ -36,6 +36,7 @@
 import java.io.Reader;
 import java.net.InetSocketAddress;
 import java.net.Socket;
+import java.util.Objects;
 
 /**
  * A Zookeeper minicluster service implementation.
@@ -76,7 +77,7 @@
   private boolean started = false;
 
   public ZookeeperTestService(Configuration config) {
-    this.workDir = Files.createTempDir().getAbsolutePath();
+    this.workDir = FileIOUtils.createTempDir().getAbsolutePath();
     this.hadoopConf = config;
   }
 
@@ -85,7 +86,7 @@ public Configuration getHadoopConf() {
   }
 
   public ZooKeeperServer start() throws IOException, InterruptedException {
-    Preconditions.checkState(workDir != null, "The localBaseFsLocation must be set before starting cluster.");
+    Objects.requireNonNull(workDir, "The localBaseFsLocation must be set before starting cluster.");
 
     setupTestEnv();
     stop();
@@ -171,13 +172,10 @@ private static boolean waitForServerDown(int port, long timeout) {
     long start = System.currentTimeMillis();
     while (true) {
       try {
-        Socket sock = new Socket("localhost", port);
-        try {
+        try (Socket sock = new Socket("localhost", port)) {
           OutputStream outstream = sock.getOutputStream();
           outstream.write("stat".getBytes());
           outstream.flush();
-        } finally {
-          sock.close();
         }
       } catch (IOException e) {
         return true;
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/model/HoodieTestUtils.java b/hudi-common/src/test/java/org/apache/hudi/common/model/HoodieTestUtils.java
index 6f27dbce6..cfb373ab1 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/model/HoodieTestUtils.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/model/HoodieTestUtils.java
@@ -47,8 +47,6 @@
 import com.esotericsoftware.kryo.io.Input;
 import com.esotericsoftware.kryo.io.Output;
 import com.esotericsoftware.kryo.serializers.JavaSerializer;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
@@ -70,6 +68,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Calendar;
+import java.util.Collections;
 import java.util.Date;
 import java.util.HashMap;
 import java.util.Iterator;
@@ -125,7 +124,7 @@ public static String makeNewCommitTime() {
     return new SimpleDateFormat("yyyyMMddHHmmss").format(new Date());
   }
 
-  public static final void createCommitFiles(String basePath, String... commitTimes) throws IOException {
+  public static void createCommitFiles(String basePath, String... commitTimes) throws IOException {
     for (String commitTime : commitTimes) {
       new File(
           basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
@@ -139,11 +138,11 @@ public static final void createCommitFiles(String basePath, String... commitTime
     }
   }
 
-  public static final void createMetadataFolder(String basePath) {
+  public static void createMetadataFolder(String basePath) {
     new File(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME).mkdirs();
   }
 
-  public static final void createInflightCommitFiles(String basePath, String... commitTimes) throws IOException {
+  public static void createInflightCommitFiles(String basePath, String... commitTimes) throws IOException {
 
     for (String commitTime : commitTimes) {
       new File(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
@@ -153,7 +152,7 @@ public static final void createInflightCommitFiles(String basePath, String... co
     }
   }
 
-  public static final void createPendingCleanFiles(HoodieTableMetaClient metaClient, String... commitTimes) {
+  public static void createPendingCleanFiles(HoodieTableMetaClient metaClient, String... commitTimes) {
     for (String commitTime : commitTimes) {
       Arrays.asList(HoodieTimeline.makeRequestedCleanerFileName(commitTime),
           HoodieTimeline.makeInflightCleanerFileName(commitTime)).forEach(f -> {
@@ -180,19 +179,19 @@ public static final void createPendingCleanFiles(HoodieTableMetaClient metaClien
     }
   }
 
-  public static final String createNewDataFile(String basePath, String partitionPath, String commitTime)
+  public static String createNewDataFile(String basePath, String partitionPath, String commitTime)
       throws IOException {
     String fileID = UUID.randomUUID().toString();
     return createDataFile(basePath, partitionPath, commitTime, fileID);
   }
 
-  public static final String createNewMarkerFile(String basePath, String partitionPath, String commitTime)
+  public static String createNewMarkerFile(String basePath, String partitionPath, String commitTime)
       throws IOException {
     String fileID = UUID.randomUUID().toString();
     return createMarkerFile(basePath, partitionPath, commitTime, fileID);
   }
 
-  public static final String createDataFile(String basePath, String partitionPath, String commitTime, String fileID)
+  public static String createDataFile(String basePath, String partitionPath, String commitTime, String fileID)
       throws IOException {
     String folderPath = basePath + "/" + partitionPath + "/";
     new File(folderPath).mkdirs();
@@ -226,7 +225,7 @@ public static final String createNewLogFile(FileSystem fs, String basePath, Stri
     return fileID;
   }
 
-  public static final void createCompactionCommitFiles(FileSystem fs, String basePath, String... commitTimes)
+  public static void createCompactionCommitFiles(FileSystem fs, String basePath, String... commitTimes)
       throws IOException {
     for (String commitTime : commitTimes) {
       boolean createFile = fs.createNewFile(new Path(basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/"
@@ -237,7 +236,7 @@ public static final void createCompactionCommitFiles(FileSystem fs, String baseP
     }
   }
 
-  public static final void createCompactionRequest(HoodieTableMetaClient metaClient, String instant,
+  public static void createCompactionRequest(HoodieTableMetaClient metaClient, String instant,
       List<Pair<String, FileSlice>> fileSliceList) throws IOException {
     HoodieCompactionPlan plan = CompactionUtils.buildFromFileSlices(fileSliceList, Option.empty(), Option.empty());
     HoodieInstant compactionInstant = new HoodieInstant(State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, instant);
@@ -245,47 +244,47 @@ public static final void createCompactionRequest(HoodieTableMetaClient metaClien
         AvroUtils.serializeCompactionPlan(plan));
   }
 
-  public static final String getDataFilePath(String basePath, String partitionPath, String commitTime, String fileID) {
+  public static String getDataFilePath(String basePath, String partitionPath, String commitTime, String fileID) {
     return basePath + "/" + partitionPath + "/" + FSUtils.makeDataFileName(commitTime, DEFAULT_WRITE_TOKEN, fileID);
   }
 
-  public static final String getLogFilePath(String basePath, String partitionPath, String commitTime, String fileID,
+  public static String getLogFilePath(String basePath, String partitionPath, String commitTime, String fileID,
       Option<Integer> version) {
     return basePath + "/" + partitionPath + "/" + FSUtils.makeLogFileName(fileID, ".log", commitTime,
         version.orElse(DEFAULT_LOG_VERSION), HoodieLogFormat.UNKNOWN_WRITE_TOKEN);
   }
 
-  public static final String getCommitFilePath(String basePath, String commitTime) {
+  public static String getCommitFilePath(String basePath, String commitTime) {
     return basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + commitTime + HoodieTimeline.COMMIT_EXTENSION;
   }
 
-  public static final String getInflightCommitFilePath(String basePath, String commitTime) {
+  public static String getInflightCommitFilePath(String basePath, String commitTime) {
     return basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + commitTime
         + HoodieTimeline.INFLIGHT_COMMIT_EXTENSION;
   }
 
-  public static final String getRequestedCompactionFilePath(String basePath, String commitTime) {
+  public static String getRequestedCompactionFilePath(String basePath, String commitTime) {
     return basePath + "/" + HoodieTableMetaClient.AUXILIARYFOLDER_NAME + "/" + commitTime
         + HoodieTimeline.INFLIGHT_COMMIT_EXTENSION;
   }
 
-  public static final boolean doesDataFileExist(String basePath, String partitionPath, String commitTime,
+  public static boolean doesDataFileExist(String basePath, String partitionPath, String commitTime,
       String fileID) {
     return new File(getDataFilePath(basePath, partitionPath, commitTime, fileID)).exists();
   }
 
-  public static final boolean doesLogFileExist(String basePath, String partitionPath, String commitTime, String fileID,
+  public static boolean doesLogFileExist(String basePath, String partitionPath, String commitTime, String fileID,
       Option<Integer> version) {
     return new File(getLogFilePath(basePath, partitionPath, commitTime, fileID, version)).exists();
   }
 
-  public static final boolean doesCommitExist(String basePath, String commitTime) {
+  public static boolean doesCommitExist(String basePath, String commitTime) {
     return new File(
         basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + commitTime + HoodieTimeline.COMMIT_EXTENSION)
             .exists();
   }
 
-  public static final boolean doesInflightExist(String basePath, String commitTime) {
+  public static boolean doesInflightExist(String basePath, String commitTime) {
     return new File(
         basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + commitTime + HoodieTimeline.INFLIGHT_EXTENSION)
             .exists();
@@ -298,19 +297,16 @@ public static void createCleanFiles(HoodieTableMetaClient metaClient, String bas
     Path commitFile = new Path(
         basePath + "/" + HoodieTableMetaClient.METAFOLDER_NAME + "/" + HoodieTimeline.makeCleanerFileName(commitTime));
     FileSystem fs = FSUtils.getFs(basePath, configuration);
-    FSDataOutputStream os = fs.create(commitFile, true);
-    try {
+    try (FSDataOutputStream os = fs.create(commitFile, true)) {
       HoodieCleanStat cleanStats = new HoodieCleanStat(HoodieCleaningPolicy.KEEP_LATEST_FILE_VERSIONS,
           DEFAULT_PARTITION_PATHS[rand.nextInt(DEFAULT_PARTITION_PATHS.length)], new ArrayList<>(), new ArrayList<>(),
           new ArrayList<>(), commitTime);
       // Create the clean metadata
 
       HoodieCleanMetadata cleanMetadata =
-          CleanerUtils.convertCleanMetadata(metaClient, commitTime, Option.of(0L), Arrays.asList(cleanStats));
+          CleanerUtils.convertCleanMetadata(metaClient, commitTime, Option.of(0L), Collections.singletonList(cleanStats));
       // Write empty clean metadata
       os.write(AvroUtils.serializeCleanMetadata(cleanMetadata).get());
-    } finally {
-      os.close();
     }
   }
 
@@ -353,7 +349,7 @@ public static void writeRecordsToLogFiles(FileSystem fs, String basePath, Schema
           .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(location.getFileId())
           .overBaseCommit(location.getInstantTime()).withFs(fs).build();
 
-        Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+        Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
         header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, location.getInstantTime());
         header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
         logWriter.appendBlock(new HoodieAvroDataBlock(value.stream().map(r -> {
@@ -374,14 +370,14 @@ public static void writeRecordsToLogFiles(FileSystem fs, String basePath, Schema
 
   public static FileStatus[] listAllDataFilesInPath(FileSystem fs, String basePath) throws IOException {
     RemoteIterator<LocatedFileStatus> itr = fs.listFiles(new Path(basePath), true);
-    List<FileStatus> returns = Lists.newArrayList();
+    List<FileStatus> returns = new ArrayList<>();
     while (itr.hasNext()) {
       LocatedFileStatus status = itr.next();
       if (status.getPath().getName().contains(".parquet")) {
         returns.add(status);
       }
     }
-    return returns.toArray(new FileStatus[returns.size()]);
+    return returns.toArray(new FileStatus[0]);
   }
 
   public static List<String> monotonicIncreasingCommitTimestamps(int numTimestamps, int startSecsDelta) {
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java b/hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
index ab1d95ed8..d6b2ffc1b 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/table/TestHoodieTableMetaClient.java
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 
-import com.google.common.collect.Lists;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.IOUtils;
 import org.apache.hadoop.io.SequenceFile;
@@ -34,6 +33,7 @@
 import org.junit.Test;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.stream.Collectors;
 
 import static org.junit.Assert.assertArrayEquals;
@@ -120,8 +120,7 @@ public void checkArchiveCommitTimeline() throws IOException {
     HoodieInstant instant2 = new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, "2");
     HoodieInstant instant3 = new HoodieInstant(false, HoodieTimeline.COMMIT_ACTION, "3");
 
-    assertEquals(Lists.newArrayList(instant1, instant2, instant3),
-        archivedTimeline.getInstants().collect(Collectors.toList()));
+    assertEquals(Arrays.asList(instant1, instant2, instant3), archivedTimeline.getInstants().collect(Collectors.toList()));
 
     assertArrayEquals(new Text("data1").getBytes(), archivedTimeline.getInstantDetails(instant1).get());
     assertArrayEquals(new Text("data2").getBytes(), archivedTimeline.getInstantDetails(instant2).get());
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java b/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java
index f0a490251..273bed135 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java
@@ -39,7 +39,6 @@
 import org.apache.hudi.common.util.SchemaTestUtil;
 import org.apache.hudi.exception.CorruptedLogFileException;
 
-import com.google.common.collect.Maps;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
@@ -62,6 +61,7 @@
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
@@ -86,7 +86,7 @@
   private FileSystem fs;
   private Path partitionPath;
   private int bufferSize = 4096;
-  private Boolean readBlocksLazily = true;
+  private Boolean readBlocksLazily;
 
   public TestHoodieLogFormat(Boolean readBlocksLazily) {
     this.readBlocksLazily = readBlocksLazily;
@@ -139,7 +139,7 @@ public void testBasicAppend() throws IOException, InterruptedException, URISynta
         HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)
             .withFileId("test-fileid1").overBaseCommit("100").withFs(fs).build();
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -157,7 +157,7 @@ public void testRollover() throws IOException, InterruptedException, URISyntaxEx
         HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)
             .withFileId("test-fileid1").overBaseCommit("100").withFs(fs).build();
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -223,7 +223,7 @@ private void testConcurrentAppend(boolean logFileExists, boolean newLogFileForma
     HoodieLogFile logFile1 = writer.getLogFile();
     HoodieLogFile logFile2 = writer2.getLogFile();
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -241,7 +241,7 @@ public void testMultipleAppend() throws IOException, URISyntaxException, Interru
         HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)
             .withFileId("test-fileid1").overBaseCommit("100").withFs(fs).build();
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -285,7 +285,7 @@ public void testMultipleAppend() throws IOException, URISyntaxException, Interru
     }
   }
 
-  /**
+  /*
    * This is actually a test on concurrent append and not recovery lease. Commenting this out.
    * https://issues.apache.org/jira/browse/HUDI-117
    */
@@ -321,7 +321,7 @@ public void testAppendNotSupported() throws IOException, URISyntaxException, Int
 
     // Some data & append two times.
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -347,7 +347,7 @@ public void testBasicWriteAndScan() throws IOException, URISyntaxException, Inte
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords = records.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -376,7 +376,7 @@ public void testBasicAppendAndRead() throws IOException, URISyntaxException, Int
     Schema schema = getSimpleSchema();
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
@@ -440,7 +440,7 @@ public void testBasicAppendAndScanMultipleFiles() throws IOException, URISyntaxE
         HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)
             .withSizeThreshold(1024).withFileId("test-fileid1").overBaseCommit("100").withFs(fs).build();
     Schema schema = HoodieAvroUtils.addMetadataFields(getSimpleSchema());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
 
@@ -480,7 +480,7 @@ public void testAppendAndReadOnCorruptedLog() throws IOException, URISyntaxExcep
         HoodieLogFormat.newWriterBuilder().onParentPath(partitionPath).withFileExtension(HoodieLogFile.DELTA_EXTENSION)
             .withFileId("test-fileid1").overBaseCommit("100").withFs(fs).build();
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -566,7 +566,7 @@ public void testAvroLogRecordReaderBasic() throws IOException, URISyntaxExceptio
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
 
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
@@ -611,7 +611,7 @@ public void testAvroLogRecordReaderWithRollbackTombstone()
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
 
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
@@ -672,7 +672,7 @@ public void testAvroLogRecordReaderWithRollbackPartialBlock()
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
@@ -751,7 +751,7 @@ public void testAvroLogRecordReaderWithDeleteAndRollback()
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
@@ -836,7 +836,7 @@ public void testAvroLogRecordReaderWithFailedRollbacks()
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, "100");
 
@@ -903,7 +903,7 @@ public void testAvroLogRecordReaderWithInsertDeleteAndRollback()
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
@@ -950,7 +950,7 @@ public void testAvroLogRecordReaderWithInvalidRollback()
 
     // Write 1
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
@@ -990,7 +990,7 @@ public void testAvroLogRecordReaderWithInsertsDeleteAndRollback()
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
@@ -1037,7 +1037,7 @@ public void testAvroLogRecordReaderWithMixedInsertsCorruptsAndRollback()
 
     // Write 1
     List<IndexedRecord> records1 = SchemaTestUtil.generateHoodieTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
@@ -1123,7 +1123,7 @@ public void testBasicAppendAndReadInReverse() throws IOException, URISyntaxExcep
     List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
@@ -1190,7 +1190,7 @@ public void testAppendAndReadOnCorruptedLogInReverse() throws IOException, URISy
             .withFileId("test-fileid1").overBaseCommit("100").withFs(fs).build();
     Schema schema = getSimpleSchema();
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 100);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -1251,7 +1251,7 @@ public void testBasicAppendAndTraverseInReverse() throws IOException, URISyntaxE
     List<IndexedRecord> records1 = SchemaTestUtil.generateTestRecords(0, 100);
     List<IndexedRecord> copyOfRecords1 = records1.stream()
         .map(record -> HoodieAvroUtils.rewriteRecord((GenericRecord) record, schema)).collect(Collectors.toList());
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records1, header);
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormatAppendFailure.java b/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormatAppendFailure.java
index f09db9e42..091d2616f 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormatAppendFailure.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormatAppendFailure.java
@@ -25,7 +25,6 @@
 import org.apache.hudi.common.table.log.block.HoodieLogBlock;
 import org.apache.hudi.common.util.SchemaTestUtil;
 
-import com.google.common.collect.Maps;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
@@ -45,6 +44,7 @@
 import java.io.File;
 import java.io.IOException;
 import java.net.URISyntaxException;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.UUID;
@@ -97,7 +97,7 @@ public void testFailedToGetAppendStreamFromHDFSNameNode()
 
     // Some data & append.
     List<IndexedRecord> records = SchemaTestUtil.generateTestRecords(0, 10);
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, "100");
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, getSimpleSchema().toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java b/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java
index 88284028f..602d04a2d 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestHoodieTableFileSystemView.java
@@ -39,8 +39,6 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.collection.Pair;
 
-import com.google.common.collect.Lists;
-import com.google.common.collect.Sets;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
@@ -208,7 +206,7 @@ public void testViewForFileSlicesWithBaseFileAndInflightCompaction() throws Exce
    * @return
    */
   private Stream<FileSlice> getAllRawFileSlices(String partitionPath) {
-    return fsView.getAllFileGroups(partitionPath).map(group -> group.getAllFileSlicesIncludingInflight())
+    return fsView.getAllFileGroups(partitionPath).map(HoodieFileGroup::getAllFileSlicesIncludingInflight)
         .flatMap(sliceList -> sliceList);
   }
 
@@ -219,8 +217,8 @@ public void testViewForFileSlicesWithBaseFileAndInflightCompaction() throws Exce
    * @return
    */
   public Stream<FileSlice> getLatestRawFileSlices(String partitionPath) {
-    return fsView.getAllFileGroups(partitionPath).map(fileGroup -> fileGroup.getLatestFileSlicesIncludingInflight())
-        .filter(fileSliceOpt -> fileSliceOpt.isPresent()).map(Option::get);
+    return fsView.getAllFileGroups(partitionPath).map(HoodieFileGroup::getLatestFileSlicesIncludingInflight)
+        .filter(Option::isPresent).map(Option::get);
   }
 
   /**
@@ -274,7 +272,7 @@ protected void testViewForFileSlicesWithAsyncCompaction(boolean skipCreatingData
     partitionFileSlicesPairs.add(Pair.of(partitionPath, fileSlices.get(0)));
     HoodieCompactionPlan compactionPlan =
         CompactionUtils.buildFromFileSlices(partitionFileSlicesPairs, Option.empty(), Option.empty());
-    HoodieInstant compactionInstant = null;
+    HoodieInstant compactionInstant;
     if (isCompactionInFlight) {
       // Create a Data-file but this should be skipped by view
       new File(basePath + "/" + partitionPath + "/" + compactDataFileName).createNewFile();
@@ -384,7 +382,7 @@ protected void testViewForFileSlicesWithAsyncCompaction(boolean skipCreatingData
       dataFiles.forEach(df -> assertEquals("Expect data-file for instant 1 be returned", df.getCommitTime(), instantTime1));
     }
 
-    /** Inflight/Orphan File-groups needs to be in the view **/
+    /* Inflight/Orphan File-groups needs to be in the view **/
 
     // There is a data-file with this inflight file-id
     final String inflightFileId1 = UUID.randomUUID().toString();
@@ -506,7 +504,7 @@ protected void testViewForFileSlicesWithAsyncCompaction(boolean skipCreatingData
     assertEquals("Log File Order check", fileName4, logFiles.get(0).getFileName());
     assertEquals("Log File Order check", fileName3, logFiles.get(1).getFileName());
 
-    /** Data Files API tests */
+    /* Data Files API tests */
     dataFiles = roView.getLatestDataFiles().collect(Collectors.toList());
     assertEquals("Expect only one data-file to be sent", 1, dataFiles.size());
     dataFiles.forEach(df -> assertEquals("Expect data-file created by compaction be returned", df.getCommitTime(), compactionRequestedTime));
@@ -651,7 +649,7 @@ private void testStreamLatestVersionInPartition(boolean isLatestFileSliceOnly, S
     List<HoodieDataFile> dataFileList =
         roView.getLatestDataFilesBeforeOrOn("2016/05/01", commitTime4).collect(Collectors.toList());
     assertEquals(3, dataFileList.size());
-    Set<String> filenames = Sets.newHashSet();
+    Set<String> filenames = new HashSet<>();
     for (HoodieDataFile status : dataFileList) {
       filenames.add(status.getFileName());
     }
@@ -659,7 +657,7 @@ private void testStreamLatestVersionInPartition(boolean isLatestFileSliceOnly, S
     assertTrue(filenames.contains(FSUtils.makeDataFileName(commitTime3, TEST_WRITE_TOKEN, fileId2)));
     assertTrue(filenames.contains(FSUtils.makeDataFileName(commitTime4, TEST_WRITE_TOKEN, fileId3)));
 
-    filenames = Sets.newHashSet();
+    filenames = new HashSet<>();
     List<HoodieLogFile> logFilesList = rtView.getLatestFileSlicesBeforeOrOn("2016/05/01", commitTime4, true)
         .map(FileSlice::getLogFiles).flatMap(logFileList -> logFileList).collect(Collectors.toList());
     assertEquals(logFilesList.size(), 4);
@@ -678,7 +676,7 @@ private void testStreamLatestVersionInPartition(boolean isLatestFileSliceOnly, S
     // Reset the max commit time
     List<HoodieDataFile> dataFiles =
         roView.getLatestDataFilesBeforeOrOn("2016/05/01", commitTime3).collect(Collectors.toList());
-    filenames = Sets.newHashSet();
+    filenames = new HashSet<>();
     for (HoodieDataFile status : dataFiles) {
       filenames.add(status.getFileName());
     }
@@ -738,7 +736,7 @@ protected void testStreamEveryVersionInPartition(boolean isLatestFileSliceOnly)
 
     for (HoodieFileGroup fileGroup : fileGroups) {
       String fileId = fileGroup.getFileGroupId().getFileId();
-      Set<String> filenames = Sets.newHashSet();
+      Set<String> filenames = new HashSet<>();
       fileGroup.getAllDataFiles().forEach(dataFile -> {
         assertEquals("All same fileId should be grouped", fileId, dataFile.getFileId());
         filenames.add(dataFile.getFileName());
@@ -814,9 +812,10 @@ protected void testStreamLatestVersionInRange(boolean isLatestFileSliceOnly) thr
     roView.getAllDataFiles("2016/05/01/");
 
     List<HoodieDataFile> dataFiles =
-        roView.getLatestDataFilesInRange(Lists.newArrayList(commitTime2, commitTime3)).collect(Collectors.toList());
+        roView.getLatestDataFilesInRange(Stream.of(commitTime2, commitTime3).collect(Collectors.toList()))
+            .collect(Collectors.toList());
     assertEquals(isLatestFileSliceOnly ? 2 : 3, dataFiles.size());
-    Set<String> filenames = Sets.newHashSet();
+    Set<String> filenames = new HashSet<>();
     for (HoodieDataFile status : dataFiles) {
       filenames.add(status.getFileName());
     }
@@ -828,7 +827,8 @@ protected void testStreamLatestVersionInRange(boolean isLatestFileSliceOnly) thr
     }
 
     List<FileSlice> slices =
-        rtView.getLatestFileSliceInRange(Lists.newArrayList(commitTime3, commitTime4)).collect(Collectors.toList());
+        rtView.getLatestFileSliceInRange(Stream.of(commitTime3, commitTime4).collect(Collectors.toList()))
+            .collect(Collectors.toList());
     assertEquals(3, slices.size());
     for (FileSlice slice : slices) {
       if (slice.getFileId().equals(fileId1)) {
@@ -887,7 +887,7 @@ protected void testStreamLatestVersionsBefore(boolean isLatestFileSliceOnly) thr
         roView.getLatestDataFilesBeforeOrOn(partitionPath, commitTime2).collect(Collectors.toList());
     if (!isLatestFileSliceOnly) {
       assertEquals(2, dataFiles.size());
-      Set<String> filenames = Sets.newHashSet();
+      Set<String> filenames = new HashSet<>();
       for (HoodieDataFile status : dataFiles) {
         filenames.add(status.getFileName());
       }
@@ -982,7 +982,7 @@ protected void testStreamLatestVersions(boolean isLatestFileSliceOnly) throws IO
 
     List<HoodieDataFile> statuses1 = roView.getLatestDataFiles().collect(Collectors.toList());
     assertEquals(3, statuses1.size());
-    Set<String> filenames = Sets.newHashSet();
+    Set<String> filenames = new HashSet<>();
     for (HoodieDataFile status : statuses1) {
       filenames.add(status.getFileName());
     }
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java b/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
index 31ff91008..f34ed4cca 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/table/view/TestIncrementalFSViewSync.java
@@ -41,15 +41,15 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant.State;
 import org.apache.hudi.common.util.AvroUtils;
 import org.apache.hudi.common.util.CleanerUtils;
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.CompactionUtils;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 
-import com.google.common.base.Preconditions;
 import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Iterators;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
 import org.apache.log4j.Logger;
@@ -62,6 +62,7 @@
 import java.nio.charset.StandardCharsets;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
@@ -115,13 +116,13 @@ public void testAsyncCompaction() throws IOException {
     unscheduleCompaction(view, "14", "13", "11");
 
     // Add one more delta instant
-    instantsToFiles.putAll(testMultipleWriteSteps(view, Arrays.asList("15"), true, "11"));
+    instantsToFiles.putAll(testMultipleWriteSteps(view, Collections.singletonList("15"), true, "11"));
 
     // Schedule Compaction again
     scheduleCompaction(view, "16");
 
     // Run Compaction - This will be the second file-slice
-    testMultipleWriteSteps(view, Arrays.asList("16"), false, "16", 2);
+    testMultipleWriteSteps(view, Collections.singletonList("16"), false, "16", 2);
 
     // Run 2 more ingest
     instantsToFiles.putAll(testMultipleWriteSteps(view, Arrays.asList("17", "18"), true, "16", 2));
@@ -130,7 +131,7 @@ public void testAsyncCompaction() throws IOException {
     scheduleCompaction(view, "19");
 
     // Run one more ingestion after pending compaction. THis will be 3rd slice
-    instantsToFiles.putAll(testMultipleWriteSteps(view, Arrays.asList("20"), true, "19", 3));
+    instantsToFiles.putAll(testMultipleWriteSteps(view, Collections.singletonList("20"), true, "19", 3));
 
     // Clean first slice
     testCleans(view, Arrays.asList("21"),
@@ -138,17 +139,18 @@ public void testAsyncCompaction() throws IOException {
         instantsToFiles, Arrays.asList("11"));
 
     // Add one more ingestion instant. This should be 2nd slice now
-    instantsToFiles.putAll(testMultipleWriteSteps(view, Arrays.asList("22"), true, "19", 2));
+    instantsToFiles.putAll(testMultipleWriteSteps(view, Collections.singletonList("22"), true, "19", 2));
 
     // Restore last ingestion
-    testRestore(view, Arrays.asList("23"), true, new HashMap<>(), Arrays.asList("22"), "24", false);
+    testRestore(view, Collections.singletonList("23"), true, new HashMap<>(),
+        Collections.singletonList("22"), "24", false);
 
     // Run one more ingestion. THis is still 2nd slice
-    instantsToFiles.putAll(testMultipleWriteSteps(view, Arrays.asList("24"), true, "19", 2));
+    instantsToFiles.putAll(testMultipleWriteSteps(view, Collections.singletonList("24"), true, "19", 2));
 
     // Finish Compaction
-    instantsToFiles.putAll(testMultipleWriteSteps(view, Arrays.asList("19"), false, "19", 2,
-        Arrays.asList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "24"))));
+    instantsToFiles.putAll(testMultipleWriteSteps(view, Collections.singletonList("19"), false, "19", 2,
+        Collections.singletonList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "24"))));
   }
 
   @Test
@@ -198,13 +200,13 @@ public void testMultipleTransitions() throws IOException {
 
     SyncableFileSystemView view1 = getFileSystemView(metaClient);
     view1.sync();
-    Map<String, List<String>> instantsToFiles = null;
+    Map<String, List<String>> instantsToFiles;
 
-    /**
+    /*
      * Case where incremental syncing is catching up on more than one ingestion at a time
      */
     // Run 1 ingestion on MOR table (1 delta commits). View1 is now sync up to this point
-    instantsToFiles = testMultipleWriteSteps(view1, Arrays.asList("11"), true, "11");
+    instantsToFiles = testMultipleWriteSteps(view1, Collections.singletonList("11"), true, "11");
 
     SyncableFileSystemView view2 =
         getFileSystemView(new HoodieTableMetaClient(metaClient.getHadoopConf(), metaClient.getBasePath()));
@@ -213,7 +215,7 @@ public void testMultipleTransitions() throws IOException {
     instantsToFiles.putAll(testMultipleWriteSteps(view2, Arrays.asList("12", "13"), true, "11"));
 
     // Now Sync view1 and add 1 more ingestion. Check if view1 is able to catchup correctly
-    instantsToFiles.putAll(testMultipleWriteSteps(view1, Arrays.asList("14"), true, "11"));
+    instantsToFiles.putAll(testMultipleWriteSteps(view1, Collections.singletonList("14"), true, "11"));
 
     view2.sync();
     SyncableFileSystemView view3 =
@@ -221,8 +223,8 @@ public void testMultipleTransitions() throws IOException {
     view3.sync();
     areViewsConsistent(view1, view2, partitions.size() * fileIdsPerPartition.size());
 
-    /**
-     * Case where a compaction is scheduled and then unscheduled
+    /*
+      Case where a compaction is scheduled and then unscheduled
      */
     scheduleCompaction(view2, "15");
     unscheduleCompaction(view2, "15", "14", "11");
@@ -232,31 +234,31 @@ public void testMultipleTransitions() throws IOException {
         getFileSystemView(new HoodieTableMetaClient(metaClient.getHadoopConf(), metaClient.getBasePath()));
     view4.sync();
 
-    /**
+    /*
      * Case where a compaction is scheduled, 2 ingestion happens and then a compaction happens
      */
     scheduleCompaction(view2, "16");
     instantsToFiles.putAll(testMultipleWriteSteps(view2, Arrays.asList("17", "18"), true, "16", 2));
     // Compaction
-    testMultipleWriteSteps(view2, Arrays.asList("16"), false, "16", 2,
-        Arrays.asList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "18")));
+    testMultipleWriteSteps(view2, Collections.singletonList("16"), false, "16", 2,
+        Collections.singletonList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "18")));
     view1.sync();
     areViewsConsistent(view1, view2, partitions.size() * fileIdsPerPartition.size() * 2);
     SyncableFileSystemView view5 =
         getFileSystemView(new HoodieTableMetaClient(metaClient.getHadoopConf(), metaClient.getBasePath()));
     view5.sync();
 
-    /**
+    /*
      * Case where a clean happened and then rounds of ingestion and compaction happened
      */
-    testCleans(view2, Arrays.asList("19"),
+    testCleans(view2, Collections.singletonList("19"),
         new ImmutableMap.Builder<String, List<String>>().put("11", Arrays.asList("12", "13", "14")).build(),
         instantsToFiles, Arrays.asList("11"));
     scheduleCompaction(view2, "20");
     instantsToFiles.putAll(testMultipleWriteSteps(view2, Arrays.asList("21", "22"), true, "20", 2));
     // Compaction
-    testMultipleWriteSteps(view2, Arrays.asList("20"), false, "20", 2,
-        Arrays.asList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "22")));
+    testMultipleWriteSteps(view2, Collections.singletonList("20"), false, "20", 2,
+        Collections.singletonList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "22")));
     // Run one more round of ingestion
     instantsToFiles.putAll(testMultipleWriteSteps(view2, Arrays.asList("23", "24"), true, "20", 2));
     view1.sync();
@@ -265,17 +267,17 @@ public void testMultipleTransitions() throws IOException {
         getFileSystemView(new HoodieTableMetaClient(metaClient.getHadoopConf(), metaClient.getBasePath()));
     view6.sync();
 
-    /**
+    /*
      * Case where multiple restores and ingestions happened
      */
-    testRestore(view2, Arrays.asList("25"), true, new HashMap<>(), Arrays.asList("24"), "29", true);
-    testRestore(view2, Arrays.asList("26"), true, new HashMap<>(), Arrays.asList("23"), "29", false);
-    instantsToFiles.putAll(testMultipleWriteSteps(view2, Arrays.asList("27"), true, "20", 2));
+    testRestore(view2, Collections.singletonList("25"), true, new HashMap<>(), Collections.singletonList("24"), "29", true);
+    testRestore(view2, Collections.singletonList("26"), true, new HashMap<>(), Collections.singletonList("23"), "29", false);
+    instantsToFiles.putAll(testMultipleWriteSteps(view2, Collections.singletonList("27"), true, "20", 2));
     scheduleCompaction(view2, "28");
-    instantsToFiles.putAll(testMultipleWriteSteps(view2, Arrays.asList("29"), true, "28", 3));
+    instantsToFiles.putAll(testMultipleWriteSteps(view2, Collections.singletonList("29"), true, "28", 3));
     // Compaction
-    testMultipleWriteSteps(view2, Arrays.asList("28"), false, "28", 3,
-        Arrays.asList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "29")));
+    testMultipleWriteSteps(view2, Collections.singletonList("28"), false, "28", 3,
+        Collections.singletonList(new HoodieInstant(State.COMPLETED, HoodieTimeline.DELTA_COMMIT_ACTION, "29")));
 
     Arrays.asList(view1, view2, view3, view4, view5, view6).forEach(v -> {
       v.sync();
@@ -371,8 +373,7 @@ private void testRestore(SyncableFileSystemView view, List<String> newRestoreIns
         LOG.info("Last Instant is :" + view.getLastInstant().get());
         if (isRestore) {
           Assert.assertEquals(newRestoreInstants.get(idx), view.getLastInstant().get().getTimestamp());
-          Assert.assertEquals(isRestore ? HoodieTimeline.RESTORE_ACTION : HoodieTimeline.ROLLBACK_ACTION,
-              view.getLastInstant().get().getAction());
+          Assert.assertEquals(HoodieTimeline.RESTORE_ACTION, view.getLastInstant().get().getAction());
         }
         Assert.assertEquals(State.COMPLETED, view.getLastInstant().get().getState());
 
@@ -528,7 +529,7 @@ private void unscheduleCompaction(SyncableFileSystemView view, String compaction
       String newBaseInstant) throws IOException {
     HoodieInstant instant = new HoodieInstant(State.REQUESTED, COMPACTION_ACTION, compactionInstantTime);
     boolean deleted = metaClient.getFs().delete(new Path(metaClient.getMetaPath(), instant.getFileName()), false);
-    Preconditions.checkArgument(deleted, "Unable to delete compaction instant.");
+    ValidationUtils.checkArgument(deleted, "Unable to delete compaction instant.");
 
     view.sync();
     Assert.assertEquals(newLastInstant, view.getLastInstant().get().getTimestamp());
@@ -618,17 +619,11 @@ private void unscheduleCompaction(SyncableFileSystemView view, String compaction
       final long expTotalFileSlicesPerPartition = fileIdsPerPartition.size() * multiple;
       partitions.forEach(p -> Assert.assertEquals(expTotalFileSlicesPerPartition, view.getAllFileSlices(p).count()));
       if (deltaCommit) {
-        partitions.forEach(p -> {
-          view.getLatestFileSlices(p).forEach(f -> {
-            Assert.assertEquals(baseInstantForDeltaCommit, f.getBaseInstantTime());
-          });
-        });
+        partitions.forEach(p -> view.getLatestFileSlices(p)
+            .forEach(f -> Assert.assertEquals(baseInstantForDeltaCommit, f.getBaseInstantTime())));
       } else {
-        partitions.forEach(p -> {
-          view.getLatestDataFiles(p).forEach(f -> {
-            Assert.assertEquals(instant, f.getCommitTime());
-          });
-        });
+        partitions.forEach(p -> view.getLatestDataFiles(p)
+                .forEach(f -> Assert.assertEquals(instant, f.getCommitTime())));
       }
 
       metaClient.reloadActiveTimeline();
@@ -654,7 +649,7 @@ private void areViewsConsistent(SyncableFileSystemView view1, SyncableFileSystem
     HoodieTimeline timeline1 = view1.getTimeline();
     HoodieTimeline timeline2 = view2.getTimeline();
     Assert.assertEquals(view1.getLastInstant(), view2.getLastInstant());
-    Iterators.elementsEqual(timeline1.getInstants().iterator(), timeline2.getInstants().iterator());
+    CollectionUtils.elementsEqual(timeline1.getInstants().iterator(), timeline2.getInstants().iterator());
 
     // View Checks
     Map<HoodieFileGroupId, HoodieFileGroup> fileGroupsMap1 = partitions.stream().flatMap(view1::getAllFileGroups)
@@ -704,23 +699,22 @@ private void areViewsConsistent(SyncableFileSystemView view1, SyncableFileSystem
 
   private List<String> addInstant(HoodieTableMetaClient metaClient, String instant, boolean deltaCommit,
       String baseInstant) throws IOException {
-    List<Pair<String, HoodieWriteStat>> writeStats = partitions.stream().flatMap(p -> {
-      return fileIdsPerPartition.stream().map(f -> {
-        try {
-          File file = new File(basePath + "/" + p + "/"
-              + (deltaCommit
-                  ? FSUtils.makeLogFileName(f, ".log", baseInstant, Integer.parseInt(instant), TEST_WRITE_TOKEN)
-                  : FSUtils.makeDataFileName(instant, TEST_WRITE_TOKEN, f)));
-          file.createNewFile();
-          HoodieWriteStat w = new HoodieWriteStat();
-          w.setFileId(f);
-          w.setPath(String.format("%s/%s", p, file.getName()));
-          return Pair.of(p, w);
-        } catch (IOException e) {
-          throw new HoodieException(e);
-        }
-      });
-    }).collect(Collectors.toList());
+    List<Pair<String, HoodieWriteStat>> writeStats = partitions.stream()
+            .flatMap(p -> fileIdsPerPartition.stream().map(f -> {
+              try {
+                File file = new File(basePath + "/" + p + "/"
+                    + (deltaCommit
+                    ? FSUtils.makeLogFileName(f, ".log", baseInstant, Integer.parseInt(instant), TEST_WRITE_TOKEN)
+                    : FSUtils.makeDataFileName(instant, TEST_WRITE_TOKEN, f)));
+                file.createNewFile();
+                HoodieWriteStat w = new HoodieWriteStat();
+                w.setFileId(f);
+                w.setPath(String.format("%s/%s", p, file.getName()));
+                return Pair.of(p, w);
+              } catch (IOException e) {
+                throw new HoodieException(e);
+              }
+            })).collect(Collectors.toList());
 
     HoodieCommitMetadata metadata = new HoodieCommitMetadata();
     writeStats.forEach(e -> metadata.addWriteStat(e.getKey(), e.getValue()));
@@ -729,7 +723,7 @@ private void areViewsConsistent(SyncableFileSystemView view1, SyncableFileSystem
     metaClient.getActiveTimeline().createNewInstant(inflightInstant);
     metaClient.getActiveTimeline().saveAsComplete(inflightInstant,
         Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));
-    /**
+    /*
     // Delete pending compaction if present
     metaClient.getFs().delete(new Path(metaClient.getMetaPath(),
         new HoodieInstant(State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, instant).getFileName()));
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/util/CompactionTestUtils.java b/hudi-common/src/test/java/org/apache/hudi/common/util/CompactionTestUtils.java
index 65b11c4c3..fa14434fd 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/util/CompactionTestUtils.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/util/CompactionTestUtils.java
@@ -31,7 +31,6 @@
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;
 
-import com.google.common.collect.ImmutableList;
 import com.google.common.collect.ImmutableMap;
 import org.apache.hadoop.fs.Path;
 import org.junit.Assert;
@@ -86,8 +85,7 @@
         .put("002", "003").put("004", "005").put("006", "007").build();
     List<Integer> expectedNumEntries =
         Arrays.asList(numEntriesInPlan1, numEntriesInPlan2, numEntriesInPlan3, numEntriesInPlan4);
-    List<HoodieCompactionPlan> plans =
-        new ImmutableList.Builder<HoodieCompactionPlan>().add(plan1, plan2, plan3, plan4).build();
+    List<HoodieCompactionPlan> plans = CollectionUtils.createImmutableList(plan1, plan2, plan3, plan4);
     IntStream.range(0, 4).boxed().forEach(idx -> {
       if (expectedNumEntries.get(idx) > 0) {
         Assert.assertEquals("check if plan " + idx + " has exp entries", expectedNumEntries.get(idx).longValue(),
diff --git a/hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestDiskBasedMap.java b/hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestDiskBasedMap.java
index 76d7e0658..2cc726e13 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestDiskBasedMap.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/util/collection/TestDiskBasedMap.java
@@ -167,7 +167,7 @@ public void testSizeEstimator() throws IOException, URISyntaxException {
     schema = SchemaTestUtil.getSimpleSchema();
     List<IndexedRecord> indexedRecords = SchemaTestUtil.generateHoodieTestRecords(0, 1);
     hoodieRecords =
-        indexedRecords.stream().map(r -> new HoodieRecord(new HoodieKey(UUID.randomUUID().toString(), "0000/00/00"),
+        indexedRecords.stream().map(r -> new HoodieRecord<>(new HoodieKey(UUID.randomUUID().toString(), "0000/00/00"),
             new AvroBinaryTestPayload(Option.of((GenericRecord) r)))).collect(Collectors.toList());
     payloadSize = SpillableMapUtils.computePayloadSize(hoodieRecords.remove(0), new HoodieRecordSizeEstimator(schema));
     assertTrue(payloadSize > 0);
@@ -176,7 +176,7 @@ public void testSizeEstimator() throws IOException, URISyntaxException {
     final Schema simpleSchemaWithMetadata = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());
     indexedRecords = SchemaTestUtil.generateHoodieTestRecords(0, 1);
     hoodieRecords = indexedRecords.stream()
-        .map(r -> new HoodieRecord(new HoodieKey(UUID.randomUUID().toString(), "0000/00/00"),
+        .map(r -> new HoodieRecord<>(new HoodieKey(UUID.randomUUID().toString(), "0000/00/00"),
             new AvroBinaryTestPayload(
                 Option.of(HoodieAvroUtils.rewriteRecord((GenericRecord) r, simpleSchemaWithMetadata)))))
         .collect(Collectors.toList());
@@ -193,7 +193,7 @@ public void testSizeEstimatorPerformance() throws IOException, URISyntaxExceptio
     // Test sizeEstimatorPerformance with simpleSchema
     Schema schema = SchemaTestUtil.getSimpleSchema();
     List<HoodieRecord> hoodieRecords = SchemaTestUtil.generateHoodieTestRecords(0, 1, schema);
-    HoodieRecordSizeEstimator sizeEstimator = new HoodieRecordSizeEstimator(schema);
+    HoodieRecordSizeEstimator sizeEstimator = new HoodieRecordSizeEstimator<>(schema);
     HoodieRecord record = hoodieRecords.remove(0);
     long startTime = System.currentTimeMillis();
     SpillableMapUtils.computePayloadSize(record, sizeEstimator);
diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java
index cd1cea32e..f615dc575 100644
--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java
+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java
@@ -21,7 +21,6 @@
 import org.apache.hudi.hadoop.HoodieParquetInputFormat;
 import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;
 
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -65,11 +64,12 @@
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Set;
 import java.util.concurrent.Callable;
 import java.util.concurrent.ExecutionException;
@@ -115,7 +115,7 @@ public CheckNonCombinablePathCallable(Path[] paths, int start, int length, JobCo
 
     @Override
     public Set<Integer> call() throws Exception {
-      Set<Integer> nonCombinablePathIndices = new HashSet<Integer>();
+      Set<Integer> nonCombinablePathIndices = new HashSet<>();
       for (int i = 0; i < length; i++) {
         PartitionDesc part = HiveFileFormatUtils.getPartitionDescFromPathRecursively(pathToPartitionInfo,
             paths[i + start], IOPrepareCache.get().allocatePartitionDescMap());
@@ -332,8 +332,7 @@ public boolean equals(Object o) {
       if (o instanceof CombinePathInputFormat) {
         CombinePathInputFormat mObj = (CombinePathInputFormat) o;
         return (opList.equals(mObj.opList)) && (inputFormatClassName.equals(mObj.inputFormatClassName))
-            && (deserializerClassName == null ? (mObj.deserializerClassName == null)
-                : deserializerClassName.equals(mObj.deserializerClassName));
+            && (Objects.equals(deserializerClassName, mObj.deserializerClassName));
       }
       return false;
     }
@@ -352,11 +351,11 @@ public int hashCode() {
     init(job);
     Map<Path, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();
     Map<String, Operator<? extends OperatorDesc>> aliasToWork = mrwork.getAliasToWork();
-    /** MOD - Initialize a custom combine input format shim that will call listStatus on the custom inputFormat **/
+    /* MOD - Initialize a custom combine input format shim that will call listStatus on the custom inputFormat **/
     HoodieCombineHiveInputFormat.HoodieCombineFileInputFormatShim combine =
-        new HoodieCombineHiveInputFormat.HoodieCombineFileInputFormatShim();
+        new HoodieCombineHiveInputFormat.HoodieCombineFileInputFormatShim<>();
 
-    InputSplit[] splits = null;
+    InputSplit[] splits;
     if (combine == null) {
       splits = super.getSplits(job, numSplits);
       return splits;
@@ -365,16 +364,16 @@ public int hashCode() {
     if (combine.getInputPathsShim(job).length == 0) {
       throw new IOException("No input paths specified in job");
     }
-    ArrayList<InputSplit> result = new ArrayList<InputSplit>();
+    ArrayList<InputSplit> result = new ArrayList<>();
 
     // combine splits only from same tables and same partitions. Do not combine splits from multiple
     // tables or multiple partitions.
     Path[] paths = StringInternUtils.internUriStringsInPathArray(combine.getInputPathsShim(job));
 
-    List<Path> inpDirs = new ArrayList<Path>();
-    List<Path> inpFiles = new ArrayList<Path>();
-    Map<CombinePathInputFormat, CombineFilter> poolMap = new HashMap<CombinePathInputFormat, CombineFilter>();
-    Set<Path> poolSet = new HashSet<Path>();
+    List<Path> inpDirs = new ArrayList<>();
+    List<Path> inpFiles = new ArrayList<>();
+    Map<CombinePathInputFormat, CombineFilter> poolMap = new HashMap<>();
+    Set<Path> poolSet = new HashSet<>();
 
     for (Path path : paths) {
       PartitionDesc part = HiveFileFormatUtils.getPartitionDescFromPathRecursively(pathToPartitionInfo, path,
@@ -414,8 +413,8 @@ public int hashCode() {
       Path filterPath = path;
 
       // Does a pool exist for this path already
-      CombineFilter f = null;
-      List<Operator<? extends OperatorDesc>> opList = null;
+      CombineFilter f;
+      List<Operator<? extends OperatorDesc>> opList;
 
       if (!mrwork.isMapperCannotSpanPartns()) {
         // if mapper can span partitions, make sure a splits does not contain multiple
@@ -441,7 +440,7 @@ public int hashCode() {
         // parent directory will be grouped into one pool but not files from different parent
         // directories. This guarantees that a split will combine all files in the same partition
         // but won't cross multiple partitions if the user has asked so.
-        if (!path.getFileSystem(job).getFileStatus(path).isDir()) { // path is not directory
+        if (!path.getFileSystem(job).getFileStatus(path).isDirectory()) { // path is not directory
           filterPath = path.getParent();
           inpFiles.add(path);
           poolSet.add(filterPath);
@@ -452,7 +451,7 @@ public int hashCode() {
     }
 
     // Processing directories
-    List<CombineFileSplit> iss = new ArrayList<CombineFileSplit>();
+    List<CombineFileSplit> iss = new ArrayList<>();
     if (!mrwork.isMapperCannotSpanPartns()) {
       // mapper can span partitions
       // combine into as few as one split, subject to the PathFilters set
@@ -488,7 +487,6 @@ public int hashCode() {
   /**
    * Gets all the path indices that should not be combined.
    */
-  @VisibleForTesting
   public Set<Integer> getNonCombinablePathIndices(JobConf job, Path[] paths, int numThreads)
       throws ExecutionException, InterruptedException {
     LOG.info("Total number of paths: " + paths.length + ", launching " + numThreads
@@ -496,14 +494,14 @@ public int hashCode() {
     int numPathPerThread = (int) Math.ceil((double) paths.length / numThreads);
 
     ExecutorService executor = Executors.newFixedThreadPool(numThreads);
-    List<Future<Set<Integer>>> futureList = new ArrayList<Future<Set<Integer>>>(numThreads);
+    List<Future<Set<Integer>>> futureList = new ArrayList<>(numThreads);
     try {
       for (int i = 0; i < numThreads; i++) {
         int start = i * numPathPerThread;
         int length = i != numThreads - 1 ? numPathPerThread : paths.length - start;
         futureList.add(executor.submit(new CheckNonCombinablePathCallable(paths, start, length, job)));
       }
-      Set<Integer> nonCombinablePathIndices = new HashSet<Integer>();
+      Set<Integer> nonCombinablePathIndices = new HashSet<>();
       for (Future<Set<Integer>> future : futureList) {
         nonCombinablePathIndices.addAll(future.get());
       }
@@ -522,12 +520,12 @@ public int hashCode() {
     perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.GET_SPLITS);
     init(job);
 
-    ArrayList<InputSplit> result = new ArrayList<InputSplit>();
+    ArrayList<InputSplit> result = new ArrayList<>();
 
     Path[] paths = getInputPaths(job);
 
-    List<Path> nonCombinablePaths = new ArrayList<Path>(paths.length / 2);
-    List<Path> combinablePaths = new ArrayList<Path>(paths.length / 2);
+    List<Path> nonCombinablePaths = new ArrayList<>(paths.length / 2);
+    List<Path> combinablePaths = new ArrayList<>(paths.length / 2);
 
     int numThreads = Math.min(MAX_CHECK_NONCOMBINABLE_THREAD_NUM,
         (int) Math.ceil((double) paths.length / DEFAULT_NUM_PATH_PER_THREAD));
@@ -563,9 +561,7 @@ public int hashCode() {
     if (nonCombinablePaths.size() > 0) {
       FileInputFormat.setInputPaths(job, nonCombinablePaths.toArray(new Path[nonCombinablePaths.size()]));
       InputSplit[] splits = super.getSplits(job, numSplits);
-      for (InputSplit split : splits) {
-        result.add(split);
-      }
+      Collections.addAll(result, splits);
     }
 
     // Process the combine splits
@@ -574,9 +570,7 @@ public int hashCode() {
       Map<Path, PartitionDesc> pathToPartitionInfo = this.pathToPartitionInfo != null ? this.pathToPartitionInfo
           : Utilities.getMapWork(job).getPathToPartitionInfo();
       InputSplit[] splits = getCombineSplits(job, numSplits, pathToPartitionInfo);
-      for (InputSplit split : splits) {
-        result.add(split);
-      }
+      Collections.addAll(result, splits);
     }
 
     // Restore the old path information back
@@ -634,8 +628,8 @@ private void processPaths(JobConf job, CombineFileInputFormatShim combine, List<
    */
   private List<CombineFileSplit> sampleSplits(List<CombineFileSplit> splits) {
     HashMap<String, SplitSample> nameToSamples = mrwork.getNameToSplitSample();
-    List<CombineFileSplit> retLists = new ArrayList<CombineFileSplit>();
-    Map<String, ArrayList<CombineFileSplit>> aliasToSplitList = new HashMap<String, ArrayList<CombineFileSplit>>();
+    List<CombineFileSplit> retLists = new ArrayList<>();
+    Map<String, ArrayList<CombineFileSplit>> aliasToSplitList = new HashMap<>();
     Map<Path, ArrayList<String>> pathToAliases = mrwork.getPathToAliases();
     Map<Path, ArrayList<String>> pathToAliasesNoScheme = removeScheme(pathToAliases);
 
@@ -727,7 +721,7 @@ public RecordReader getRecordReader(InputSplit split, JobConf job, Reporter repo
     CombineHiveInputSplit hsplit = (CombineHiveInputSplit) split;
 
     String inputFormatClassName = null;
-    Class inputFormatClass = null;
+    Class inputFormatClass;
     try {
       inputFormatClassName = hsplit.inputFormatClassName();
       inputFormatClass = job.getClassByName(inputFormatClassName);
@@ -764,7 +758,7 @@ public void addPath(Path p) {
     @Override
     public boolean accept(Path path) {
       boolean find = false;
-      while (path != null && !find) {
+      while (path != null) {
         if (pStrings.contains(path.toUri().getPath())) {
           find = true;
           break;
@@ -838,19 +832,12 @@ public void createPool(JobConf conf, PathFilter... filters) {
           input = new HoodieParquetInputFormat();
         }
         input.setConf(job.getConfiguration());
-        result = new ArrayList<FileStatus>(Arrays.asList(input.listStatus(new JobConf(job.getConfiguration()))));
+        result = new ArrayList<>(Arrays.asList(input.listStatus(new JobConf(job.getConfiguration()))));
       } else {
         result = super.listStatus(job);
       }
 
-      Iterator it = result.iterator();
-
-      while (it.hasNext()) {
-        FileStatus stat = (FileStatus) it.next();
-        if (!stat.isFile()) {
-          it.remove();
-        }
-      }
+      result.removeIf(stat -> !stat.isFile());
       return result;
     }
 
@@ -870,17 +857,17 @@ public void createPool(JobConf conf, PathFilter... filters) {
       }
 
       InputSplit[] splits = super.getSplits(job, numSplits);
-      ArrayList inputSplitShims = new ArrayList();
+      List<InputSplitShim> inputSplitShims = new ArrayList<>();
 
-      for (int pos = 0; pos < splits.length; ++pos) {
-        CombineFileSplit split = (CombineFileSplit) splits[pos];
+      for (InputSplit inputSplit : splits) {
+        CombineFileSplit split = (CombineFileSplit) inputSplit;
         if (split.getPaths().length > 0) {
-          inputSplitShims.add(new HadoopShimsSecure.InputSplitShim(job, split.getPaths(), split.getStartOffsets(),
+          inputSplitShims.add(new InputSplitShim(job, split.getPaths(), split.getStartOffsets(),
               split.getLengths(), split.getLocations()));
         }
       }
 
-      return (CombineFileSplit[]) inputSplitShims.toArray(new HadoopShimsSecure.InputSplitShim[inputSplitShims.size()]);
+      return inputSplitShims.toArray(new CombineFileSplit[0]);
     }
 
     @Override
diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
index e7f3f0831..8ad4200c9 100644
--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java
@@ -27,13 +27,12 @@
 import org.apache.hudi.common.table.view.HoodieTableFileSystemView;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.hadoop.HoodieParquetInputFormat;
 import org.apache.hudi.hadoop.UseFileSplitsFromInputFormat;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.Sets;
 import org.apache.hadoop.conf.Configurable;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
@@ -108,7 +107,7 @@
     // for all unique split parents, obtain all delta files based on delta commit timeline,
     // grouped on file id
     List<HoodieRealtimeFileSplit> rtSplits = new ArrayList<>();
-    partitionsToParquetSplits.keySet().stream().forEach(partitionPath -> {
+    partitionsToParquetSplits.keySet().forEach(partitionPath -> {
       // for each partition path obtain the data & log file groupings, then map back to inputsplits
       HoodieTableMetaClient metaClient = partitionsToMetaClient.get(partitionPath);
       HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline());
@@ -135,8 +134,8 @@
               // Get the maxCommit from the last delta or compaction or commit - when
               // bootstrapped from COW table
               String maxCommitTime = metaClient
-                  .getActiveTimeline().getTimelineOfActions(Sets.newHashSet(HoodieTimeline.COMMIT_ACTION,
-                      HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))
+                  .getActiveTimeline().getTimelineOfActions(Stream.of(HoodieTimeline.COMMIT_ACTION,
+                      HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION).collect(Collectors.toSet()))
                   .filterCompletedInstants().lastInstant().get().getTimestamp();
               rtSplits.add(new HoodieRealtimeFileSplit(split, metaClient.getBasePath(), logFilePaths, maxCommitTime));
             } catch (IOException e) {
@@ -247,7 +246,7 @@ private static Configuration cleanProjectionColumnIds(Configuration conf) {
     LOG.info("Creating record reader with readCols :" + job.get(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR)
         + ", Ids :" + job.get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));
     // sanity check
-    Preconditions.checkArgument(split instanceof HoodieRealtimeFileSplit,
+    ValidationUtils.checkArgument(split instanceof HoodieRealtimeFileSplit,
         "HoodieRealtimeRecordReader can only work on HoodieRealtimeFileSplit and not with " + split);
 
     return new HoodieRealtimeRecordReader((HoodieRealtimeFileSplit) split, job,
diff --git a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
index f1a8eb87e..adfa8b002 100644
--- a/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
+++ b/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java
@@ -78,8 +78,7 @@ public RealtimeUnmergedRecordReader(HoodieRealtimeFileSplit split, JobConf job,
     this.iterator = this.executor.getQueue().iterator();
     this.logRecordScanner = new HoodieUnMergedLogRecordScanner(FSUtils.getFs(split.getPath().toString(), jobConf),
         split.getBasePath(), split.getDeltaFilePaths(), getReaderSchema(), split.getMaxCommitTime(),
-        Boolean
-            .valueOf(jobConf.get(COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP, DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED)),
+        Boolean.parseBoolean(jobConf.get(COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP, DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED)),
         false, jobConf.getInt(MAX_DFS_STREAM_BUFFER_SIZE_PROP, DEFAULT_MAX_DFS_STREAM_BUFFER_SIZE), record -> {
           // convert Hoodie log record to Hadoop AvroWritable and buffer
           GenericRecord rec = (GenericRecord) record.getData().getInsertValue(getReaderSchema()).get();
diff --git a/hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/realtime/TestHoodieRealtimeRecordReader.java b/hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/realtime/TestHoodieRealtimeRecordReader.java
index aaadebead..d8560acf6 100644
--- a/hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/realtime/TestHoodieRealtimeRecordReader.java
+++ b/hudi-hadoop-mr/src/test/java/org/apache/hudi/hadoop/realtime/TestHoodieRealtimeRecordReader.java
@@ -37,7 +37,6 @@
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.hadoop.InputFormatTestUtil;
 
-import com.google.common.collect.Maps;
 import org.apache.avro.Schema;
 import org.apache.avro.Schema.Field;
 import org.apache.avro.generic.IndexedRecord;
@@ -68,9 +67,10 @@
 import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
+import java.util.Collections;
 import java.util.List;
+import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
@@ -109,7 +109,7 @@ private Writer writeRollback(File partitionDir, Schema schema, String fileId, St
         .overBaseCommit(baseCommit).withFs(fs).withLogVersion(logVersion).withLogWriteToken("1-0-1")
         .withFileExtension(HoodieLogFile.DELTA_EXTENSION).build();
     // generate metadata
-    Map<HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HeaderMetadataType, String> header = new HashMap<>();
     header.put(HeaderMetadataType.INSTANT_TIME, newCommit);
     header.put(HeaderMetadataType.TARGET_INSTANT_TIME, rolledBackInstant);
     header.put(HeaderMetadataType.COMMAND_BLOCK_TYPE,
@@ -130,7 +130,7 @@ private Writer writeRollback(File partitionDir, Schema schema, String fileId, St
       records.add(SchemaTestUtil.generateAvroRecordFromJson(schema, i, newCommit, "fileid0"));
     }
     Schema writeSchema = records.get(0).getSchema();
-    Map<HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, newCommit);
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, writeSchema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
@@ -144,7 +144,7 @@ private Writer writeRollback(File partitionDir, Schema schema, String fileId, St
         .withFileExtension(HoodieLogFile.DELTA_EXTENSION).withFileId(fileId).overBaseCommit(baseCommit)
         .withLogVersion(logVersion).withFs(fs).build();
 
-    Map<HoodieLogBlock.HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HoodieLogBlock.HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, newCommit);
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     header.put(HoodieLogBlock.HeaderMetadataType.TARGET_INSTANT_TIME, oldCommit);
@@ -167,9 +167,9 @@ public void testNonPartitionedReader() throws Exception {
 
   private void setHiveColumnNameProps(List<Schema.Field> fields, JobConf jobConf, boolean isPartitioned) {
     String names = fields.stream().map(Field::name).collect(Collectors.joining(","));
-    String postions = fields.stream().map(f -> String.valueOf(f.pos())).collect(Collectors.joining(","));
+    String positions = fields.stream().map(f -> String.valueOf(f.pos())).collect(Collectors.joining(","));
     jobConf.set(ColumnProjectionUtils.READ_COLUMN_NAMES_CONF_STR, names);
-    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, postions);
+    jobConf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, positions);
 
     String hiveOrderedColumnNames = fields.stream().filter(field -> !field.name().equalsIgnoreCase(PARTITION_COLUMN))
         .map(Field::name).collect(Collectors.joining(","));
@@ -210,7 +210,7 @@ private void testReader(boolean partitioned) throws Exception {
             action.equals(HoodieTimeline.ROLLBACK_ACTION) ? String.valueOf(baseInstantTs + logVersion - 2)
                 : instantTime;
 
-        HoodieLogFormat.Writer writer = null;
+        HoodieLogFormat.Writer writer;
         if (action.equals(HoodieTimeline.ROLLBACK_ACTION)) {
           writer = writeRollback(partitionDir, schema, "fileid0", baseInstant, instantTime,
               String.valueOf(baseInstantTs + logVersion - 1), logVersion);
@@ -286,7 +286,7 @@ public void testUnMergedReader() throws Exception {
     String logFilePath = writer.getLogFile().getPath().toString();
     HoodieRealtimeFileSplit split = new HoodieRealtimeFileSplit(
         new FileSplit(new Path(partitionDir + "/fileid0_1-0-1_" + commitTime + ".parquet"), 0, 1, jobConf),
-        basePath.getRoot().getPath(), Arrays.asList(logFilePath), newCommitTime);
+        basePath.getRoot().getPath(), Collections.singletonList(logFilePath), newCommitTime);
 
     // create a RecordReader to be used by HoodieRealtimeRecordReader
     RecordReader<NullWritable, ArrayWritable> reader = new MapredParquetInputFormat().getRecordReader(
@@ -317,7 +317,7 @@ public void testUnMergedReader() throws Exception {
         numRecordsAtCommit2++;
         Assert.assertTrue(gotKey > firstBatchLastRecordKey);
         Assert.assertTrue(gotKey <= secondBatchLastRecordKey);
-        assertEquals((int) gotKey, lastSeenKeyFromLog + 1);
+        assertEquals(gotKey, lastSeenKeyFromLog + 1);
         lastSeenKeyFromLog++;
       } else {
         numRecordsAtCommit1++;
@@ -361,7 +361,7 @@ public void testReaderWithNestedAndComplexSchema() throws Exception {
     String logFilePath = writer.getLogFile().getPath().toString();
     HoodieRealtimeFileSplit split = new HoodieRealtimeFileSplit(
         new FileSplit(new Path(partitionDir + "/fileid0_1-0-1_" + commitTime + ".parquet"), 0, 1, jobConf),
-        basePath.getRoot().getPath(), Arrays.asList(logFilePath), newCommitTime);
+        basePath.getRoot().getPath(), Collections.singletonList(logFilePath), newCommitTime);
 
     // create a RecordReader to be used by HoodieRealtimeRecordReader
     RecordReader<NullWritable, ArrayWritable> reader = new MapredParquetInputFormat().getRecordReader(
diff --git a/hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java b/hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
index 7579f86ae..020ebb403 100644
--- a/hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
+++ b/hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java
@@ -18,6 +18,7 @@
 
 package org.apache.hudi.hive;
 
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hudi.common.model.HoodieCommitMetadata;
 import org.apache.hudi.common.model.HoodieFileFormat;
 import org.apache.hudi.common.model.HoodieLogFile;
@@ -28,14 +29,12 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieIOException;
 import org.apache.hudi.exception.InvalidTableException;
 import org.apache.hudi.hive.util.SchemaUtil;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -64,14 +63,12 @@
 import java.sql.SQLException;
 import java.sql.Statement;
 import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.stream.Collectors;
 
-@SuppressWarnings("ConstantConditions")
 public class HoodieHiveClient {
 
   private static final String HOODIE_LAST_COMMIT_TIME_SYNC = "last_commit_time_sync";
@@ -182,18 +179,18 @@ private String constructAddPartitions(List<String> partitions) {
    */
   private String getPartitionClause(String partition) {
     List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);
-    Preconditions.checkArgument(syncConfig.partitionFields.size() == partitionValues.size(),
+    ValidationUtils.checkArgument(syncConfig.partitionFields.size() == partitionValues.size(),
         "Partition key parts " + syncConfig.partitionFields + " does not match with partition values " + partitionValues
             + ". Check partition strategy. ");
     List<String> partBuilder = new ArrayList<>();
     for (int i = 0; i < syncConfig.partitionFields.size(); i++) {
       partBuilder.add("`" + syncConfig.partitionFields.get(i) + "`='" + partitionValues.get(i) + "'");
     }
-    return partBuilder.stream().collect(Collectors.joining(","));
+    return String.join(",", partBuilder);
   }
 
   private List<String> constructChangePartitions(List<String> partitions) {
-    List<String> changePartitions = Lists.newArrayList();
+    List<String> changePartitions = new ArrayList<>();
     // Hive 2.x doesn't like db.table name for operations, hence we need to change to using the database first
     String useDatabase = "USE " + HIVE_ESCAPE_CHARACTER + syncConfig.databaseName + HIVE_ESCAPE_CHARACTER;
     changePartitions.add(useDatabase);
@@ -215,7 +212,7 @@ private String getPartitionClause(String partition) {
    * Generate a list of PartitionEvent based on the changes required.
    */
   List<PartitionEvent> getPartitionEvents(List<Partition> tablePartitions, List<String> partitionStoragePartitions) {
-    Map<String, String> paths = Maps.newHashMap();
+    Map<String, String> paths = new HashMap<>();
     for (Partition tablePartition : tablePartitions) {
       List<String> hivePartitionValues = tablePartition.getValues();
       Collections.sort(hivePartitionValues);
@@ -224,7 +221,7 @@ private String getPartitionClause(String partition) {
       paths.put(String.join(", ", hivePartitionValues), fullTablePartitionPath);
     }
 
-    List<PartitionEvent> events = Lists.newArrayList();
+    List<PartitionEvent> events = new ArrayList<>();
     for (String storagePartition : partitionStoragePartitions) {
       Path storagePartitionPath = FSUtils.getPartitionPath(syncConfig.basePath, storagePartition);
       String fullStoragePartitionPath = Path.getPathWithoutSchemeAndAuthority(storagePartitionPath).toUri().getPath();
@@ -287,7 +284,7 @@ void createTable(MessageType storageSchema, String inputFormatClass, String outp
         throw new IllegalArgumentException(
             "Failed to get schema for table " + syncConfig.tableName + " does not exist");
       }
-      Map<String, String> schema = Maps.newHashMap();
+      Map<String, String> schema = new HashMap<>();
       ResultSet result = null;
       try {
         DatabaseMetaData databaseMetaData = connection.getMetaData();
@@ -320,10 +317,10 @@ void createTable(MessageType storageSchema, String inputFormatClass, String outp
       final long start = System.currentTimeMillis();
       Table table = this.client.getTable(syncConfig.databaseName, syncConfig.tableName);
       Map<String, String> partitionKeysMap =
-          table.getPartitionKeys().stream().collect(Collectors.toMap(f -> f.getName(), f -> f.getType().toUpperCase()));
+          table.getPartitionKeys().stream().collect(Collectors.toMap(FieldSchema::getName, f -> f.getType().toUpperCase()));
 
       Map<String, String> columnsMap =
-          table.getSd().getCols().stream().collect(Collectors.toMap(f -> f.getName(), f -> f.getType().toUpperCase()));
+          table.getSd().getCols().stream().collect(Collectors.toMap(FieldSchema::getName, f -> f.getType().toUpperCase()));
 
       Map<String, String> schema = new HashMap<>();
       schema.putAll(columnsMap);
@@ -388,11 +385,10 @@ public MessageType getDataSchema() {
                       // No Log files in Delta-Commit. Check if there are any parquet files
                       return commitMetadata.getFileIdAndFullPaths(metaClient.getBasePath()).values().stream()
                           .filter(s -> s.contains((metaClient.getTableConfig().getROFileFormat().getFileExtension())))
-                          .findAny().map(f -> Pair.of(f, HoodieFileFormat.PARQUET)).orElseThrow(() -> {
-                            return new IllegalArgumentException("Could not find any data file written for commit "
-                                + lastDeltaInstant + ", could not get schema for table " + metaClient.getBasePath()
-                                + ", CommitMetadata :" + commitMetadata);
-                          });
+                          .findAny().map(f -> Pair.of(f, HoodieFileFormat.PARQUET))
+                              .orElseThrow(() -> new IllegalArgumentException("Could not find any data file written for commit "
+                              + lastDeltaInstant + ", could not get schema for table " + metaClient.getBasePath()
+                              + ", CommitMetadata :" + commitMetadata));
                     });
             switch (filePathWithFormat.getRight()) {
               case HOODIE_LOG:
@@ -418,7 +414,6 @@ public MessageType getDataSchema() {
   /**
    * Read schema from a data file from the last compaction commit done.
    */
-  @SuppressWarnings("OptionalUsedAsFieldOrParameterType")
   private MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompactionCommitOpt) throws IOException {
     HoodieInstant lastCompactionCommit = lastCompactionCommitOpt.orElseThrow(() -> new HoodieHiveSyncException(
         "Could not read schema from last compaction, no compaction commits found on path " + syncConfig.basePath));
@@ -435,7 +430,6 @@ private MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompa
   /**
    * Read the schema from the log file on path.
    */
-  @SuppressWarnings("OptionalUsedAsFieldOrParameterType")
   private MessageType readSchemaFromLogFile(Option<HoodieInstant> lastCompactionCommitOpt, Path path)
       throws IOException {
     MessageType messageType = SchemaUtil.readSchemaFromLogFile(fs, path);
@@ -500,7 +494,7 @@ public void updateHiveSQL(String s) {
    * @param sql SQL statement to execute
    */
   public CommandProcessorResponse updateHiveSQLUsingHiveDriver(String sql) throws HoodieHiveSyncException {
-    List<CommandProcessorResponse> responses = updateHiveSQLs(Arrays.asList(sql));
+    List<CommandProcessorResponse> responses = updateHiveSQLs(Collections.singletonList(sql));
     return responses.get(responses.size() - 1);
   }
 
@@ -627,7 +621,6 @@ public void close() {
     }
   }
 
-  @SuppressWarnings("OptionalUsedAsFieldOrParameterType")
   List<String> getPartitionsWrittenToSince(Option<String> lastCommitTimeSynced) {
     if (!lastCommitTimeSynced.isPresent()) {
       LOG.info("Last commit time synced is not known, listing all partitions in " + syncConfig.basePath + ",FS :" + fs);
diff --git a/hudi-hive/src/main/java/org/apache/hudi/hive/MultiPartKeysValueExtractor.java b/hudi-hive/src/main/java/org/apache/hudi/hive/MultiPartKeysValueExtractor.java
index aa6ec307d..ec37a82a6 100644
--- a/hudi-hive/src/main/java/org/apache/hudi/hive/MultiPartKeysValueExtractor.java
+++ b/hudi-hive/src/main/java/org/apache/hudi/hive/MultiPartKeysValueExtractor.java
@@ -18,7 +18,7 @@
 
 package org.apache.hudi.hive;
 
-import com.google.common.base.Preconditions;
+import org.apache.hudi.common.util.ValidationUtils;
 
 import java.util.Arrays;
 import java.util.List;
@@ -35,7 +35,7 @@
     return Arrays.stream(splits).map(s -> {
       if (s.contains("=")) {
         String[] moreSplit = s.split("=");
-        Preconditions.checkArgument(moreSplit.length == 2, "Partition Field (" + s + ") not in expected format");
+        ValidationUtils.checkArgument(moreSplit.length == 2, "Partition Field (" + s + ") not in expected format");
         return moreSplit[1];
       }
       return s;
diff --git a/hudi-hive/src/main/java/org/apache/hudi/hive/SchemaDifference.java b/hudi-hive/src/main/java/org/apache/hudi/hive/SchemaDifference.java
index 21152cea5..ebb5fad55 100644
--- a/hudi-hive/src/main/java/org/apache/hudi/hive/SchemaDifference.java
+++ b/hudi-hive/src/main/java/org/apache/hudi/hive/SchemaDifference.java
@@ -19,12 +19,11 @@
 package org.apache.hudi.hive;
 
 import com.google.common.base.Objects;
-import com.google.common.collect.ImmutableList;
-import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
 import org.apache.parquet.schema.MessageType;
 
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -43,9 +42,9 @@ private SchemaDifference(MessageType storageSchema, Map<String, String> tableSch
       Map<String, String> updateColumnTypes, Map<String, String> addColumnTypes) {
     this.storageSchema = storageSchema;
     this.tableSchema = tableSchema;
-    this.deleteColumns = ImmutableList.copyOf(deleteColumns);
-    this.updateColumnTypes = ImmutableMap.copyOf(updateColumnTypes);
-    this.addColumnTypes = ImmutableMap.copyOf(addColumnTypes);
+    this.deleteColumns = Collections.unmodifiableList(deleteColumns);
+    this.updateColumnTypes = Collections.unmodifiableMap(updateColumnTypes);
+    this.addColumnTypes = Collections.unmodifiableMap(addColumnTypes);
   }
 
   public List<String> getDeleteColumns() {
@@ -85,9 +84,9 @@ public boolean isEmpty() {
     public Builder(MessageType storageSchema, Map<String, String> tableSchema) {
       this.storageSchema = storageSchema;
       this.tableSchema = tableSchema;
-      deleteColumns = Lists.newArrayList();
-      updateColumnTypes = Maps.newHashMap();
-      addColumnTypes = Maps.newHashMap();
+      deleteColumns = new ArrayList<>();
+      updateColumnTypes = new HashMap<>();
+      addColumnTypes = new HashMap<>();
     }
 
     public Builder deleteTableColumn(String column) {
diff --git a/hudi-hive/src/main/java/org/apache/hudi/hive/util/ColumnNameXLator.java b/hudi-hive/src/main/java/org/apache/hudi/hive/util/ColumnNameXLator.java
index fdf242e92..16fa6de58 100644
--- a/hudi-hive/src/main/java/org/apache/hudi/hive/util/ColumnNameXLator.java
+++ b/hudi-hive/src/main/java/org/apache/hudi/hive/util/ColumnNameXLator.java
@@ -18,14 +18,13 @@
 
 package org.apache.hudi.hive.util;
 
-import com.google.common.collect.Maps;
-
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.Map;
 
 public class ColumnNameXLator {
 
-  private static Map<String, String> xformMap = Maps.newHashMap();
+  private static Map<String, String> xformMap = new HashMap<>();
 
   public static String translateNestedColumn(String colName) {
     Map.Entry entry;
diff --git a/hudi-hive/src/main/java/org/apache/hudi/hive/util/SchemaUtil.java b/hudi-hive/src/main/java/org/apache/hudi/hive/util/SchemaUtil.java
index 16d9aae64..d14767c84 100644
--- a/hudi-hive/src/main/java/org/apache/hudi/hive/util/SchemaUtil.java
+++ b/hudi-hive/src/main/java/org/apache/hudi/hive/util/SchemaUtil.java
@@ -27,8 +27,6 @@
 import org.apache.hudi.hive.HoodieHiveSyncException;
 import org.apache.hudi.hive.SchemaDifference;
 
-import com.google.common.collect.Maps;
-import com.google.common.collect.Sets;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.log4j.LogManager;
@@ -43,10 +41,11 @@
 
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.LinkedHashMap;
 import java.util.List;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
-import java.util.stream.Collectors;
 
 /**
  * Schema Utilities.
@@ -69,7 +68,7 @@ public static SchemaDifference getSchemaDifference(MessageType storageSchema, Ma
     }
     LOG.info("Getting schema difference for " + tableSchema + "\r\n\r\n" + newTableSchema);
     SchemaDifference.Builder schemaDiffBuilder = SchemaDifference.newBuilder(storageSchema, tableSchema);
-    Set<String> tableColumns = Sets.newHashSet();
+    Set<String> tableColumns = new HashSet<>();
 
     for (Map.Entry<String, String> field : tableSchema.entrySet()) {
       String fieldName = field.getKey().toLowerCase();
@@ -142,7 +141,7 @@ private static boolean isFieldExistsInSchema(Map<String, String> newTableSchema,
    * @return : Hive Table schema read from parquet file MAP[String,String]
    */
   public static Map<String, String> convertParquetSchemaToHiveSchema(MessageType messageType) throws IOException {
-    Map<String, String> schema = Maps.newLinkedHashMap();
+    Map<String, String> schema = new LinkedHashMap<>();
     List<Type> parquetFields = messageType.getFields();
     for (Type parquetType : parquetFields) {
       StringBuilder result = new StringBuilder();
@@ -367,10 +366,9 @@ public static boolean isSchemaTypeUpdateAllowed(String prevType, String newType)
       return true;
     } else if (prevType.equalsIgnoreCase("float") && newType.equalsIgnoreCase("double")) {
       return true;
-    } else if (prevType.contains("struct") && newType.toLowerCase().contains("struct")) {
-      return true;
+    } else {
+      return prevType.contains("struct") && newType.toLowerCase().contains("struct");
     }
-    return false;
   }
 
   public static String generateSchemaString(MessageType storageSchema) throws IOException {
@@ -399,22 +397,20 @@ public static String generateCreateDDL(MessageType storageSchema, HiveSyncConfig
     List<String> partitionFields = new ArrayList<>();
     for (String partitionKey : config.partitionFields) {
       String partitionKeyWithTicks = tickSurround(partitionKey);
-      partitionFields.add(new StringBuilder().append(partitionKeyWithTicks).append(" ")
-          .append(getPartitionKeyType(hiveSchema, partitionKeyWithTicks)).toString());
+      partitionFields.add(partitionKeyWithTicks + " " + getPartitionKeyType(hiveSchema, partitionKeyWithTicks));
     }
 
-    String partitionsStr = partitionFields.stream().collect(Collectors.joining(","));
+    String partitionsStr = String.join(",", partitionFields);
     StringBuilder sb = new StringBuilder("CREATE EXTERNAL TABLE  IF NOT EXISTS ");
-    sb = sb.append(HIVE_ESCAPE_CHARACTER).append(config.databaseName).append(HIVE_ESCAPE_CHARACTER)
-            .append(".").append(HIVE_ESCAPE_CHARACTER).append(config.tableName).append(HIVE_ESCAPE_CHARACTER);
-    sb = sb.append("( ").append(columns).append(")");
+    sb.append(HIVE_ESCAPE_CHARACTER).append(config.databaseName).append(HIVE_ESCAPE_CHARACTER)
+        .append(".").append(HIVE_ESCAPE_CHARACTER).append(config.tableName).append(HIVE_ESCAPE_CHARACTER);
+    sb.append("( ").append(columns).append(")");
     if (!config.partitionFields.isEmpty()) {
-      sb = sb.append(" PARTITIONED BY (").append(partitionsStr).append(")");
+      sb.append(" PARTITIONED BY (").append(partitionsStr).append(")");
     }
-    sb = sb.append(" ROW FORMAT SERDE '").append(serdeClass).append("'");
-    sb = sb.append(" STORED AS INPUTFORMAT '").append(inputFormatClass).append("'");
-    sb = sb.append(" OUTPUTFORMAT '").append(outputFormatClass).append("' LOCATION '").append(config.basePath)
-        .append("'");
+    sb.append(" ROW FORMAT SERDE '").append(serdeClass).append("'");
+    sb.append(" STORED AS INPUTFORMAT '").append(inputFormatClass).append("'");
+    sb.append(" OUTPUTFORMAT '").append(outputFormatClass).append("' LOCATION '").append(config.basePath).append("'");
     return sb.toString();
   }
 
@@ -433,7 +429,6 @@ private static String getPartitionKeyType(Map<String, String> hiveSchema, String
    * 
    * @return
    */
-  @SuppressWarnings("OptionalUsedAsFieldOrParameterType")
   public static MessageType readSchemaFromLogFile(FileSystem fs, Path path) throws IOException {
     Reader reader = HoodieLogFormat.newReader(fs, new HoodieLogFile(path), null);
     HoodieAvroDataBlock lastBlock = null;
diff --git a/hudi-hive/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java b/hudi-hive/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
index 6639ea2fe..118e84950 100644
--- a/hudi-hive/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
+++ b/hudi-hive/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java
@@ -24,7 +24,6 @@
 import org.apache.hudi.hive.HoodieHiveClient.PartitionEvent.PartitionEventType;
 import org.apache.hudi.hive.util.SchemaUtil;
 
-import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.parquet.schema.MessageType;
 import org.apache.parquet.schema.OriginalType;
@@ -42,12 +41,13 @@
 import java.util.Arrays;
 import java.util.Collection;
 import java.util.List;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
 
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
 
-@SuppressWarnings("ConstantConditions")
 @RunWith(Parameterized.class)
 public class TestHiveSyncTool {
 
@@ -346,7 +346,7 @@ public void testMultiPartitionKeySync() throws Exception {
     HiveSyncConfig hiveSyncConfig = HiveSyncConfig.copy(TestUtil.hiveSyncConfig);
     hiveSyncConfig.partitionValueExtractorClass = MultiPartKeysValueExtractor.class.getCanonicalName();
     hiveSyncConfig.tableName = "multi_part_key";
-    hiveSyncConfig.partitionFields = Lists.newArrayList("year", "month", "day");
+    hiveSyncConfig.partitionFields = Stream.of("year", "month", "day").collect(Collectors.toList());
     TestUtil.getCreatedTablesSet().add(hiveSyncConfig.databaseName + "." + hiveSyncConfig.tableName);
 
     HoodieHiveClient hiveClient = new HoodieHiveClient(hiveSyncConfig, TestUtil.getHiveConf(), TestUtil.fileSystem);
diff --git a/hudi-hive/src/test/java/org/apache/hudi/hive/TestUtil.java b/hudi-hive/src/test/java/org/apache/hudi/hive/TestUtil.java
index 425854a08..71caf81ee 100644
--- a/hudi-hive/src/test/java/org/apache/hudi/hive/TestUtil.java
+++ b/hudi-hive/src/test/java/org/apache/hudi/hive/TestUtil.java
@@ -43,9 +43,6 @@
 import org.apache.hudi.common.util.SchemaTestUtil;
 import org.apache.hudi.hive.util.HiveTestService;
 
-import com.google.common.collect.Lists;
-import com.google.common.collect.Maps;
-import com.google.common.collect.Sets;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.conf.Configuration;
@@ -68,11 +65,16 @@
 import java.io.IOException;
 import java.net.URISyntaxException;
 import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
 import java.util.List;
+import java.util.HashMap;
+import java.util.HashSet;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
 import java.util.UUID;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
 
 import static org.junit.Assert.fail;
 
@@ -86,7 +88,7 @@
   static HiveSyncConfig hiveSyncConfig;
   private static DateTimeFormatter dtfOut;
   static FileSystem fileSystem;
-  private static Set<String> createdTablesSet = Sets.newHashSet();
+  private static Set<String> createdTablesSet = new HashSet<>();
 
   public static void setUp() throws IOException, InterruptedException {
     if (dfsCluster == null) {
@@ -114,7 +116,7 @@ public static void setUp() throws IOException, InterruptedException {
     hiveSyncConfig.basePath = "/tmp/hdfs/TestHiveSyncTool/";
     hiveSyncConfig.assumeDatePartitioning = true;
     hiveSyncConfig.usePreApacheInputFormat = false;
-    hiveSyncConfig.partitionFields = Lists.newArrayList("datestr");
+    hiveSyncConfig.partitionFields = Stream.of("datestr").collect(Collectors.toList());
 
     dtfOut = DateTimeFormat.forPattern("yyyy/MM/dd");
 
@@ -250,7 +252,7 @@ private static HoodieCommitMetadata createPartitions(int numberOfPartitions, boo
 
   private static List<HoodieWriteStat> createTestData(Path partPath, boolean isParquetSchemaSimple, String commitTime)
       throws IOException, URISyntaxException {
-    List<HoodieWriteStat> writeStats = Lists.newArrayList();
+    List<HoodieWriteStat> writeStats = new ArrayList<>();
     for (int i = 0; i < 5; i++) {
       // Create 5 files
       String fileId = UUID.randomUUID().toString();
@@ -298,7 +300,7 @@ private static HoodieLogFile generateLogData(Path parquetFilePath, boolean isLog
         .overBaseCommit(dataFile.getCommitTime()).withFs(fileSystem).build();
     List<IndexedRecord> records = (isLogSchemaSimple ? SchemaTestUtil.generateTestRecords(0, 100)
         : SchemaTestUtil.generateEvolvedTestRecords(100, 100));
-    Map<HeaderMetadataType, String> header = Maps.newHashMap();
+    Map<HeaderMetadataType, String> header = new HashMap<>();
     header.put(HoodieLogBlock.HeaderMetadataType.INSTANT_TIME, dataFile.getCommitTime());
     header.put(HoodieLogBlock.HeaderMetadataType.SCHEMA, schema.toString());
     HoodieAvroDataBlock dataBlock = new HoodieAvroDataBlock(records, header);
diff --git a/hudi-hive/src/test/java/org/apache/hudi/hive/util/HiveTestService.java b/hudi-hive/src/test/java/org/apache/hudi/hive/util/HiveTestService.java
index d82c33bd5..a2a32231a 100644
--- a/hudi-hive/src/test/java/org/apache/hudi/hive/util/HiveTestService.java
+++ b/hudi-hive/src/test/java/org/apache/hudi/hive/util/HiveTestService.java
@@ -21,9 +21,6 @@
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.util.FileIOUtils;
 
-import com.google.common.base.Preconditions;
-import com.google.common.collect.Maps;
-import com.google.common.io.Files;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -53,7 +50,9 @@
 import java.io.IOException;
 import java.net.InetSocketAddress;
 import java.net.SocketException;
+import java.util.HashMap;
 import java.util.Map;
+import java.util.Objects;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 
@@ -73,13 +72,13 @@
   private int serverPort = 9999;
   private boolean clean = true;
 
-  private Map<String, String> sysProps = Maps.newHashMap();
+  private Map<String, String> sysProps = new HashMap<>();
   private ExecutorService executorService;
   private TServer tServer;
   private HiveServer2 hiveServer;
 
   public HiveTestService(Configuration configuration) {
-    this.workDir = Files.createTempDir().getAbsolutePath();
+    this.workDir = FileIOUtils.createTempDir().getAbsolutePath();
   }
 
   public Configuration getHadoopConf() {
@@ -87,7 +86,7 @@ public Configuration getHadoopConf() {
   }
 
   public HiveServer2 start() throws IOException {
-    Preconditions.checkState(workDir != null, "The work dir must be set before starting cluster.");
+    Objects.requireNonNull(workDir, "The work dir must be set before starting cluster.");
 
     if (hadoopConf == null) {
       hadoopConf = HoodieTestUtils.getDefaultHadoopConf();
@@ -139,7 +138,7 @@ private HiveConf configureHive(Configuration conf, String localHiveLocation) thr
     File derbyLogFile = new File(localHiveDir, "derby.log");
     derbyLogFile.createNewFile();
     setSystemProperty("derby.stream.error.file", derbyLogFile.getPath());
-    conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, Files.createTempDir().getAbsolutePath());
+    conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, FileIOUtils.createTempDir().getAbsolutePath());
     conf.set("datanucleus.schema.autoCreateTables", "true");
     conf.set("hive.metastore.schema.verification", "false");
     setSystemProperty("derby.stream.error.file", derbyLogFile.getPath());
@@ -265,11 +264,11 @@ public TServer startMetaStore(String forceBindIP, int port, HiveConf conf) throw
             ? new ChainedTTransportFactory(new TFramedTransport.Factory(), new TUGIContainingTransport.Factory())
             : new TUGIContainingTransport.Factory();
 
-        processor = new TUGIBasedProcessor<IHMSHandler>(handler);
+        processor = new TUGIBasedProcessor<>(handler);
         LOG.info("Starting DB backed MetaStore Server with SetUGI enabled");
       } else {
         transFactory = useFramedTransport ? new TFramedTransport.Factory() : new TTransportFactory();
-        processor = new TSetIpAddressProcessor<IHMSHandler>(handler);
+        processor = new TSetIpAddressProcessor<>(handler);
         LOG.info("Starting DB backed MetaStore Server");
       }
 
@@ -278,12 +277,7 @@ public TServer startMetaStore(String forceBindIP, int port, HiveConf conf) throw
           .minWorkerThreads(minWorkerThreads).maxWorkerThreads(maxWorkerThreads);
 
       final TServer tServer = new TThreadPoolServer(args);
-      executorService.submit(new Runnable() {
-        @Override
-        public void run() {
-          tServer.serve();
-        }
-      });
+      executorService.submit(tServer::serve);
       return tServer;
     } catch (Throwable x) {
       throw new IOException(x);
diff --git a/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java b/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
index 39ac69484..4a227c6c9 100644
--- a/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
+++ b/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java
@@ -87,7 +87,7 @@
     cmd.add("hive.stats.autogather=false");
     cmd.add("-e");
     cmd.add("\"" + fullCommand + "\"");
-    return cmd.stream().toArray(String[]::new);
+    return cmd.toArray(new String[0]);
   }
 
   private static String getHiveConsoleCommandFile(String commandFile, String additionalVar) {
diff --git a/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java b/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
index 99a8d010e..af1fdc10f 100644
--- a/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
+++ b/hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestHoodieDemo.java
@@ -18,11 +18,13 @@
 
 package org.apache.hudi.integ;
 
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.collection.Pair;
 
 import com.google.common.collect.ImmutableList;
 import org.junit.Test;
 
+import java.util.Collections;
 import java.util.List;
 
 /**
@@ -105,8 +107,7 @@ private void setupDemo() throws Exception {
     executeCommandStringsInDocker(ADHOC_1_CONTAINER, cmds);
 
     // create input dir in presto coordinator
-    cmds = new ImmutableList.Builder<String>()
-        .add("mkdir -p " + HDFS_DATA_DIR).build();
+    cmds = Collections.singletonList("mkdir -p " + HDFS_DATA_DIR);
     executeCommandStringsInDocker(PRESTO_COORDINATOR, cmds);
 
     // copy presto sql files to presto coordinator
diff --git a/hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java b/hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java
index 8784526ef..87b2366d4 100644
--- a/hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java
+++ b/hudi-spark/src/main/java/org/apache/hudi/HoodieDataSourceHelpers.java
@@ -24,11 +24,11 @@
 import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 
-import com.google.common.collect.Sets;
 import org.apache.hadoop.fs.FileSystem;
 
 import java.util.List;
 import java.util.stream.Collectors;
+import java.util.stream.Stream;
 
 /**
  * List of helpers to aid, construction of instanttime for read and write operations using datasource.
@@ -68,7 +68,8 @@ public static HoodieTimeline allCompletedCommitsCompactions(FileSystem fs, Strin
     HoodieTableMetaClient metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);
     if (metaClient.getTableType().equals(HoodieTableType.MERGE_ON_READ)) {
       return metaClient.getActiveTimeline().getTimelineOfActions(
-          Sets.newHashSet(HoodieActiveTimeline.COMMIT_ACTION, HoodieActiveTimeline.DELTA_COMMIT_ACTION));
+          Stream.of(HoodieActiveTimeline.COMMIT_ACTION, HoodieActiveTimeline.DELTA_COMMIT_ACTION)
+                  .collect(Collectors.toSet()));
     } else {
       return metaClient.getCommitTimeline().filterCompletedInstants();
     }
diff --git a/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java b/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java
index 1ce31af3e..9e48d2a76 100644
--- a/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java
+++ b/hudi-timeline-service/src/main/java/org/apache/hudi/timeline/service/FileSystemViewHandler.java
@@ -28,13 +28,13 @@
 import org.apache.hudi.common.table.timeline.dto.TimelineDTO;
 import org.apache.hudi.common.table.view.FileSystemViewManager;
 import org.apache.hudi.common.table.view.RemoteHoodieTableFileSystemView;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.timeline.service.handlers.DataFileHandler;
 import org.apache.hudi.timeline.service.handlers.FileSliceHandler;
 import org.apache.hudi.timeline.service.handlers.TimelineHandler;
 
 import com.fasterxml.jackson.core.JsonProcessingException;
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.google.common.base.Preconditions;
 import io.javalin.Context;
 import io.javalin.Handler;
 import io.javalin.Javalin;
@@ -339,7 +339,7 @@ public void handle(@NotNull Context context) throws Exception {
                   + " but server has the following timeline "
                   + viewManager.getFileSystemView(context.queryParam(RemoteHoodieTableFileSystemView.BASEPATH_PARAM))
                       .getTimeline().getInstants().collect(Collectors.toList());
-          Preconditions.checkArgument(!isLocalViewBehind(context), errMsg);
+          ValidationUtils.checkArgument(!isLocalViewBehind(context), errMsg);
           long endFinalCheck = System.currentTimeMillis();
           finalCheckTimeTaken = endFinalCheck - beginFinalCheck;
         }
@@ -350,8 +350,7 @@ public void handle(@NotNull Context context) throws Exception {
       } finally {
         long endTs = System.currentTimeMillis();
         long timeTakenMillis = endTs - beginTs;
-        LOG
-            .info(String.format(
+        LOG.info(String.format(
                 "TimeTakenMillis[Total=%d, Refresh=%d, handle=%d, Check=%d], "
                     + "Success=%s, Query=%s, Host=%s, synced=%s",
                 timeTakenMillis, refreshCheckTimeTaken, handleTimeTaken, finalCheckTimeTaken, success,
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
index 856e68247..bab595bca 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java
@@ -35,7 +35,6 @@
 import com.beust.jcommander.JCommander;
 import com.beust.jcommander.Parameter;
 import com.beust.jcommander.ParameterException;
-import com.google.common.annotations.VisibleForTesting;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.hadoop.fs.FileSystem;
@@ -56,6 +55,7 @@
 import java.time.format.DateTimeFormatter;
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Properties;
 
@@ -81,7 +81,7 @@ public HDFSParquetImporter(Config cfg) {
     this.cfg = cfg;
   }
 
-  public static void main(String[] args) throws Exception {
+  public static void main(String[] args) {
     final Config cfg = new Config();
     JCommander cmd = new JCommander(cfg, null, args);
     if (cfg.help || args.length == 0) {
@@ -119,7 +119,6 @@ public int dataImport(JavaSparkContext jsc, int retry) {
     return ret;
   }
 
-  @VisibleForTesting
   protected int dataImport(JavaSparkContext jsc) throws IOException {
     try {
       if (fs.exists(new Path(cfg.targetPath))) {
@@ -160,8 +159,7 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {
     AvroReadSupport.setAvroReadSchema(jsc.hadoopConfiguration(), (new Schema.Parser().parse(schemaStr)));
     ParquetInputFormat.setReadSupportClass(job, (AvroReadSupport.class));
 
-    return jsc
-        .newAPIHadoopFile(cfg.srcPath, ParquetInputFormat.class, Void.class, GenericRecord.class,
+    return jsc.newAPIHadoopFile(cfg.srcPath, ParquetInputFormat.class, Void.class, GenericRecord.class,
             job.getConfiguration())
         // To reduce large number of tasks.
         .coalesce(16 * cfg.parallelism).map(entry -> {
@@ -198,7 +196,7 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {
    * @param <T> Type
    */
   protected <T extends HoodieRecordPayload> JavaRDD<WriteStatus> load(HoodieWriteClient client, String instantTime,
-      JavaRDD<HoodieRecord<T>> hoodieRecords) throws Exception {
+      JavaRDD<HoodieRecord<T>> hoodieRecords) {
     switch (cfg.command.toLowerCase()) {
       case "upsert": {
         return client.upsert(hoodieRecords, instantTime);
@@ -227,7 +225,7 @@ public void validate(String name, String value) throws ParameterException {
 
   public static class FormatValidator implements IValueValidator<String> {
 
-    List<String> validFormats = Arrays.asList("parquet");
+    List<String> validFormats = Collections.singletonList("parquet");
 
     @Override
     public void validate(String name, String value) throws ParameterException {
@@ -241,7 +239,7 @@ public void validate(String name, String value) throws ParameterException {
   public static class Config implements Serializable {
 
     @Parameter(names = {"--command", "-c"}, description = "Write command Valid values are insert(default)/upsert/bulkinsert",
-        required = false, validateValueWith = CommandValidator.class)
+        validateValueWith = CommandValidator.class)
     public String command = "INSERT";
     @Parameter(names = {"--src-path", "-sp"}, description = "Base path for the input table", required = true)
     public String srcPath = null;
@@ -260,14 +258,14 @@ public void validate(String name, String value) throws ParameterException {
     public int parallelism = 1;
     @Parameter(names = {"--schema-file", "-sf"}, description = "path for Avro schema file", required = true)
     public String schemaFile = null;
-    @Parameter(names = {"--format", "-f"}, description = "Format for the input data.", required = false,
+    @Parameter(names = {"--format", "-f"}, description = "Format for the input data.",
         validateValueWith = FormatValidator.class)
     public String format = null;
-    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
+    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master")
     public String sparkMaster = null;
     @Parameter(names = {"--spark-memory", "-sm"}, description = "spark memory to use", required = true)
     public String sparkMemory = null;
-    @Parameter(names = {"--retry", "-rt"}, description = "number of retries", required = false)
+    @Parameter(names = {"--retry", "-rt"}, description = "number of retries")
     public int retry = 0;
     @Parameter(names = {"--props"}, description = "path to properties file on localfs or dfs, with configurations for "
         + "hoodie client for importing")
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieWithTimelineServer.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieWithTimelineServer.java
index f89aa78ea..ad599005d 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieWithTimelineServer.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieWithTimelineServer.java
@@ -18,9 +18,10 @@
 
 package org.apache.hudi.utilities;
 
+import org.apache.hudi.common.util.ValidationUtils;
+
 import com.beust.jcommander.JCommander;
 import com.beust.jcommander.Parameter;
-import com.google.common.base.Preconditions;
 import io.javalin.Javalin;
 import org.apache.http.HttpResponse;
 import org.apache.http.client.methods.HttpGet;
@@ -89,7 +90,7 @@ public void run(JavaSparkContext jsc) throws UnknownHostException {
     IntStream.range(0, cfg.numPartitions).forEach(i -> messages.add("Hello World"));
     List<String> gotMessages = jsc.parallelize(messages).map(msg -> sendRequest(driverHost, cfg.serverPort)).collect();
     System.out.println("Got Messages :" + gotMessages);
-    Preconditions.checkArgument(gotMessages.equals(messages), "Got expected reply from Server");
+    ValidationUtils.checkArgument(gotMessages.equals(messages), "Got expected reply from Server");
   }
 
   public String sendRequest(String driverHost, int port) throws RuntimeException {
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
index 4e0c86cf3..d6402a607 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java
@@ -24,6 +24,7 @@
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.ReflectionUtils;
 import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
 import org.apache.hudi.config.HoodieWriteConfig;
@@ -33,7 +34,6 @@
 import org.apache.hudi.utilities.sources.Source;
 import org.apache.hudi.utilities.transform.Transformer;
 
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -118,7 +118,7 @@ public static TypedProperties buildProperties(List<String> props) {
     TypedProperties properties = new TypedProperties();
     props.forEach(x -> {
       String[] kv = x.split("=");
-      Preconditions.checkArgument(kv.length == 2);
+      ValidationUtils.checkArgument(kv.length == 2);
       properties.setProperty(kv[0], kv[1]);
     });
     return properties;
@@ -167,9 +167,8 @@ private static SparkConf buildSparkConf(String appName, String defaultMaster, Ma
     sparkConf.set("spark.hadoop.mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec");
     sparkConf.set("spark.hadoop.mapred.output.compression.type", "BLOCK");
 
-    additionalConfigs.entrySet().forEach(e -> sparkConf.set(e.getKey(), e.getValue()));
-    SparkConf newSparkConf = HoodieWriteClient.registerClasses(sparkConf);
-    return newSparkConf;
+    additionalConfigs.forEach(sparkConf::set);
+    return HoodieWriteClient.registerClasses(sparkConf);
   }
 
   public static JavaSparkContext buildSparkContext(String appName, String defaultMaster, Map<String, String> configs) {
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
index 8dc438eb7..4ac4df5a1 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java
@@ -32,6 +32,7 @@
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.config.HoodieCompactionConfig;
 import org.apache.hudi.config.HoodieIndexConfig;
@@ -49,7 +50,6 @@
 import org.apache.hudi.utilities.transform.Transformer;
 
 import com.codahale.metrics.Timer;
-import com.google.common.base.Preconditions;
 import org.apache.avro.Schema;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.hadoop.conf.Configuration;
@@ -365,8 +365,8 @@ private void refreshTimeline() throws IOException {
       throw new HoodieDeltaStreamerException("Unknown operation :" + cfg.operation);
     }
 
-    long totalErrorRecords = writeStatusRDD.mapToDouble(ws -> ws.getTotalErrorRecords()).sum().longValue();
-    long totalRecords = writeStatusRDD.mapToDouble(ws -> ws.getTotalRecords()).sum().longValue();
+    long totalErrorRecords = writeStatusRDD.mapToDouble(WriteStatus::getTotalErrorRecords).sum().longValue();
+    long totalRecords = writeStatusRDD.mapToDouble(WriteStatus::getTotalRecords).sum().longValue();
     boolean hasErrors = totalErrorRecords > 0;
     long hiveSyncTimeMs = 0;
     if (!hasErrors || cfg.commitOnErrors) {
@@ -403,10 +403,10 @@ private void refreshTimeline() throws IOException {
     } else {
       LOG.error("Delta Sync found errors when writing. Errors/Total=" + totalErrorRecords + "/" + totalRecords);
       LOG.error("Printing out the top 100 errors");
-      writeStatusRDD.filter(ws -> ws.hasErrors()).take(100).forEach(ws -> {
+      writeStatusRDD.filter(WriteStatus::hasErrors).take(100).forEach(ws -> {
         LOG.error("Global error :", ws.getGlobalError());
         if (ws.getErrors().size() > 0) {
-          ws.getErrors().entrySet().forEach(r -> LOG.trace("Error for key:" + r.getKey() + " is " + r.getValue()));
+          ws.getErrors().forEach((key, value) -> LOG.trace("Error for key:" + key + " is " + value));
         }
       });
       // Rolling back instant
@@ -491,10 +491,10 @@ private HoodieWriteConfig getHoodieClientConfig(SchemaProvider schemaProvider) {
     HoodieWriteConfig config = builder.build();
 
     // Validate what deltastreamer assumes of write-config to be really safe
-    Preconditions.checkArgument(config.isInlineCompaction() == cfg.isInlineCompactionEnabled());
-    Preconditions.checkArgument(!config.shouldAutoCommit());
-    Preconditions.checkArgument(config.shouldCombineBeforeInsert() == cfg.filterDupes);
-    Preconditions.checkArgument(config.shouldCombineBeforeUpsert());
+    ValidationUtils.checkArgument(config.isInlineCompaction() == cfg.isInlineCompactionEnabled());
+    ValidationUtils.checkArgument(!config.shouldAutoCommit());
+    ValidationUtils.checkArgument(config.shouldCombineBeforeInsert() == cfg.filterDupes);
+    ValidationUtils.checkArgument(config.shouldCombineBeforeUpsert());
 
     return config;
   }
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
index a56591df0..1baf880a7 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java
@@ -29,6 +29,7 @@
 import org.apache.hudi.common.util.FSUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.TypedProperties;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 import org.apache.hudi.exception.HoodieException;
 import org.apache.hudi.exception.HoodieIOException;
@@ -41,7 +42,6 @@
 import com.beust.jcommander.JCommander;
 import com.beust.jcommander.Parameter;
 import com.beust.jcommander.ParameterException;
-import com.google.common.base.Preconditions;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -64,7 +64,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.locks.Condition;
 import java.util.concurrent.locks.ReentrantLock;
-import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
 /**
@@ -356,7 +355,7 @@ public DeltaSyncService(HoodieDeltaStreamer.Config cfg, JavaSparkContext jssc, F
             new HoodieTableMetaClient(new Configuration(fs.getConf()), cfg.targetBasePath, false);
         tableType = meta.getTableType();
         // This will guarantee there is no surprise with table type
-        Preconditions.checkArgument(tableType.equals(HoodieTableType.valueOf(cfg.storageType)),
+        ValidationUtils.checkArgument(tableType.equals(HoodieTableType.valueOf(cfg.storageType)),
             "Hoodie table is of type " + tableType + " but passed in CLI argument is " + cfg.storageType);
       } else {
         tableType = HoodieTableType.valueOf(cfg.storageType);
@@ -557,29 +556,27 @@ private HoodieInstant fetchNextCompactionInstant() throws InterruptedException {
     @Override
     protected Pair<CompletableFuture, ExecutorService> startService() {
       ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction);
-      List<CompletableFuture<Boolean>> compactionFutures =
-          IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {
-            try {
-              // Set Compactor Pool Name for allowing users to prioritize compaction
-              LOG.info("Setting Spark Pool name for compaction to " + SchedulerConfGenerator.COMPACT_POOL_NAME);
-              jssc.setLocalProperty("spark.scheduler.pool", SchedulerConfGenerator.COMPACT_POOL_NAME);
-
-              while (!isShutdownRequested()) {
-                final HoodieInstant instant = fetchNextCompactionInstant();
-                if (null != instant) {
-                  compactor.compact(instant);
-                }
-              }
-              LOG.info("Compactor shutting down properly!!");
-            } catch (InterruptedException ie) {
-              LOG.warn("Compactor executor thread got interrupted exception. Stopping", ie);
-            } catch (IOException e) {
-              LOG.error("Compactor executor failed", e);
-              throw new HoodieIOException(e.getMessage(), e);
+      return Pair.of(CompletableFuture.allOf(IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {
+        try {
+          // Set Compactor Pool Name for allowing users to prioritize compaction
+          LOG.info("Setting Spark Pool name for compaction to " + SchedulerConfGenerator.COMPACT_POOL_NAME);
+          jssc.setLocalProperty("spark.scheduler.pool", SchedulerConfGenerator.COMPACT_POOL_NAME);
+
+          while (!isShutdownRequested()) {
+            final HoodieInstant instant = fetchNextCompactionInstant();
+            if (null != instant) {
+              compactor.compact(instant);
             }
-            return true;
-          }, executor)).collect(Collectors.toList());
-      return Pair.of(CompletableFuture.allOf(compactionFutures.stream().toArray(CompletableFuture[]::new)), executor);
+          }
+          LOG.info("Compactor shutting down properly!!");
+        } catch (InterruptedException ie) {
+          LOG.warn("Compactor executor thread got interrupted exception. Stopping", ie);
+        } catch (IOException e) {
+          LOG.error("Compactor executor failed", e);
+          throw new HoodieIOException(e.getMessage(), e);
+        }
+        return true;
+      }, executor)).toArray(CompletableFuture[]::new)), executor);
     }
   }
 
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java
index 7fa0da502..e17786527 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java
@@ -114,7 +114,7 @@ public void run() throws IOException {
     d2.close();
 
     System.out.println("\n\n\nDumping all File Slices");
-    selected.stream().forEach(p -> fsView.getAllFileSlices(p).forEach(s -> System.out.println("\tMyFileSlice=" + s)));
+    selected.forEach(p -> fsView.getAllFileSlices(p).forEach(s -> System.out.println("\tMyFileSlice=" + s)));
 
     // Waiting for curl queries
     if (!useExternalTimelineServer && cfg.waitForManualQueries) {
@@ -131,17 +131,15 @@ public void run() throws IOException {
 
   public List<PerfStats> runLookups(JavaSparkContext jsc, List<String> partitionPaths, SyncableFileSystemView fsView,
       int numIterations, int concurrency) {
-    List<PerfStats> perfStats = jsc.parallelize(partitionPaths, cfg.numExecutors).flatMap(p -> {
+    return jsc.parallelize(partitionPaths, cfg.numExecutors).flatMap(p -> {
       ScheduledThreadPoolExecutor executor = new ScheduledThreadPoolExecutor(100);
       final List<PerfStats> result = new ArrayList<>();
       final List<ScheduledFuture<PerfStats>> futures = new ArrayList<>();
       List<FileSlice> slices = fsView.getLatestFileSlices(p).collect(Collectors.toList());
       String fileId = slices.isEmpty() ? "dummyId"
           : slices.get(new Random(Double.doubleToLongBits(Math.random())).nextInt(slices.size())).getFileId();
-      IntStream.range(0, concurrency).forEach(i -> {
-        futures.add(executor.schedule(() -> runOneRound(fsView, p, fileId, i, numIterations), 0, TimeUnit.NANOSECONDS));
-      });
-      futures.stream().forEach(x -> {
+      IntStream.range(0, concurrency).forEach(i -> futures.add(executor.schedule(() -> runOneRound(fsView, p, fileId, i, numIterations), 0, TimeUnit.NANOSECONDS)));
+      futures.forEach(x -> {
         try {
           result.add(x.get());
         } catch (InterruptedException | ExecutionException e) {
@@ -149,12 +147,9 @@ public void run() throws IOException {
         }
       });
       System.out.println("SLICES are=");
-      slices.stream().forEach(s -> {
-        System.out.println("\t\tFileSlice=" + s);
-      });
+      slices.forEach(s -> System.out.println("\t\tFileSlice=" + s));
       return result.iterator();
     }).collect();
-    return perfStats;
   }
 
   private static PerfStats runOneRound(SyncableFileSystemView fsView, String partition, String fileId, int id,
@@ -194,7 +189,7 @@ private void addHeader() throws IOException {
     }
 
     public void dump(List<PerfStats> stats) {
-      stats.stream().forEach(x -> {
+      stats.forEach(x -> {
         String row = String.format("%s,%d,%d,%d,%f,%f,%f,%f\n", x.partition, x.id, x.minTime, x.maxTime, x.meanTime,
             x.medianTime, x.p75, x.p95);
         System.out.println(row);
@@ -260,7 +255,7 @@ public PerfStats(String partition, int id, long minTime, long maxTime, double me
     @Parameter(names = {"--num-iterations", "-i"}, description = "Number of iterations for each partitions")
     public Integer numIterations = 10;
 
-    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master", required = false)
+    @Parameter(names = {"--spark-master", "-ms"}, description = "Spark master")
     public String sparkMaster = "local[2]";
 
     @Parameter(names = {"--server-port", "-p"}, description = " Server Port")
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
index 888eec702..9396e2440 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/HoodieIncrSource.java
@@ -135,7 +135,7 @@ public HoodieIncrSource(TypedProperties props, JavaSparkContext sparkContext, Sp
      * instantEndpts.getValue()); if (!partitionFields.isEmpty()) { // _hoodie_partition_path String hoodiePartitionPath
      * = row.getString(3); List<Object> partitionVals =
      * extractor.extractPartitionValuesInPath(hoodiePartitionPath).stream() .map(o -> (Object)
-     * o).collect(Collectors.toList()); Preconditions.checkArgument(partitionVals.size() == partitionFields.size(),
+     * o).collect(Collectors.toList()); ValidationUtils.checkArgument(partitionVals.size() == partitionFields.size(),
      * "#partition-fields != #partition-values-extracted"); List<Object> rowObjs = new
      * ArrayList<>(scala.collection.JavaConversions.seqAsJavaList(row.toSeq())); rowObjs.addAll(partitionVals); return
      * RowFactory.create(rowObjs.toArray()); } return row; }, RowEncoder.apply(newSchema));
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
index 54ea0f3af..44fd50dd9 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/IncrSourceHelper.java
@@ -22,12 +22,14 @@
 import org.apache.hudi.common.table.HoodieTimeline;
 import org.apache.hudi.common.table.timeline.HoodieInstant;
 import org.apache.hudi.common.util.Option;
+import org.apache.hudi.common.util.ValidationUtils;
 import org.apache.hudi.common.util.collection.Pair;
 
-import com.google.common.base.Preconditions;
 import org.apache.spark.api.java.JavaSparkContext;
 import org.apache.spark.sql.Row;
 
+import java.util.Objects;
+
 public class IncrSourceHelper {
 
   /**
@@ -37,8 +39,8 @@
    */
   private static String getStrictlyLowerTimestamp(String timestamp) {
     long ts = Long.parseLong(timestamp);
-    Preconditions.checkArgument(ts > 0, "Timestamp must be positive");
-    Long lower = ts - 1;
+    ValidationUtils.checkArgument(ts > 0, "Timestamp must be positive");
+    long lower = ts - 1;
     return "" + lower;
   }
 
@@ -54,7 +56,7 @@ private static String getStrictlyLowerTimestamp(String timestamp) {
    */
   public static Pair<String, String> calculateBeginAndEndInstants(JavaSparkContext jssc, String srcBasePath,
       int numInstantsPerFetch, Option<String> beginInstant, boolean readLatestOnMissingBeginInstant) {
-    Preconditions.checkArgument(numInstantsPerFetch > 0,
+    ValidationUtils.checkArgument(numInstantsPerFetch > 0,
         "Make sure the config hoodie.deltastreamer.source.hoodieincr.num_instants is set to a positive value");
     HoodieTableMetaClient srcMetaClient = new HoodieTableMetaClient(jssc.hadoopConfiguration(), srcBasePath, true);
 
@@ -73,7 +75,7 @@ private static String getStrictlyLowerTimestamp(String timestamp) {
 
     Option<HoodieInstant> nthInstant = Option.fromJavaOptional(activeCommitTimeline
         .findInstantsAfter(beginInstantTime, numInstantsPerFetch).getInstants().reduce((x, y) -> y));
-    return Pair.of(beginInstantTime, nthInstant.map(instant -> instant.getTimestamp()).orElse(beginInstantTime));
+    return Pair.of(beginInstantTime, nthInstant.map(HoodieInstant::getTimestamp).orElse(beginInstantTime));
   }
 
   /**
@@ -85,11 +87,11 @@ private static String getStrictlyLowerTimestamp(String timestamp) {
    * @param endInstant end instant of the batch
    */
   public static void validateInstantTime(Row row, String instantTime, String sinceInstant, String endInstant) {
-    Preconditions.checkNotNull(instantTime);
-    Preconditions.checkArgument(HoodieTimeline.compareTimestamps(instantTime, sinceInstant, HoodieTimeline.GREATER),
+    Objects.requireNonNull(instantTime);
+    ValidationUtils.checkArgument(HoodieTimeline.compareTimestamps(instantTime, sinceInstant, HoodieTimeline.GREATER),
         "Instant time(_hoodie_commit_time) in row (" + row + ") was : " + instantTime + "but expected to be between "
             + sinceInstant + "(excl) - " + endInstant + "(incl)");
-    Preconditions.checkArgument(
+    ValidationUtils.checkArgument(
         HoodieTimeline.compareTimestamps(instantTime, endInstant, HoodieTimeline.LESSER_OR_EQUAL),
         "Instant time(_hoodie_commit_time) in row (" + row + ") was : " + instantTime + "but expected to be between "
             + sinceInstant + "(excl) - " + endInstant + "(incl)");
diff --git a/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java b/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
index c17a5cff7..3409f4b6f 100644
--- a/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
+++ b/hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java
@@ -101,8 +101,7 @@ public static String offsetsToStr(OffsetRange[] ranges) {
 
       // Create initial offset ranges for each 'to' partition, with from = to offsets.
       OffsetRange[] ranges = new OffsetRange[toOffsetMap.size()];
-      toOffsetMap.entrySet().stream().map(e -> {
-        TopicAndPartition tp = e.getKey();
+      toOffsetMap.keySet().stream().map(tp -> {
         long fromOffset = fromOffsetMap.getOrDefault(tp, new LeaderOffset("", -1, 0)).offset();
         return OffsetRange.create(tp, fromOffset, fromOffset);
       }).sorted(byPartition).collect(Collectors.toList()).toArray(ranges);
@@ -186,7 +185,7 @@ public static long totalNewMessages(OffsetRange[] ranges) {
 
   public KafkaOffsetGen(TypedProperties props) {
     this.props = props;
-    kafkaParams = new HashMap<String, String>();
+    kafkaParams = new HashMap<>();
     for (Object prop : props.keySet()) {
       kafkaParams.put(prop.toString(), props.getString(prop.toString()));
     }
@@ -208,7 +207,6 @@ public KafkaOffsetGen(TypedProperties props) {
 
     // Determine the offset ranges to read from
     HashMap<TopicAndPartition, KafkaCluster.LeaderOffset> fromOffsets;
-    HashMap<TopicAndPartition, KafkaCluster.LeaderOffset> checkpointOffsets;
     if (lastCheckpointStr.isPresent()) {
       fromOffsets = checkupValidOffsets(cluster, lastCheckpointStr, topicPartitions);
     } else {
@@ -217,11 +215,11 @@ public KafkaOffsetGen(TypedProperties props) {
       switch (autoResetValue) {
         case SMALLEST:
           fromOffsets =
-              new HashMap(ScalaHelpers.toJavaMap(cluster.getEarliestLeaderOffsets(topicPartitions).right().get()));
+              new HashMap<>(ScalaHelpers.toJavaMap(cluster.getEarliestLeaderOffsets(topicPartitions).right().get()));
           break;
         case LARGEST:
           fromOffsets =
-              new HashMap(ScalaHelpers.toJavaMap(cluster.getLatestLeaderOffsets(topicPartitions).right().get()));
+              new HashMap<>(ScalaHelpers.toJavaMap(cluster.getLatestLeaderOffsets(topicPartitions).right().get()));
           break;
         default:
           throw new HoodieNotSupportedException("Auto reset value must be one of 'smallest' or 'largest' ");
@@ -230,7 +228,7 @@ public KafkaOffsetGen(TypedProperties props) {
 
     // Obtain the latest offsets.
     HashMap<TopicAndPartition, KafkaCluster.LeaderOffset> toOffsets =
-        new HashMap(ScalaHelpers.toJavaMap(cluster.getLatestLeaderOffsets(topicPartitions).right().get()));
+        new HashMap<>(ScalaHelpers.toJavaMap(cluster.getLatestLeaderOffsets(topicPartitions).right().get()));
 
     // Come up with final set of OffsetRanges to read (account for new partitions, limit number of events)
     long maxEventsToReadFromKafka = props.getLong(Config.MAX_EVENTS_FROM_KAFKA_SOURCE_PROP,
@@ -238,9 +236,7 @@ public KafkaOffsetGen(TypedProperties props) {
     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)
         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;
     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;
-    OffsetRange[] offsetRanges = CheckpointUtils.computeOffsetRanges(fromOffsets, toOffsets, numEvents);
-
-    return offsetRanges;
+    return CheckpointUtils.computeOffsetRanges(fromOffsets, toOffsets, numEvents);
   }
 
   // check up checkpoint offsets is valid or not, if true, return checkpoint offsets,
@@ -250,7 +246,7 @@ public KafkaOffsetGen(TypedProperties props) {
     HashMap<TopicAndPartition, KafkaCluster.LeaderOffset> checkpointOffsets =
         CheckpointUtils.strToOffsets(lastCheckpointStr.get());
     HashMap<TopicAndPartition, KafkaCluster.LeaderOffset> earliestOffsets =
-        new HashMap(ScalaHelpers.toJavaMap(cluster.getEarliestLeaderOffsets(topicPartitions).right().get()));
+        new HashMap<>(ScalaHelpers.toJavaMap(cluster.getEarliestLeaderOffsets(topicPartitions).right().get()));
 
     boolean checkpointOffsetReseter = checkpointOffsets.entrySet().stream()
         .anyMatch(offset -> offset.getValue().offset() < earliestOffsets.get(offset.getKey()).offset());
diff --git a/hudi-utilities/src/test/java/org/apache/hudi/utilities/UtilitiesTestBase.java b/hudi-utilities/src/test/java/org/apache/hudi/utilities/UtilitiesTestBase.java
index f0db1048b..30d81eec0 100644
--- a/hudi-utilities/src/test/java/org/apache/hudi/utilities/UtilitiesTestBase.java
+++ b/hudi-utilities/src/test/java/org/apache/hudi/utilities/UtilitiesTestBase.java
@@ -25,6 +25,7 @@
 import org.apache.hudi.common.model.HoodieTableType;
 import org.apache.hudi.common.model.HoodieTestUtils;
 import org.apache.hudi.common.table.HoodieTableMetaClient;
+import org.apache.hudi.common.util.CollectionUtils;
 import org.apache.hudi.common.util.Option;
 import org.apache.hudi.common.util.TypedProperties;
 import org.apache.hudi.hive.HiveSyncConfig;
@@ -32,7 +33,6 @@
 import org.apache.hudi.hive.util.HiveTestService;
 import org.apache.hudi.utilities.sources.TestDataSource;
 
-import com.google.common.collect.ImmutableList;
 import org.apache.avro.generic.GenericRecord;
 import org.apache.avro.generic.IndexedRecord;
 import org.apache.hadoop.fs.FileSystem;
@@ -134,7 +134,7 @@ protected static HiveSyncConfig getHiveSyncConfig(String basePath, String tableN
     hiveSyncConfig.basePath = basePath;
     hiveSyncConfig.assumeDatePartitioning = false;
     hiveSyncConfig.usePreApacheInputFormat = false;
-    hiveSyncConfig.partitionFields = new ImmutableList.Builder<String>().add("datestr").build();
+    hiveSyncConfig.partitionFields = CollectionUtils.createImmutableList("datestr");
     return hiveSyncConfig;
   }
 
