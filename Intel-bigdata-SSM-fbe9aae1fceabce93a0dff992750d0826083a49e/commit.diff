diff --git a/smart-hadoop-support/smart-hadoop-2/src/main/java/org/smartdata/hdfs/CompatibilityHelper2.java b/smart-hadoop-support/smart-hadoop-2/src/main/java/org/smartdata/hdfs/CompatibilityHelper2.java
index 625ad4e9e2..33af305aee 100644
--- a/smart-hadoop-support/smart-hadoop-2/src/main/java/org/smartdata/hdfs/CompatibilityHelper2.java
+++ b/smart-hadoop-support/smart-hadoop-2/src/main/java/org/smartdata/hdfs/CompatibilityHelper2.java
@@ -85,7 +85,8 @@ public byte getErasureCodingPolicyByName(DFSClient client, String ecPolicyName)
     return null;
   }
 
-  public List<String> getStorageTypeForEcBlock(LocatedBlock lb, BlockStoragePolicy policy, byte policyId) {
+  public List<String> getStorageTypeForEcBlock(LocatedBlock lb, BlockStoragePolicy policy,
+      byte policyId) throws IOException {
     return null;
   }
 
diff --git a/smart-hadoop-support/smart-hadoop-3.1/src/main/java/org/smartdata/hdfs/CompatibilityHelper31.java b/smart-hadoop-support/smart-hadoop-3.1/src/main/java/org/smartdata/hdfs/CompatibilityHelper31.java
index 247422b05d..04c1358f16 100644
--- a/smart-hadoop-support/smart-hadoop-3.1/src/main/java/org/smartdata/hdfs/CompatibilityHelper31.java
+++ b/smart-hadoop-support/smart-hadoop-3.1/src/main/java/org/smartdata/hdfs/CompatibilityHelper31.java
@@ -258,13 +258,22 @@ public byte getErasureCodingPolicyByName(DFSClient client, String ecPolicyName)
 
   @Override
   public List<String> getStorageTypeForEcBlock(
-      LocatedBlock lb, BlockStoragePolicy policy, byte policyId) {
+      LocatedBlock lb, BlockStoragePolicy policy, byte policyId) throws IOException {
     if (lb.isStriped()) {
       //TODO: verify the current storage policy (policyID) or the target one
       //TODO: output log for unsupported storage policy for EC block
+      String policyName = policy.getName();
+      // Exclude onessd/onedisk action to be executed on EC block.
+      // EC blocks can only be put on a same storage medium.
+      if (policyName.equalsIgnoreCase("Warm") |
+          policyName.equalsIgnoreCase("One_SSD")) {
+        throw new IOException("onessd or onedisk is not applicable to EC block!");
+      }
       if (ErasureCodingPolicyManager
           .checkStoragePolicySuitableForECStripedMode(policyId)) {
         return chooseStorageTypes(policy, (short) lb.getLocations().length);
+      } else {
+        throw new IOException("Unsupported storage policy for EC block: " + policy.getName());
       }
     }
     return null;
diff --git a/smart-hadoop-support/smart-hadoop-common/src/main/java/org/smartdata/hdfs/CompatibilityHelper.java b/smart-hadoop-support/smart-hadoop-common/src/main/java/org/smartdata/hdfs/CompatibilityHelper.java
index a5f6839516..39418ddea5 100644
--- a/smart-hadoop-support/smart-hadoop-common/src/main/java/org/smartdata/hdfs/CompatibilityHelper.java
+++ b/smart-hadoop-support/smart-hadoop-common/src/main/java/org/smartdata/hdfs/CompatibilityHelper.java
@@ -99,7 +99,7 @@ HdfsFileStatus createHdfsFileStatus(
 
   Map<Byte, String> getErasureCodingPolicies(DFSClient client) throws IOException;
 
-  List<String> getStorageTypeForEcBlock(LocatedBlock lb, BlockStoragePolicy policy, byte policyId);
+  List<String> getStorageTypeForEcBlock(LocatedBlock lb, BlockStoragePolicy policy, byte policyId) throws IOException;
 
   DBlock newDBlock(LocatedBlock lb, HdfsFileStatus status);
 
diff --git a/smart-hadoop-support/smart-hadoop/src/main/java/org/smartdata/hdfs/metric/fetcher/MovePlanMaker.java b/smart-hadoop-support/smart-hadoop/src/main/java/org/smartdata/hdfs/metric/fetcher/MovePlanMaker.java
index b11e8ba4fb..7abc96348f 100644
--- a/smart-hadoop-support/smart-hadoop/src/main/java/org/smartdata/hdfs/metric/fetcher/MovePlanMaker.java
+++ b/smart-hadoop-support/smart-hadoop/src/main/java/org/smartdata/hdfs/metric/fetcher/MovePlanMaker.java
@@ -19,7 +19,6 @@
 
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hdfs.DFSClient;
-import org.apache.hadoop.hdfs.protocol.Block;
 import org.apache.hadoop.hdfs.protocol.BlockStoragePolicy;
 import org.apache.hadoop.hdfs.protocol.DatanodeInfo;
 import org.apache.hadoop.hdfs.protocol.DirectoryListing;
@@ -96,7 +95,8 @@ public synchronized void updateClusterInfo(StorageMap storages, NetworkTopology
    * @return whether there is still remaining migration work for the next
    * round
    */
-  public synchronized FileMovePlan processNamespace(Path targetPath, String destPolicy) throws IOException {
+  public synchronized FileMovePlan processNamespace(Path targetPath, String destPolicy)
+      throws IOException {
     schedulePlan = new FileMovePlan();
     String filePath = targetPath.toUri().getPath();
     schedulePlan.setFileName(filePath);
@@ -144,7 +144,8 @@ public synchronized FileMovePlan processNamespace(Path targetPath, String destPo
   /**
    * @return true if it is necessary to run another round of migration
    */
-  private void processFile(String fullPath, HdfsLocatedFileStatus status, String destPolicy) {
+  private void processFile(String fullPath, HdfsLocatedFileStatus status,
+      String destPolicy) throws IOException {
     final BlockStoragePolicy policy = mapStoragePolicies.get(destPolicy);
     if (policy == null) {
       LOG.warn("Failed to get the storage policy of file " + fullPath);
@@ -183,11 +184,18 @@ private void processFile(String fullPath, HdfsLocatedFileStatus status, String d
     }
   }
 
-  boolean scheduleMoveBlock(StorageTypeDiff diff, LocatedBlock lb, HdfsFileStatus status) {
+  /**
+   * TODO: consider the case that fails to move some blocks, i.e., scheduleMoveReplica fails.
+   */
+  void scheduleMoveBlock(StorageTypeDiff diff, LocatedBlock lb, HdfsFileStatus status) {
     final List<MLocation> locations = MLocation.toLocations(lb);
     if (!CompatibilityHelperLoader.getHelper().isLocatedStripedBlock(lb)) {
+      // Shuffle replica locations to make storage medium in balance.
+      // E.g., if three replicas are under ALL_SSD policy and ONE_SSD is the target policy,
+      // with shuffling locations, two randomly picked replicas will be moved to DISK.
       Collections.shuffle(locations);
     }
+    // EC block case is considered.
     final DBlock db =
         CompatibilityHelperLoader.getHelper().newDBlock(lb, status);
     for (MLocation ml : locations) {
@@ -196,24 +204,32 @@ boolean scheduleMoveBlock(StorageTypeDiff diff, LocatedBlock lb, HdfsFileStatus
         db.addLocation(source);
       }
     }
-    boolean needMove = false;
 
-    for (int i = 0; i < diff.existing.size(); i++) {
-      String t = diff.existing.get(i);
-      MLocation ml = locations.get(i);
-      final Source source = storages.getSource(ml);
-      if (ml.getStorageType().equals(t) && source != null) {
-        // try to schedule one replica move.
-        if (scheduleMoveReplica(db, source, Arrays.asList(diff.expected.get(i)))) {
-          needMove = true;
+    for (int index = 0; index < diff.existing.size(); index++) {
+      String t = diff.existing.get(index);
+      Iterator<MLocation> iter = locations.iterator();
+      while (iter.hasNext()) {
+        MLocation ml = iter.next();
+        final Source source = storages.getSource(ml);
+        // Check whether the replica's storage type equals with the one
+        // in diff's existing list. If so, try to schedule the moving.
+        if (ml.getStorageType() == t && source != null) {
+          // Schedule moving a replica on a source location.
+          // The corresponding storage type in diff's expected list is used.
+          if (scheduleMoveReplica(db, source,
+              Arrays.asList(diff.expected.get(index)))) {
+            // If the replica is successfully scheduled to move.
+            // No need to consider it any more.
+            iter.remove();
+            // Tackle the next storage type in diff existing list.
+            break;
+          }
         }
       }
     }
-    return needMove;
   }
 
-  boolean scheduleMoveReplica(DBlock db, Source source,
-                              List<String> targetTypes) {
+  boolean scheduleMoveReplica(DBlock db, Source source, List<String> targetTypes) {
     // Match storage on the same node
     if (chooseTargetInSameNode(db, source, targetTypes)) {
       return true;
diff --git a/smart-server/src/test/java/org/smartdata/server/engine/cmdlet/TestCompressDecompress.java b/smart-server/src/test/java/org/smartdata/server/engine/cmdlet/TestCompressDecompress.java
index 5a7fa93f32..6c712d6b5e 100644
--- a/smart-server/src/test/java/org/smartdata/server/engine/cmdlet/TestCompressDecompress.java
+++ b/smart-server/src/test/java/org/smartdata/server/engine/cmdlet/TestCompressDecompress.java
@@ -34,6 +34,7 @@
 import org.smartdata.hdfs.client.SmartDFSClient;
 import org.smartdata.hdfs.scheduler.CompressionScheduler;
 import org.smartdata.metastore.MetaStore;
+import org.smartdata.model.CmdletInfo;
 import org.smartdata.model.CmdletState;
 import org.smartdata.model.CompressionFileState;
 import org.smartdata.model.FileState;
@@ -372,7 +373,11 @@ private void waitTillActionDone(long cmdId) throws Exception {
     while (true) {
       Thread.sleep(1000);
       CmdletManager cmdletManager = ssm.getCmdletManager();
-      CmdletState state = cmdletManager.getCmdletInfo(cmdId).getState();
+      CmdletInfo info = cmdletManager.getCmdletInfo(cmdId);
+      if (info == null) {
+        continue;
+      }
+      CmdletState state = info.getState();
       if (state == CmdletState.DONE) {
         return;
       } else if (state == CmdletState.FAILED) {
diff --git a/smart-zeppelin/zeppelin-web/src/app/dashboard/views/actions/submit/help.html b/smart-zeppelin/zeppelin-web/src/app/dashboard/views/actions/submit/help.html
index 7bdf691cbb..2baae08f06 100755
--- a/smart-zeppelin/zeppelin-web/src/app/dashboard/views/actions/submit/help.html
+++ b/smart-zeppelin/zeppelin-web/src/app/dashboard/views/actions/submit/help.html
@@ -165,17 +165,19 @@ <h3 class="text-center">Action Usage</h3>
     <tr>
       <td>onedisk</td>
       <td>-file $file</td>
-      <td>Move one replica of $file to disk.</td>
+      <td>Move one replica of $file to disk. Not applicable to EC block.</td>
     </tr>
     <tr>
       <td>onessd</td>
       <td>-file $file</td>
-      <td>Move one replica of $file to SSD.</td>
+      <td>Move one replica of $file to SSD. Not applicable to EC block.</td>
     </tr>
     <tr>
       <td>ramdisk</td>
       <td>-file $file</td>
-      <td>Move $file to RAM_DISK.</td>
+      <td>Set LAZY_PERSIST storage policy. $file should be a dir path and RAM_DISK is
+        required to be configured beforehand. After the setting, the newly created
+        file under this dir will be lazily persisted.</td>
     </tr>
     <tr>
       <td>read</td>
